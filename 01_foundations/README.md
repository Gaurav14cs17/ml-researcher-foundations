<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Mathematical%20Foundations&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=32" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-01-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Topics-6-4ECDC4?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Proofs-Complete-success?style=flat-square" alt="Proofs"/>
  <img src="https://img.shields.io/badge/Examples-50+-informational?style=flat-square" alt="Examples"/>
  <img src="https://img.shields.io/badge/Code-Python%20%7C%20PyTorch-orange?style=flat-square" alt="Code"/>
  <img src="https://img.shields.io/badge/ML_Applications-âœ“-brightgreen?style=flat-square" alt="ML"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ TL;DR

> **Mathematical foundations provide the rigorous thinking patterns essential for ML research.**

| Foundation | Purpose | ML Application |
|------------|---------|----------------|
| ğŸ“ **Mathematical Thinking** | Abstraction, logical reasoning | Understanding paper proofs |
| ğŸ”¢ **Proof Techniques** | Verify claims rigorously | Proving convergence guarantees |
| ğŸ“Š **Set Theory** | Foundation for probability | Sample spaces, Ïƒ-algebras |
| ğŸ§  **Logic** | Formal reasoning | Specifications, constraints |
| â±ï¸ **Asymptotic Analysis** | Algorithm efficiency | Model scalability |
| ğŸ’» **Numerical Computation** | Floating-point stability | Training stability |

---

## ğŸ“š Table of Contents

| # | Topic | Key Concepts | Time | Link |
|:-:|-------|--------------|:----:|:----:|
| 1 | [Mathematical Thinking](#-1-mathematical-thinking) | Abstraction, Necessary vs Sufficient | 3h | [ğŸ“](./01_mathematical_thinking/) |
| 2 | [Proof Techniques](#-2-proof-techniques) | Direct, Contradiction, Induction | 4h | [ğŸ“](./02_proof_techniques/) |
| 3 | [Set Theory](#-3-set-theory) | Sets, Functions, Relations, Ïƒ-algebras | 3h | [ğŸ“](./03_set_theory/) |
| 4 | [Logic](#-4-logic) | Propositional, Predicate, Inference | 3h | [ğŸ“](./04_logic/) |
| 5 | [Asymptotic Analysis](#-5-asymptotic-analysis) | Big-O, Î©, Î˜, little-o Notation | 3h | [ğŸ“](./05_asymptotic_analysis/) |
| 6 | [Numerical Computation](#-6-numerical-computation) | Floating Point, Stability, Mixed Precision | 3h | [ğŸ“](./06_numerical_computation/) |

**Total: ~19 hours**

---

## ğŸ—ºï¸ Visual Overview

```
+---------------------------------------------------------------------------------+
|                          MATHEMATICAL FOUNDATIONS                                |
|                     Building Blocks for ML Research                              |
+---------------------------------------------------------------------------------+
|                                                                                  |
|     +------------------+         +------------------+                           |
|     |  ğŸ“ Mathematical |         |  ğŸ”¢ Proof        |                           |
|     |     Thinking     |--------â–¶|   Techniques     |                           |
|     |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |         |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |                           |
|     |  â€¢ Abstraction   |         |  â€¢ Direct Proof  |                           |
|     |  â€¢ Nec. vs Suff. |         |  â€¢ Contradiction |                           |
|     |  â€¢ Definitions   |         |  â€¢ Induction     |                           |
|     +--------+---------+         +--------+---------+                           |
|              |                            |                                      |
|              â–¼                            â–¼                                      |
|     +------------------+         +------------------+                           |
|     |  ğŸ“Š Set Theory   |         |  ğŸ§  Logic        |                           |
|     |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |         |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |                           |
|     |  â€¢ Operations    |â—€-------â–¶|  â€¢ Propositional |                           |
|     |  â€¢ Functions     |         |  â€¢ Predicate     |                           |
|     |  â€¢ Ïƒ-algebras    |         |  â€¢ Inference     |                           |
|     +--------+---------+         +--------+---------+                           |
|              |                            |                                      |
|              â–¼                            â–¼                                      |
|     +------------------+         +------------------+                           |
|     |  â±ï¸ Asymptotic   |         |  ğŸ’» Numerical    |                           |
|     |    Analysis      |         |   Computation    |                           |
|     |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |         |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |                           |
|     |  â€¢ Big-O, Î©, Î˜   |         |  â€¢ IEEE 754      |                           |
|     |  â€¢ Complexity    |         |  â€¢ Stability     |                           |
|     |  â€¢ Scalability   |         |  â€¢ Mixed Prec.   |                           |
|     +--------+---------+         +--------+---------+                           |
|              |                            |                                      |
|              +------------+---------------+                                      |
|                           â–¼                                                      |
|              +------------------------------+                                    |
|              |     ğŸ¤– MACHINE LEARNING      |                                    |
|              |  â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ  |                                    |
|              |  Linear Algebra â€¢ Calculus   |                                    |
|              |  Probability â€¢ Optimization  |                                    |
|              +------------------------------+                                    |
+---------------------------------------------------------------------------------+
```

---

## ğŸ“ 1. Mathematical Thinking

> **ğŸ“ [Detailed Notes â†’](./01_mathematical_thinking/)**

### Core Concepts

#### 1.1 Abstraction Levels

**Definition:** Abstraction is the process of extracting essential features while ignoring irrelevant details.

```
+-----------------------------------------------------------------+
|                    ABSTRACTION LEVELS IN ML                      |
+-----------------------------------------------------------------+
|                                                                  |
|  Level 4   +-------------------------------------+              |
|  (Highest) |  "Train a classifier"               |  â† User      |
|            +-------------------------------------+              |
|                           |                                      |
|  Level 3   +-------------------------------------+              |
|            |  "Minimize cross-entropy loss"      |  â† ML Eng    |
|            +-------------------------------------+              |
|                           |                                      |
|  Level 2   +-------------------------------------+              |
|            |  Î¸ â† Î¸ - Î·âˆ‡L(Î¸)                     |  â† Researcher|
|            +-------------------------------------+              |
|                           |                                      |
|  Level 1   +-------------------------------------+              |
|            |  for w in weights: w -= lr * dL/dw  |  â† Implement |
|            +-------------------------------------+              |
|                           |                                      |
|  Level 0   +-------------------------------------+              |
|  (Lowest)  |  Binary floating-point operations   |  â† Hardware  |
|            +-------------------------------------+              |
+-----------------------------------------------------------------+
```

**PyTorch Example:**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# ===============================================================
# HIGH ABSTRACTION (PyTorch)
# ===============================================================
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
loss = F.cross_entropy(model(x), y)
loss.backward()  # Automatic differentiation!

# ===============================================================
# LOW ABSTRACTION (NumPy - manual everything)
# ===============================================================
import numpy as np

def forward(x, W1, b1, W2, b2):
    z1 = x @ W1 + b1           # Linear
    a1 = np.maximum(0, z1)      # ReLU
    z2 = a1 @ W2 + b2           # Linear
    return z2, z1, a1

def backward(x, y, z2, z1, a1, W1, W2):

    # Softmax + cross-entropy gradient
    exp_z2 = np.exp(z2 - z2.max(axis=1, keepdims=True))
    softmax = exp_z2 / exp_z2.sum(axis=1, keepdims=True)
    
    dz2 = softmax.copy()
    dz2[range(len(y)), y] -= 1
    dz2 /= len(y)
    
    dW2 = a1.T @ dz2
    db2 = dz2.sum(axis=0)
    
    da1 = dz2 @ W2.T
    dz1 = da1 * (z1 > 0)  # ReLU derivative
    
    dW1 = x.T @ dz1
    db1 = dz1.sum(axis=0)
    
    return dW1, db1, dW2, db2
```

#### 1.2 Necessary vs Sufficient Conditions

**Formal Definitions:**

| Condition | Notation | Meaning | Example |
|-----------|----------|---------|---------|
| **Necessary** | Q â†’ P | P is required for Q | "Fuel is necessary to drive" |
| **Sufficient** | P â†’ Q | P guarantees Q | "Winning lottery is sufficient to be rich" |
| **Nec. & Suff.** | P â†” Q | P if and only if Q | "Triangle has 3 sides â†” Triangle" |

**ML Examples with Analysis:**

| Statement | Nec? | Suff? | Explanation |
|-----------|:----:|:-----:|-------------|
| Convexity â†’ Global optimum | âŒ | âœ… | Non-convex can have global optimum too |
| âˆ‡f(x*) = 0 for minimum | âœ… | âŒ | Could be saddle point or maximum |
| Full rank for unique solution | âœ… | âœ… | Exactly characterizes unique solution |
| Large dataset for good model | âŒ | âŒ | Neither necessary nor sufficient |
| Lipschitz continuity for convergence | âœ… | âŒ | Required but not enough alone |

**Complete Proof: Gradient = 0 is Necessary but Not Sufficient**

```
==================================================================
THEOREM: For differentiable f, âˆ‡f(x*) = 0 is NECESSARY for x* 
         to be a local minimum.
==================================================================

PROOF (by contradiction):
----------------------------------------------------------------

1. ASSUME: x* is a local minimum AND âˆ‡f(x*) â‰  0

2. CONSTRUCT: Let d = -âˆ‡f(x*) (negative gradient direction)
   Since âˆ‡f(x*) â‰  0, we have d â‰  0

3. TAYLOR EXPANSION: For small Îµ > 0:
   f(x* + Îµd) = f(x*) + ÎµâŸ¨âˆ‡f(x*), dâŸ© + O(ÎµÂ²)
              = f(x*) + ÎµâŸ¨âˆ‡f(x*), -âˆ‡f(x*)âŸ© + O(ÎµÂ²)
              = f(x*) - Îµâ€–âˆ‡f(x*)â€–Â² + O(ÎµÂ²)

4. ANALYZE: For sufficiently small Îµ:
   The term -Îµâ€–âˆ‡f(x*)â€–Â² < 0 dominates O(ÎµÂ²)
   Therefore: f(x* + Îµd) < f(x*)

5. CONTRADICTION: This means we found a point x* + Îµd 
   with lower function value, contradicting that x* 
   is a local minimum.

6. CONCLUSION: âˆ‡f(x*) = 0 is necessary for local minimum âˆ

==================================================================
COUNTEREXAMPLE: âˆ‡f(x*) = 0 is NOT SUFFICIENT
==================================================================

Consider f(x) = xÂ³

âˆ‡f(x) = 3xÂ² 
âˆ‡f(0) = 0 âœ“ (satisfies necessary condition)

But f(x) = xÂ³ has:
- f(-Îµ) < 0 = f(0) for Îµ > 0
- f(+Îµ) > 0 = f(0) for Îµ > 0

Therefore x = 0 is an INFLECTION POINT, not a minimum!
The gradient being zero is not sufficient. âˆ
```

---

## ğŸ”¢ 2. Proof Techniques

> **ğŸ“ [Detailed Notes â†’](./02_proof_techniques/)**

### Decision Tree for Choosing Proof Technique

```
                    +-------------------------+
                    |   What to prove?        |
                    +-----------+-------------+
                                |
        +-----------------------+-----------------------+
        |                       |                       |
        â–¼                       â–¼                       â–¼
+---------------+     +-----------------+     +-----------------+
| P â†’ Q         |     | âˆ€n âˆˆ â„•: P(n)   |     | Â¬âˆƒ or unique    |
| (implication) |     | (for all n)    |     | (impossibility) |
+-------+-------+     +--------+--------+     +--------+--------+
        |                      |                       |
        â–¼                      â–¼                       â–¼
+---------------+     +-----------------+     +-----------------+
| DIRECT PROOF  |     |   INDUCTION     |     | CONTRADICTION   |
| Assume P,     |     | Base + Step     |     | Assume opposite |
| derive Q      |     |                 |     | find conflict   |
+---------------+     +-----------------+     +-----------------+
```

### 2.1 Direct Proof

**Structure:** Assume P â†’ Apply logical steps â†’ Conclude Q

**Example: Gradient Descent Convergence for Convex Functions**

```
==================================================================
THEOREM (Convergence Rate for Smooth Convex Functions):
For L-smooth convex f, gradient descent with step size Î· = 1/L:
    f(x_k) - f(x*) â‰¤ â€–x_0 - x*â€–Â²L / (2k)
==================================================================

PROOF:
----------------------------------------------------------------

Step 1: L-smoothness inequality
   f(y) â‰¤ f(x) + âŸ¨âˆ‡f(x), y-xâŸ© + (L/2)â€–y-xâ€–Â²

Step 2: Apply to GD update x_{k+1} = x_k - (1/L)âˆ‡f(x_k)
   Let y = x_{k+1} = x_k - (1/L)âˆ‡f(x_k)
   
   f(x_{k+1}) â‰¤ f(x_k) + âŸ¨âˆ‡f(x_k), -(1/L)âˆ‡f(x_k)âŸ© 
                       + (L/2)â€–(1/L)âˆ‡f(x_k)â€–Â²
             
             = f(x_k) - (1/L)â€–âˆ‡f(x_k)â€–Â² + (1/2L)â€–âˆ‡f(x_k)â€–Â²
             
             = f(x_k) - (1/2L)â€–âˆ‡f(x_k)â€–Â²

Step 3: Use convexity f(x*) â‰¥ f(x_k) + âŸ¨âˆ‡f(x_k), x* - x_kâŸ©
   Rearranging: f(x_k) - f(x*) â‰¤ âŸ¨âˆ‡f(x_k), x_k - x*âŸ©
   
   By Cauchy-Schwarz: â‰¤ â€–âˆ‡f(x_k)â€– Â· â€–x_k - x*â€–

Step 4: Combine and telescope
   After k iterations of summing and bounding:
   
   f(x_k) - f(x*) â‰¤ Lâ€–x_0 - x*â€–Â² / (2k)  âˆ
```

### 2.2 Proof by Contradiction

**Structure:** Assume Â¬Q â†’ Derive logical contradiction â†’ Conclude Q

**Example: âˆš2 is Irrational**

```
==================================================================
THEOREM: âˆš2 is irrational
==================================================================

PROOF (by contradiction):
----------------------------------------------------------------

1. ASSUME (for contradiction): âˆš2 is rational
   Then âˆš2 = p/q where p, q âˆˆ â„¤, q â‰  0, gcd(p,q) = 1

2. SQUARE both sides:
   2 = pÂ²/qÂ²
   pÂ² = 2qÂ²

3. ANALYZE p:
   pÂ² is even (since pÂ² = 2qÂ²)
   âŸ¹ p is even (since oddÂ² = odd)
   Let p = 2m for some integer m

4. SUBSTITUTE:
   (2m)Â² = 2qÂ²
   4mÂ² = 2qÂ²
   qÂ² = 2mÂ²

5. ANALYZE q:
   qÂ² is even (since qÂ² = 2mÂ²)
   âŸ¹ q is even

6. CONTRADICTION:
   Both p and q are even
   âŸ¹ gcd(p,q) â‰¥ 2
   But we assumed gcd(p,q) = 1  âš¡

7. CONCLUSION: âˆš2 is irrational âˆ
```

### 2.3 Mathematical Induction

**Structure:** Base case P(1) + [P(k) â†’ P(k+1)] âŸ¹ âˆ€n â‰¥ 1: P(n)

**Example: Backpropagation Correctness**

```
==================================================================
THEOREM: Backpropagation correctly computes âˆ‚L/âˆ‚w for all layers
==================================================================

PROOF (by strong induction on layer depth, from output to input):
----------------------------------------------------------------

Setup: L-layer network with activations a_l = Ïƒ(z_l), z_l = W_l a_{l-1} + b_l

BASE CASE (Layer L - output layer):
   âˆ‚L/âˆ‚W_L = âˆ‚L/âˆ‚a_L Â· âˆ‚a_L/âˆ‚z_L Â· âˆ‚z_L/âˆ‚W_L
           = Î´_L Â· a_{L-1}^T
   
   where Î´_L = âˆ‚L/âˆ‚z_L = âˆ‚L/âˆ‚a_L âŠ™ Ïƒ'(z_L)
   
   This is exactly what backprop computes âœ“

INDUCTIVE HYPOTHESIS:
   Assume backprop correctly computes gradients for layers l+1, ..., L

INDUCTIVE STEP (Layer l):
   By chain rule:
   âˆ‚L/âˆ‚W_l = âˆ‚L/âˆ‚z_l Â· âˆ‚z_l/âˆ‚W_l
           = Î´_l Â· a_{l-1}^T
   
   where Î´_l = âˆ‚L/âˆ‚z_l = (âˆ‚L/âˆ‚z_{l+1}) Â· (âˆ‚z_{l+1}/âˆ‚a_l) Â· (âˆ‚a_l/âˆ‚z_l)
             = (W_{l+1}^T Î´_{l+1}) âŠ™ Ïƒ'(z_l)
   
   By IH, Î´_{l+1} is correctly computed
   âŸ¹ Î´_l is correctly computed
   âŸ¹ âˆ‚L/âˆ‚W_l is correctly computed âœ“

CONCLUSION: By induction, backprop is correct for all layers âˆ
```

**Code Pattern - Induction in Algorithms:**

```python

# Recursive algorithms mirror induction proofs!

def factorial(n: int) -> int:
    """
    Correctness proof by induction:
    - Base: factorial(0) = 1 = 0! âœ“
    - Step: factorial(k+1) = (k+1) * factorial(k) 
                           = (k+1) * k!  [by IH]
                           = (k+1)! âœ“
    """
    if n == 0:
        return 1  # Base case
    return n * factorial(n - 1)  # Inductive step

def merge_sort(arr: list) -> list:
    """
    Correctness by strong induction on len(arr):
    - Base: len(arr) â‰¤ 1 â†’ already sorted âœ“
    - Step: Assume works for all arrays < k elements
            For array of k elements:
            - Split into two arrays < k elements
            - By IH, both halves sort correctly
            - Merge preserves sorted order âœ“
    """
    if len(arr) <= 1:
        return arr  # Base case
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])    # IH applies
    right = merge_sort(arr[mid:])   # IH applies
    return merge(left, right)       # Correct merge
```

---

## ğŸ“Š 3. Set Theory

> **ğŸ“ [Detailed Notes â†’](./03_set_theory/)**

### 3.1 Set Operations

| Operation | Notation | Definition | Python |
|-----------|----------|------------|--------|
| Union | A âˆª B | {x : x âˆˆ A or x âˆˆ B} | `A \| B` |
| Intersection | A âˆ© B | {x : x âˆˆ A and x âˆˆ B} | `A & B` |
| Difference | A \ B | {x : x âˆˆ A and x âˆ‰ B} | `A - B` |
| Symmetric Diff | A â–³ B | (A \ B) âˆª (B \ A) | `A ^ B` |
| Complement | Aá¶œ | {x âˆˆ U : x âˆ‰ A} | `U - A` |
| Power Set | P(A) | {S : S âŠ† A} | `itertools.combinations` |
| Cartesian Product | A Ã— B | {(a,b) : a âˆˆ A, b âˆˆ B} | `itertools.product` |

### 3.2 De Morgan's Laws - Complete Proof

```
==================================================================
THEOREM (De Morgan's Laws):
   (A âˆª B)á¶œ = Aá¶œ âˆ© Bá¶œ
   (A âˆ© B)á¶œ = Aá¶œ âˆª Bá¶œ
==================================================================

PROOF of (A âˆª B)á¶œ = Aá¶œ âˆ© Bá¶œ:
----------------------------------------------------------------

(âŠ†) Let x âˆˆ (A âˆª B)á¶œ
    âŸ¹ x âˆ‰ (A âˆª B)           [definition of complement]
    âŸ¹ Â¬(x âˆˆ A âˆ¨ x âˆˆ B)      [definition of union]
    âŸ¹ (x âˆ‰ A) âˆ§ (x âˆ‰ B)     [De Morgan's law for logic]
    âŸ¹ x âˆˆ Aá¶œ âˆ§ x âˆˆ Bá¶œ       [definition of complement]
    âŸ¹ x âˆˆ Aá¶œ âˆ© Bá¶œ           [definition of intersection]

(âŠ‡) Let x âˆˆ Aá¶œ âˆ© Bá¶œ
    âŸ¹ x âˆˆ Aá¶œ âˆ§ x âˆˆ Bá¶œ       [definition of intersection]
    âŸ¹ x âˆ‰ A âˆ§ x âˆ‰ B         [definition of complement]
    âŸ¹ Â¬(x âˆˆ A âˆ¨ x âˆˆ B)      [De Morgan's law for logic]
    âŸ¹ x âˆ‰ (A âˆª B)           [definition of union]
    âŸ¹ x âˆˆ (A âˆª B)á¶œ          [definition of complement]

Both inclusions proven âŸ¹ (A âˆª B)á¶œ = Aá¶œ âˆ© Bá¶œ âˆ
```

### 3.3 Functions - Formal Definitions

| Property | Definition | Test |
|----------|------------|------|
| **Injective** (1-1) | f(a) = f(b) âŸ¹ a = b | Different inputs â†’ different outputs |
| **Surjective** (onto) | âˆ€y âˆˆ Y, âˆƒx âˆˆ X: f(x) = y | Every y has a preimage |
| **Bijective** | Injective âˆ§ Surjective | 1-1 correspondence |

**ML Application - Activation Functions:**

```python
import numpy as np
import matplotlib.pyplot as plt

# ===============================================================
# SIGMOID: (0, 1) - NOT surjective onto â„
# ===============================================================
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Range is (0, 1), not all of â„
# NOT injective on extended domain if we consider limits

# ===============================================================
# ReLU: [0, âˆ) - NOT injective
# ===============================================================
def relu(x):
    return np.maximum(0, x)

# f(-1) = f(-2) = f(-100) = 0
# Many inputs map to same output!

# ===============================================================
# LEAKY ReLU: â„ â†’ â„ - BIJECTIVE!
# ===============================================================
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# Invertible: different inputs always give different outputs
# Covers all of â„ as output
```

### 3.4 Ïƒ-Algebras (Critical for Probability Theory)

```
==================================================================
DEFINITION: A Ïƒ-algebra F on set Î© is a collection of subsets such that:
   1. Î© âˆˆ F                    (contains the whole space)
   2. A âˆˆ F âŸ¹ Aá¶œ âˆˆ F           (closed under complement)  
   3. Aâ‚, Aâ‚‚, ... âˆˆ F âŸ¹ â‹ƒAáµ¢ âˆˆ F  (closed under countable union)
==================================================================

ML APPLICATION: Probability Space (Î©, F, P)
----------------------------------------------------------------

Example: Coin flip
   Î© = {H, T}                      # Sample space
   F = {âˆ…, {H}, {T}, {H,T}}        # Ïƒ-algebra (all subsets here)
   P: F â†’ [0,1]                    # Probability measure

Example: Image classification  
   Î© = {all possible 28Ã—28 images} # Sample space
   F = Borel Ïƒ-algebra on â„^784   # Generated by open sets
   P = data distribution           # Unknown, we sample from it
```

---

## ğŸ§  4. Logic

> **ğŸ“ [Detailed Notes â†’](./04_logic/)**

### 4.1 Propositional Logic

**Truth Tables:**

| P | Q | P âˆ§ Q | P âˆ¨ Q | P â†’ Q | P â†” Q | Â¬P |
|:-:|:-:|:-----:|:-----:|:-----:|:-----:|:--:|
| T | T |   T   |   T   |   T   |   T   | F  |
| T | F |   F   |   T   |   F   |   F   | F  |
| F | T |   F   |   T   |   T   |   F   | T  |
| F | F |   F   |   F   |   T   |   T   | T  |

**Key Logical Equivalences:**

```
===============================================================
IMPLICATION EQUIVALENCES:
   P â†’ Q  â‰¡  Â¬P âˆ¨ Q           (definition of implication)
   P â†’ Q  â‰¡  Â¬Q â†’ Â¬P          (contrapositive - EQUIVALENT!)
   P â†’ Q  â‰¢  Q â†’ P            (converse - NOT equivalent!)
   P â†’ Q  â‰¢  Â¬P â†’ Â¬Q          (inverse - NOT equivalent!)

DE MORGAN'S LAWS (Logic):
   Â¬(P âˆ§ Q) â‰¡ Â¬P âˆ¨ Â¬Q
   Â¬(P âˆ¨ Q) â‰¡ Â¬P âˆ§ Â¬Q

DISTRIBUTION:
   P âˆ§ (Q âˆ¨ R) â‰¡ (P âˆ§ Q) âˆ¨ (P âˆ§ R)
   P âˆ¨ (Q âˆ§ R) â‰¡ (P âˆ¨ Q) âˆ§ (P âˆ¨ R)
===============================================================
```

### 4.2 Predicate Logic & Quantifiers

**Quantifier Negation Rules:**

| Original | Negation |
|----------|----------|
| âˆ€x P(x) | âˆƒx Â¬P(x) |
| âˆƒx P(x) | âˆ€x Â¬P(x) |
| âˆ€xâˆƒy P(x,y) | âˆƒxâˆ€y Â¬P(x,y) |

**ML Example - Convergence Statements:**

```
===============================================================
STATEMENT: "SGD converges for all convex functions"

FORMAL: âˆ€f: [Convex(f) â†’ Converges(SGD, f)]

NEGATION: "There exists a convex function where SGD doesn't converge"
          âˆƒf: [Convex(f) âˆ§ Â¬Converges(SGD, f)]

===============================================================
STATEMENT: "For every Îµ > 0, there exists N such that for all n > N, |aâ‚™ - L| < Îµ"
           (Definition of limit)

FORMAL: âˆ€Îµ > 0, âˆƒN âˆˆ â„•, âˆ€n > N: |aâ‚™ - L| < Îµ

NEGATION: âˆƒÎµ > 0, âˆ€N âˆˆ â„•, âˆƒn > N: |aâ‚™ - L| â‰¥ Îµ
          "Sequence does NOT converge to L"
===============================================================
```

### 4.3 Rules of Inference

| Rule | Form | Name |
|------|------|------|
| P, P â†’ Q âŠ¢ Q | If P and P implies Q, then Q | Modus Ponens |
| Â¬Q, P â†’ Q âŠ¢ Â¬P | If not Q and P implies Q, then not P | Modus Tollens |
| P â†’ Q, Q â†’ R âŠ¢ P â†’ R | Chain implications | Hypothetical Syllogism |
| P âˆ¨ Q, Â¬P âŠ¢ Q | If P or Q and not P, then Q | Disjunctive Syllogism |

---

## â±ï¸ 5. Asymptotic Analysis

> **ğŸ“ [Detailed Notes â†’](./05_asymptotic_analysis/)**

### 5.1 Formal Definitions

```
===============================================================
BIG-O (Upper Bound):
   f(n) = O(g(n)) âŸº âˆƒc > 0, nâ‚€: âˆ€n â‰¥ nâ‚€, f(n) â‰¤ cÂ·g(n)
   
BIG-OMEGA (Lower Bound):
   f(n) = Î©(g(n)) âŸº âˆƒc > 0, nâ‚€: âˆ€n â‰¥ nâ‚€, f(n) â‰¥ cÂ·g(n)
   
BIG-THETA (Tight Bound):
   f(n) = Î˜(g(n)) âŸº f(n) = O(g(n)) âˆ§ f(n) = Î©(g(n))
   
LITTLE-O (Strict Upper Bound):
   f(n) = o(g(n)) âŸº lim_{nâ†’âˆ} f(n)/g(n) = 0
===============================================================
```

### 5.2 Complexity Hierarchy

```
Fastest â†----------------------------------------------â†’ Slowest

O(1) < O(log n) < O(âˆšn) < O(n) < O(n log n) < O(nÂ²) < O(nÂ³) < O(2â¿) < O(n!)
 |        |         |       |        |          |        |        |       |
 |        |         |       |        |          |        |        |       + Brute force
 |        |         |       |        |          |        |        + Subset enumeration
 |        |         |       |        |          |        + Matrix multiplication
 |        |         |       |        |          + Naive attention O(nÂ²d)
 |        |         |       |        + Merge sort, FFT
 |        |         |       + Linear scan
 |        |         + Meet in the middle
 |        + Binary search, tree operations
 + Hash table lookup
```

### 5.3 ML Model Complexities

| Model | Training | Inference | Space |
|-------|----------|-----------|-------|
| Linear Regression | O(ndÂ² + dÂ³) | O(d) | O(dÂ²) |
| k-NN | O(1) | O(nd) | O(nd) |
| Decision Tree | O(nd log n) | O(log n) | O(nodes) |
| Random Forest | O(kÂ·nd log n) | O(k log n) | O(kÂ·nodes) |
| Naive Bayes | O(nd) | O(d) | O(d) |
| SVM (kernel) | O(nÂ²d) - O(nÂ³) | O(svÂ·d) | O(nÂ²) |
| Transformer | O(nÂ²d) | O(nÂ²d) | O(nÂ² + nd) |
| Flash Attention | O(nÂ²d) | O(nÂ²d) | O(n) memory! |

**Proof: 3nÂ² + 2n + 1 = O(nÂ²)**

```
==================================================================
CLAIM: 3nÂ² + 2n + 1 = O(nÂ²)
==================================================================

PROOF:
We need to find c > 0 and nâ‚€ such that:
   3nÂ² + 2n + 1 â‰¤ cÂ·nÂ² for all n â‰¥ nâ‚€

For n â‰¥ 1:
   â€¢ 2n â‰¤ 2nÂ²    (since n â‰¤ nÂ² for n â‰¥ 1)
   â€¢ 1 â‰¤ nÂ²      (since 1 â‰¤ nÂ² for n â‰¥ 1)

Therefore:
   3nÂ² + 2n + 1 â‰¤ 3nÂ² + 2nÂ² + nÂ²
                = 6nÂ²

Choose c = 6, nâ‚€ = 1

Verification: For all n â‰¥ 1:
   3nÂ² + 2n + 1 â‰¤ 6nÂ² âœ“

Therefore, 3nÂ² + 2n + 1 = O(nÂ²) âˆ
```

---

## ğŸ’» 6. Numerical Computation

> **ğŸ“ [Detailed Notes â†’](./06_numerical_computation/)**

### 6.1 IEEE 754 Floating Point

```
+-----------------------------------------------------------------------------+
|                         IEEE 754 FLOATING POINT                              |
+-----------------------------------------------------------------------------+
|                                                                              |
|  32-bit (float32):                                                          |
|  +---+----------+-----------------------------------+                       |
|  | S | Exponent |           Mantissa                |                       |
|  | 1 |    8     |             23                    |                       |
|  +---+----------+-----------------------------------+                       |
|                                                                              |
|  64-bit (float64):                                                          |
|  +---+-------------+----------------------------------------------------+   |
|  | S |  Exponent   |                    Mantissa                        |   |
|  | 1 |     11      |                      52                            |   |
|  +---+-------------+----------------------------------------------------+   |
|                                                                              |
|  16-bit (float16 / half):                                                   |
|  +---+-------+----------------+                                             |
|  | S |  Exp  |    Mantissa    |                                             |
|  | 1 |   5   |       10       |                                             |
|  +---+-------+----------------+                                             |
|                                                                              |
|  Value = (-1)^S Ã— 2^(Exp - bias) Ã— (1 + Mantissa/2^bits)                    |
|                                                                              |
|  Bias: float16=15, float32=127, float64=1023                                |
+-----------------------------------------------------------------------------+
```

**Key Numbers:**

| Type | Min Positive | Max | Epsilon | Decimal Digits |
|------|-------------|-----|---------|----------------|
| float16 | 6.1e-5 | 6.5e4 | 9.77e-4 | ~3 |
| float32 | 1.2e-38 | 3.4e38 | 1.19e-7 | ~7 |
| float64 | 2.2e-308 | 1.8e308 | 2.22e-16 | ~15 |

### 6.2 Numerical Stability Issues & Solutions

**Problem 1: Softmax Overflow**

```python
import numpy as np
import torch

# ===============================================================
# PROBLEM: Overflow in naive softmax
# ===============================================================
def softmax_unstable(x):
    exp_x = np.exp(x)  # Can overflow for large x!
    return exp_x / np.sum(exp_x)

x = np.array([1000, 1001, 1002])
print(softmax_unstable(x))  # [nan, nan, nan] - BROKEN!

# ===============================================================
# SOLUTION: Subtract max (mathematically equivalent)
# ===============================================================
def softmax_stable(x):
    x_shifted = x - np.max(x)  # Shift to prevent overflow
    exp_x = np.exp(x_shifted)
    return exp_x / np.sum(exp_x)

print(softmax_stable(x))  # [0.09, 0.24, 0.67] - Correct!

# PROOF OF EQUIVALENCE:
# exp(x - max) / Î£exp(x - max) = exp(x)/exp(max) / (Î£exp(x)/exp(max))
#                              = exp(x) / Î£exp(x) âœ“
```

**Problem 2: Log-Sum-Exp Underflow**

```python

# ===============================================================
# PROBLEM: Underflow in naive log-sum-exp
# ===============================================================
def logsumexp_unstable(x):
    return np.log(np.sum(np.exp(x)))

x = np.array([-1000, -1001, -1002])
print(logsumexp_unstable(x))  # -inf (underflow!)

# ===============================================================
# SOLUTION: Factor out max
# ===============================================================
def logsumexp_stable(x):
    c = np.max(x)
    return c + np.log(np.sum(np.exp(x - c)))

print(logsumexp_stable(x))  # -999.59 - Correct!

# PROOF:
# log(Î£exp(xáµ¢)) = log(Î£exp(xáµ¢ - c + c))
#               = log(exp(c) Â· Î£exp(xáµ¢ - c))
#               = c + log(Î£exp(xáµ¢ - c)) âœ“
```

**Problem 3: Catastrophic Cancellation**

```python

# ===============================================================
# PROBLEM: Loss of precision in variance calculation
# ===============================================================
def variance_unstable(x):
    """Two-pass formula: Var = E[XÂ²] - E[X]Â²"""
    n = len(x)
    mean_sq = np.sum(x**2) / n
    sq_mean = (np.sum(x) / n)**2
    return mean_sq - sq_mean  # Subtracting similar large numbers!

# ===============================================================
# SOLUTION: Welford's online algorithm
# ===============================================================
def variance_welford(x):
    """Single-pass numerically stable algorithm"""
    n = 0
    mean = 0
    M2 = 0
    
    for xi in x:
        n += 1
        delta = xi - mean
        mean += delta / n
        delta2 = xi - mean
        M2 += delta * delta2
    
    return M2 / n if n > 0 else 0
```

### 6.3 Mixed Precision Training

```python
import torch
from torch.cuda.amp import autocast, GradScaler

# ===============================================================
# MIXED PRECISION TRAINING
# Forward: FP16 (fast, memory efficient)
# Master weights & gradients: FP32 (precision)
# ===============================================================

model = MyModel().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
scaler = GradScaler()  # Scales gradients to prevent FP16 underflow

for batch_idx, (data, target) in enumerate(dataloader):
    data, target = data.cuda(), target.cuda()
    optimizer.zero_grad()
    
    # Forward pass in FP16
    with autocast():
        output = model(data)
        loss = F.cross_entropy(output, target)
    
    # Backward pass with gradient scaling
    scaler.scale(loss).backward()
    
    # Unscale gradients, clip, then step
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    scaler.step(optimizer)
    scaler.update()
```

---

## ğŸ“ Key Formulas Summary

| Category | Formula | Description |
|----------|---------|-------------|
| **Induction** | P(1) âˆ§ [P(k)â†’P(k+1)] âŸ¹ âˆ€n P(n) | Mathematical induction principle |
| **De Morgan (Sets)** | (AâˆªB)á¶œ = Aá¶œâˆ©Bá¶œ | Complement of union |
| **De Morgan (Logic)** | Â¬(Pâˆ§Q) â‰¡ Â¬Pâˆ¨Â¬Q | Negation of conjunction |
| **Contrapositive** | (Pâ†’Q) â‰¡ (Â¬Qâ†’Â¬P) | Equivalent form of implication |
| **Big-O** | f = O(g) âŸº âˆƒc,nâ‚€: f(n) â‰¤ cg(n) âˆ€nâ‰¥nâ‚€ | Asymptotic upper bound |
| **Big-Î˜** | f = Î˜(g) âŸº f = O(g) âˆ§ f = Î©(g) | Asymptotic tight bound |
| **Softmax** | Ïƒ(x)áµ¢ = exp(xáµ¢)/Î£â±¼exp(xâ±¼) | Probability distribution |
| **Z-score** | z = (x-Î¼)/Ïƒ | Standardization |
| **Condition Number** | Îº(A) = â€–Aâ€–Â·â€–Aâ»Â¹â€– | Sensitivity to perturbation |
| **Machine Epsilon** | Îµ = min{x > 0 : 1 + x â‰  1} | Smallest representable difference |

---

## âš ï¸ Common Mistakes

| Mistake | Correct Understanding |
|---------|----------------------|
| "âˆ‡f=0 means minimum" | Only necessary, not sufficient (could be saddle point) |
| "Pâ†’Q means P causes Q" | Implication â‰  causation |
| "Converse equals original" | Pâ†’Q â‰¢ Qâ†’P |
| "O(nÂ²) is always slow" | Depends on constants and n; O(nÂ²) with small c beats O(n) with huge c |
| "float == for equality" | Use `abs(a-b) < Îµ` for floating point comparison |
| "More data = better model" | Diminishing returns; quality and diversity matter |
| "FP16 always works" | Need gradient scaling; some ops need FP32 |

---

## ğŸ“š Resources

### Books
| Title | Author | Topic |
|-------|--------|-------|
| How to Prove It | Velleman | Proof techniques |
| Discrete Mathematics and Its Applications | Rosen | Logic, sets, proofs |
| Introduction to Algorithms (CLRS) | Cormen et al. | Complexity analysis |
| Numerical Linear Algebra | Trefethen & Bau | Numerical stability |

### Online Courses
| Course | Platform | Link |
|--------|----------|------|
| MIT 6.042J Mathematics for CS | MIT OCW | [Link](https://ocw.mit.edu/6-042J) |
| Introduction to Proofs | Coursera | [Link](https://www.coursera.org/learn/proofs) |
| 3Blue1Brown Essence Series | YouTube | [Link](https://youtube.com/c/3blue1brown) |

---

## ğŸ§­ Navigation

### Section Navigation (Within This Series)

<table width="100%">
<tr>
<td align="left" width="33%">

â¬…ï¸ **Previous**<br>
[ğŸ  Main Course](../README.md)

</td>
<td align="center" width="34%">

ğŸ“ **Current: Section 1**<br>
**ğŸ“ Mathematical Foundations**

</td>
<td align="right" width="33%">

â¡ï¸ **Next Section**<br>
[ğŸ“Š 02: Mathematics](../02_mathematics/README.md)

</td>
</tr>
</table>

### Sub-Topics in This Section

| # | Topic | Direct Link |
|:-:|-------|:-----------:|
| 1 | Mathematical Thinking | [ğŸ“ Open](./01_mathematical_thinking/README.md) |
| 2 | Proof Techniques | [ğŸ“ Open](./02_proof_techniques/README.md) |
| 3 | Set Theory | [ğŸ“ Open](./03_set_theory/README.md) |
| 4 | Logic | [ğŸ“ Open](./04_logic/README.md) |
| 5 | Asymptotic Analysis | [ğŸ“ Open](./05_asymptotic_analysis/README.md) |
| 6 | Numerical Computation | [ğŸ“ Open](./06_numerical_computation/README.md) |

### Full Course Navigation

| Section | Topic | Link |
|:-------:|-------|:----:|
| 01 | **Mathematical Foundations** â† You are here | â€” |
| 02 | Mathematics (Linear Algebra, Calculus) | [Go â†’](../02_mathematics/README.md) |
| 03 | Probability & Statistics | [Go â†’](../03_probability_statistics/README.md) |
| 04 | Optimization | [Go â†’](../04_optimization/README.md) |
| 05 | ML Theory | [Go â†’](../05_ml_theory/README.md) |
| 06 | Deep Learning | [Go â†’](../06_deep_learning/README.md) |
| 07 | Reinforcement Learning | [Go â†’](../07_reinforcement_learning/README.md) |
| 08 | Model Compression | [Go â†’](../08_model_compression/README.md) |
| 09 | Efficient ML | [Go â†’](../09_efficient_ml/README.md) |

---

## ğŸ“ Section Structure

```
01_foundations/
+-- README.md                      â† You are here (Overview)
|
+-- 01_mathematical_thinking/
|   +-- README.md                  # Abstraction, necessary vs sufficient
|   +-- images/
|
+-- 02_proof_techniques/
|   +-- README.md                  # Direct, contradiction, induction
|   +-- images/
|
+-- 03_set_theory/
|   +-- README.md                  # Sets, functions, relations, Ïƒ-algebras
|   +-- images/
|
+-- 04_logic/
|   +-- README.md                  # Propositional, predicate, inference
|   +-- images/
|
+-- 05_asymptotic_analysis/
|   +-- README.md                  # Big-O, Î©, Î˜, little-o analysis
|   +-- images/
|
+-- 06_numerical_computation/
|   +-- README.md                  # Floating point, stability, mixed precision
|   +-- images/
|
+-- images/                        # Shared images
```

---

## âœ… Learning Checklist

- [ ] Can identify necessary vs sufficient conditions in ML contexts
- [ ] Can construct direct proofs, proofs by contradiction, and induction
- [ ] Understand set operations and their notation
- [ ] Can negate quantified statements correctly
- [ ] Can analyze algorithm complexity using Big-O notation
- [ ] Understand floating-point representation and common pitfalls
- [ ] Can implement numerically stable versions of common operations

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer" width="100%"/>
</p>
