<!-- Animated Header -->
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=700&size=40&pause=1000&color=00D4AA&center=true&vCenter=true&width=800&lines=â±ï¸+Asymptotic+Analysis;Big-O+and+Algorithm+Complexity" alt="Asymptotic Analysis" />
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=18,19,20&height=180&section=header&text=Asymptotic%20Analysis&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=32&desc=Big-O%20â€¢%20Big-Î©%20â€¢%20Big-Î˜%20â€¢%20Complexity%20Classes&descAlignY=52&descSize=18" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-05_of_06-00D4AA?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Topics-6_Concepts-FF6B6B?style=for-the-badge&logo=buffer&logoColor=white" alt="Topics"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-6C63FF?style=for-the-badge&logo=github&logoColor=white" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-4ECDC4?style=for-the-badge&logo=calendar&logoColor=white" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

**ğŸ  [Home](../README.md)** Â· **ğŸ“š Series:** [Mathematical Thinking](../01_mathematical_thinking/README.md) â†’ [Proof Techniques](../02_proof_techniques/README.md) â†’ [Set Theory](../03_set_theory/README.md) â†’ [Logic](../04_logic/README.md) â†’ Asymptotic Analysis â†’ [Numerical Computation](../06_numerical_computation/README.md)

---

## ğŸ“Œ TL;DR

Asymptotic analysis tells you **how algorithms scale**. Essential for ML model selection:

- **Big-O (O)** â€” Upper bound on growth: "at most this fast"
- **Big-Omega (Î©)** â€” Lower bound: "at least this fast"  
- **Big-Theta (Î˜)** â€” Tight bound: "exactly this fast"
- **little-o (o)** â€” Strictly slower growth

> [!IMPORTANT]
> **Transformers are O(nÂ²) in sequence length!**
> - n=512: 262K ops | n=8192: 67M ops (256Ã— more!)
> - This is why Flash Attention and linear attention are hot research areas.

---

## ğŸ“š What You'll Learn

- [ ] Define and prove Big-O, Big-Î©, Big-Î˜ bounds formally
- [ ] Analyze time and space complexity of algorithms
- [ ] Understand ML model complexities (Transformers, CNNs, RNNs)
- [ ] Apply Master Theorem for recursive algorithms
- [ ] Compare complexity classes and their implications

---

## ğŸ“‘ Table of Contents

1. [Complexity Hierarchy](#-complexity-hierarchy)
2. [Big-O Notation](#1-big-o-notation)
3. [Big-Omega and Big-Theta](#2-big-omega-and-big-theta)
4. [Little-o and Little-omega](#3-little-o-and-little-omega)
5. [ML Model Complexities](#4-ml-model-complexities)
6. [Master Theorem](#5-master-theorem)
7. [Key Formulas Summary](#-key-formulas-summary)
8. [Common Mistakes](#-common-mistakes--pitfalls)
9. [Code Implementations](#-code-implementations)
10. [Resources](#-resources)

---

## ğŸ“Š Complexity Hierarchy

```
FAST â—€--------------------------------------------------------â–¶ SLOW/AVOID

O(1)   O(log n)   O(n)   O(n log n)   O(nÂ²)   O(nÂ³)   O(2â¿)   O(n!)
 |         |        |         |          |        |        |       |
Hash    Binary   Array    Merge      Attention  MatMul  Brute   Perms
lookup  search   scan     sort       (naive)    (naive)  force
```

| Complexity | Name | n=1000 | Example |
|:----------:|:-----|:------:|:--------|
| O(1) | Constant | 1 | Hash lookup, array access |
| O(log n) | Logarithmic | 10 | Binary search |
| O(n) | Linear | 1,000 | Array scan, softmax |
| O(n log n) | Linearithmic | 10,000 | Merge sort, FFT |
| O(nÂ²) | Quadratic | 1,000,000 | **Transformer attention** |
| O(nÂ³) | Cubic | 10â¹ | Naive matrix multiply |
| O(2â¿) | Exponential | âˆ | Subset enumeration |
| O(n!) | Factorial | âˆ | All permutations |

---

## 1. Big-O Notation

### ğŸ“– Definition

> **Big-O:** f(n) = O(g(n)) means there exist constants c > 0 and nâ‚€ such that:
> $$
\forall n \geq n_0: f(n) \leq c \cdot g(n)

```math
> 
> *"f grows at most as fast as g"*

### ğŸ“ Formal Proof Template

To prove f(n) = O(g(n)):
1. Find constants c > 0 and nâ‚€
2. Show f(n) â‰¤ cÂ·g(n) for all n â‰¥ nâ‚€

### ğŸ“ Example 1: Prove 3nÂ² + 5n + 2 = O(nÂ²)

| Step | Statement | Justification |
|:----:|:----------|:--------------|
| 1 | f(n) = 3nÂ² + 5n + 2 | Given |
| 2 | For n â‰¥ 1: n â‰¤ nÂ² and 1 â‰¤ nÂ² | Basic inequality |
| 3 | 3nÂ² + 5n + 2 â‰¤ 3nÂ² + 5nÂ² + 2nÂ² | Substitution |
| 4 | = 10nÂ² | Simplify |
| 5 | Choose c = 10, nâ‚€ = 1 | Constants |
| 6 | f(n) â‰¤ 10Â·nÂ² for all n â‰¥ 1 | âˆ |

### ğŸ“ Example 2: Prove log(n!) = O(n log n)

| Step | Statement | Justification |
|:----:|:----------|:--------------|
| 1 | log(n!) = log(1Â·2Â·3Â·...Â·n) = Î£áµ¢ log(i) | Definition |
| 2 | Î£áµ¢â‚Œâ‚â¿ log(i) â‰¤ Î£áµ¢â‚Œâ‚â¿ log(n) = nÂ·log(n) | log(i) â‰¤ log(n) |
| 3 | So log(n!) â‰¤ nÂ·log(n) | |
| 4 | log(n!) = O(n log n) with c=1, nâ‚€=1 | âˆ |

### ğŸ“ Example 3: Big-O Rules

| Rule | Statement |
|:-----|:----------|
| **Sum** | O(f) + O(g) = O(max(f,g)) |
| **Product** | O(f) Â· O(g) = O(fÂ·g) |
| **Constants** | cÂ·O(f) = O(f) |
| **Transitivity** | f = O(g), g = O(h) âŸ¹ f = O(h) |

---

## 2. Big-Omega and Big-Theta

### ğŸ“– Definitions

> **Big-Omega (Î©):** f(n) = Î©(g(n)) means there exist c > 0 and nâ‚€ such that:
> $$
\forall n \geq n_0: f(n) \geq c \cdot g(n)
```

> *"f grows at least as fast as g"*

> **Big-Theta (Î˜):** f(n) = Î˜(g(n)) iff f(n) = O(g(n)) AND f(n) = Î©(g(n))
> *"f grows exactly as fast as g"*

### ğŸ“ Proof: 3nÂ² + 5n = Î˜(nÂ²)

**Upper bound (O):** Shown in Example 1: 3nÂ² + 5n â‰¤ 10nÂ² for n â‰¥ 1

**Lower bound (Î©):**

| Step | Statement | Justification |
|:----:|:----------|:--------------|
| 1 | 3nÂ² + 5n â‰¥ 3nÂ² | 5n â‰¥ 0 for n â‰¥ 0 |
| 2 | 3nÂ² + 5n â‰¥ 3Â·nÂ² | |
| 3 | Choose c = 3, nâ‚€ = 0 | |
| 4 | f(n) = Î©(nÂ²) | âˆ |

**Conclusion:** Since f(n) = O(nÂ²) and f(n) = Î©(nÂ²), we have f(n) = Î˜(nÂ²) âˆ

### ğŸ“Š Comparison Table

| Notation | Meaning | Analogy |
|:--------:|:--------|:--------|
| f = O(g) | f â‰¤ cÂ·g eventually | f â‰¤ g |
| f = Î©(g) | f â‰¥ cÂ·g eventually | f â‰¥ g |
| f = Î˜(g) | Both O and Î© | f = g (same order) |
| f = o(g) | f/g â†’ 0 | f < g (strictly slower) |
| f = Ï‰(g) | f/g â†’ âˆ | f > g (strictly faster) |

---

## 3. Little-o and Little-omega

### ğŸ“– Definition

> **Little-o:** f(n) = o(g(n)) means:
> $$
\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0

```math
> *"f grows strictly slower than g"*

### ğŸ“ Examples
```

n = o(nÂ²)           # n grows slower than nÂ²
log n = o(n)        # log grows slower than linear
nÂ² = o(2â¿)          # polynomial < exponential
n^k = o(n^{k+1})    # lower power < higher power
```

### ğŸ“ Proof: n = o(nÂ²)

| Step | Statement | Justification |
|:----:|:----------|:--------------|
| 1 | Consider lim_{nâ†’âˆ} n/nÂ² | Definition |
| 2 | = lim_{nâ†’âˆ} 1/n | Simplify |
| 3 | = 0 | Limit |
| 4 | n = o(nÂ²) | âˆ |

### ğŸ“– Growth Rate Hierarchy

```
1 â‰º log log n â‰º log n â‰º n^Îµ â‰º n â‰º n log n â‰º nÂ² â‰º ... â‰º 2^n â‰º n!

Where f â‰º g means f = o(g) (f grows strictly slower)
```

---

## 4. ML Model Complexities

### ğŸ“Š Model Comparison

| Model | Time Complexity | Space Complexity |
|:------|:---------------:|:----------------:|
| **Transformer Attention** | O(nÂ²d) | O(nÂ²) or O(n) |
| **Flash Attention** | O(nÂ²d) | O(n) ğŸ”¥ |
| **Linear Attention** | O(ndÂ²) | O(nd) |
| **CNN (per layer)** | O(kÂ²Â·cÂ·hÂ·w) | O(cÂ·hÂ·w) |
| **RNN (per step)** | O(hÂ²) | O(h) |
| **GNN (message passing)** | O(\|E\|Â·d) | O(\|V\|Â·d) |

### ğŸ“ Transformer Attention Derivation

```
Attention(Q, K, V) = softmax(QK^T / âˆšd) V

Where: Q, K, V âˆˆ â„^{nÃ—d}

Step 1: QK^T                    â†’ O(nÂ²d) time, O(nÂ²) space for scores
Step 2: softmax(Â·/âˆšd)           â†’ O(nÂ²) time
Step 3: Multiply by V           â†’ O(nÂ²d) time

Total: O(nÂ²d) time, O(nÂ²) space (for attention matrix)
```

### ğŸ“Š Scaling Impact

| Sequence Length n | O(nÂ²) Operations | Relative |
|:-----------------:|:----------------:|:--------:|
| 512 | 262,144 | 1Ã— |
| 2,048 | 4,194,304 | 16Ã— |
| 8,192 | 67,108,864 | **256Ã—** |
| 32,768 | 1,073,741,824 | **4,096Ã—** |

> [!IMPORTANT]
> This is why GPT-3 had 2K context, and why Flash Attention, Linear Attention, and sparse attention are critical research areas!

---

## 5. Master Theorem

### ğŸ“– Definition

For recurrences of the form: T(n) = aT(n/b) + f(n)

| Case | Condition | Result |
|:----:|:----------|:-------|
| 1 | f(n) = O(n^{log_b(a) - Îµ}) | T(n) = Î˜(n^{log_b(a)}) |
| 2 | f(n) = Î˜(n^{log_b(a)}) | T(n) = Î˜(n^{log_b(a)} log n) |
| 3 | f(n) = Î©(n^{log_b(a) + Îµ}) | T(n) = Î˜(f(n)) |

### ğŸ“ Example: Merge Sort

```
T(n) = 2T(n/2) + O(n)

a = 2, b = 2, f(n) = O(n)
log_b(a) = log_2(2) = 1
n^{log_b(a)} = n^1 = n
f(n) = O(n) = Î˜(n^1)

Case 2 applies: T(n) = Î˜(n log n)
```

### ğŸ“ Example: Binary Search

```
T(n) = T(n/2) + O(1)

a = 1, b = 2, f(n) = O(1)
log_b(a) = log_2(1) = 0
n^0 = 1
f(n) = Î˜(1) = Î˜(n^0)

Case 2 applies: T(n) = Î˜(log n)
```

---

## ğŸ“Š Key Formulas Summary

| Notation | Definition | Intuition |
|:---------|:-----------|:----------|
| f = O(g) | âˆƒc,nâ‚€: f(n) â‰¤ cÂ·g(n) âˆ€nâ‰¥nâ‚€ | At most g |
| f = Î©(g) | âˆƒc,nâ‚€: f(n) â‰¥ cÂ·g(n) âˆ€nâ‰¥nâ‚€ | At least g |
| f = Î˜(g) | O(g) âˆ§ Î©(g) | Exactly g |
| f = o(g) | lim f(n)/g(n) = 0 | Strictly < g |
| f = Ï‰(g) | lim f(n)/g(n) = âˆ | Strictly > g |

---

## âš ï¸ Common Mistakes & Pitfalls

### Mistake 1: Dropping Constants Incorrectly

```
âŒ WRONG: O(2n) = O(n) means 2n â‰ˆ n

âœ… RIGHT: O(2n) = O(n) means they have SAME GROWTH RATE
          In practice, 2n is still 2Ã— slower than n!
```

### Mistake 2: Comparing Different Variables

```
âŒ WRONG: O(nÂ²) is always worse than O(n)

âœ… RIGHT: Depends on what n means!
          O(nÂ²) in sequence length vs O(n) in dimension
          might have very different implications
```

### Mistake 3: Ignoring Space Complexity

```
âŒ WRONG: Focus only on time complexity

âœ… RIGHT: GPU memory is limited!
          O(nÂ²) space for attention means 8K sequence on 40GB A100
```

---

## ğŸ’» Code Implementations

```python
import numpy as np
import time
from typing import Callable

def measure_complexity(f: Callable, sizes: list, name: str):
    """Empirically measure algorithm complexity."""
    times = []
    for n in sizes:
        start = time.perf_counter()
        f(n)
        elapsed = time.perf_counter() - start
        times.append(elapsed)
    
    print(f"\n{name}:")
    print(f"{'n':>10} | {'Time (ms)':>12} | {'Ratio':>8}")
    print("-" * 35)
    for i, (n, t) in enumerate(zip(sizes, times)):
        ratio = times[i] / times[0] if i > 0 else 1.0
        print(f"{n:>10} | {t*1000:>12.3f} | {ratio:>8.2f}")

# O(n) - Linear
def linear_example(n):
    return sum(range(n))

# O(nÂ²) - Quadratic (like attention)
def quadratic_example(n):
    total = 0
    for i in range(n):
        for j in range(n):
            total += i + j
    return total

# O(n log n) - Like merge sort
def nlogn_example(n):
    arr = np.random.rand(n)
    return np.sort(arr)

# O(log n) - Binary search
def logn_example(n):
    arr = list(range(n))
    target = n // 2
    lo, hi = 0, n - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1

# Measure
sizes = [100, 200, 400, 800, 1600]
measure_complexity(linear_example, sizes, "O(n) - Linear")
measure_complexity(quadratic_example, sizes, "O(nÂ²) - Quadratic")
measure_complexity(nlogn_example, sizes, "O(n log n)")
```

---

## ğŸ“š Resources

| Type | Title | Link |
|:-----|:------|:-----|
| ğŸ“– Book | Introduction to Algorithms (CLRS) | Chapter 3 |
| ğŸ¥ Video | MIT 6.006 - Algorithm Analysis | [YouTube](https://www.youtube.com/watch?v=HtSuA80QTyo) |
| ğŸ“„ Paper | Attention Is All You Need | [arXiv](https://arxiv.org/abs/1706.03762) |
| ğŸ“„ Paper | FlashAttention | [arXiv](https://arxiv.org/abs/2205.14135) |

---

## ğŸ§­ Navigation

<table width="100%">
<tr>
<td align="left" width="33%">

â¬…ï¸ **Previous**<br>
[ğŸ”€ Logic](../04_logic/README.md)

</td>
<td align="center" width="34%">

ğŸ“ **Current: 5 of 6**<br>
**â±ï¸ Asymptotic Analysis**

</td>
<td align="right" width="33%">

â¡ï¸ **Next**<br>
[ğŸ”¢ Numerical Computation](../06_numerical_computation/README.md)

</td>
</tr>
</table>

### Quick Links

| Direction | Destination |
|:---------:|-------------|
| ğŸ  Section Home | [01: Mathematical Foundations](../README.md) |
| ğŸ“‹ Full Course | [Course Home](../../README.md) |

---

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=18,19,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>

<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=18&pause=1000&color=00D4AA&center=true&vCenter=true&width=600&lines=Made+with+â¤ï¸+by+Gaurav+Goswami" alt="Footer" />
</p>

```