<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=9C27B0&height=120&section=header&text=KL%20Divergence&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-03-9C27B0?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/kl-divergence.svg" width="100%">

*Caption: KL(P||Q) = Î£ P(x) log(P(x)/Q(x)) measures how Q differs from P. Non-symmetric: KL(P||Q) â‰  KL(Q||P). Zero iff P=Q. Appears in VAE loss, variational inference, and information-theoretic bounds.*

---

## ğŸ“‚ Overview

KL divergence is the standard measure of distribution similarity in ML. It's the extra bits needed to encode P using Q's code, and drives many modern generative models.

---

## ğŸ“ Mathematical Definition

### KL Divergence Formula

```
Discrete:
    D_KL(P || Q) = Î£_x P(x) log(P(x) / Q(x))
                 = Î£_x P(x) log P(x) - Î£_x P(x) log Q(x)
                 = -H(P) + H(P, Q)
                 = H(P, Q) - H(P)
                 
Continuous:
    D_KL(P || Q) = âˆ« p(x) log(p(x) / q(x)) dx
```

### Key Properties

```
1. Non-negativity: D_KL(P || Q) â‰¥ 0  (Gibbs' inequality)
2. Zero iff equal: D_KL(P || Q) = 0 âŸº P = Q
3. Non-symmetric: D_KL(P || Q) â‰  D_KL(Q || P) generally
4. Not a metric: Doesn't satisfy triangle inequality
```

### Interpretation

```
D_KL(P || Q) = Extra bits needed to encode samples from P
               using code optimized for Q
               
If Q is a good approximation of P â†’ low KL
If Q is poor approximation â†’ high KL
```

---

## ğŸ“Š Forward vs Reverse KL

| Aspect | D_KL(P \|\| Q) | D_KL(Q \|\| P) |
|--------|---------------|---------------|
| Name | Forward KL | Reverse KL |
| Used in | Variational inference | Expectation propagation |
| Behavior | Zero-forcing | Zero-avoiding |
| Q fits | All of P's modes | Some of P's modes |
| When Q too narrow | High penalty | Low penalty |

```
True distribution P is bimodal:

Forward KL (D_KL(P||Q)):
    Q spreads to cover both modes (moment matching)
    May place mass between modes
    
    P: â‹€  â‹€     Q: ___/â€¾â€¾â€¾\___
       Mode matching
       
Reverse KL (D_KL(Q||P)):
    Q focuses on one mode (mode seeking)
    Avoids regions where P(x)=0
    
    P: â‹€  â‹€     Q: â‹€
       One mode only
```

---

## ğŸŒ ML Applications

### 1. VAE (Variational Autoencoder)

```
ELBO = E_q[log p(x|z)] - D_KL(q(z|x) || p(z))
                          +---- Regularization ----+

Push q(z|x) towards prior p(z) = N(0, I)
```

### 2. Variational Inference

```
Approximate posterior p(Î¸|D) with q(Î¸):

q* = argmin_q D_KL(q(Î¸) || p(Î¸|D))
   = argmax_q ELBO(q)
```

### 3. Knowledge Distillation

```
Student learns from teacher:

L_distill = D_KL(p_teacher || p_student)

Soft targets transfer "dark knowledge"
```

### 4. Policy Optimization (RL)

```
TRPO/PPO constraint:
    D_KL(Ï€_old || Ï€_new) â‰¤ Î´
    
Ensure policy doesn't change too much
```

---

## ğŸ’» Code Examples

### Computing KL Divergence

```python
import numpy as np
import torch
import torch.nn.functional as F

def kl_divergence_discrete(p, q, eps=1e-10):
    """
    KL(P || Q) for discrete distributions
    p, q: probability arrays (sum to 1)
    """
    p = np.clip(p, eps, 1)
    q = np.clip(q, eps, 1)
    return np.sum(p * np.log(p / q))

# Example
p = np.array([0.4, 0.3, 0.3])  # True distribution
q = np.array([0.33, 0.33, 0.34])  # Approximation

kl_pq = kl_divergence_discrete(p, q)
kl_qp = kl_divergence_discrete(q, p)
print(f"KL(P||Q) = {kl_pq:.4f}")
print(f"KL(Q||P) = {kl_qp:.4f}")  # Different! (asymmetric)
```

### KL for Gaussians (Closed Form)

```python
import torch
import torch.distributions as D

def kl_gaussian(mu1, logvar1, mu2=None, logvar2=None):
    """
    KL(N(Î¼â‚,Ïƒâ‚Â²) || N(Î¼â‚‚,Ïƒâ‚‚Â²))
    
    If mu2, logvar2 are None, uses standard normal N(0,1)
    """
    if mu2 is None:
        # KL(q || p) where p = N(0, I)
        # = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
        return -0.5 * torch.sum(1 + logvar1 - mu1.pow(2) - logvar1.exp())
    else:
        # General case
        var1 = logvar1.exp()
        var2 = logvar2.exp()
        return 0.5 * torch.sum(
            logvar2 - logvar1 + var1/var2 + (mu1-mu2).pow(2)/var2 - 1
        )

# Example: VAE latent regularization
mu = torch.randn(32, 64)  # Batch of 32, latent dim 64
logvar = torch.randn(32, 64)
kl_loss = kl_gaussian(mu, logvar)  # KL to standard normal
```

### PyTorch KL Divergence

```python
import torch.nn.functional as F

# For log-probabilities
log_p = F.log_softmax(logits_p, dim=-1)
log_q = F.log_softmax(logits_q, dim=-1)

# KL(P || Q) where inputs are log-probs
kl = F.kl_div(log_q, log_p.exp(), reduction='batchmean')

# Or using distributions
p = torch.distributions.Categorical(logits=logits_p)
q = torch.distributions.Categorical(logits=logits_q)
kl = torch.distributions.kl_divergence(p, q)
```

### Knowledge Distillation

```python
def distillation_loss(student_logits, teacher_logits, temperature=2.0):
    """
    Knowledge distillation: KL(teacher || student) with temperature
    """
    # Soft targets with temperature
    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)
    log_soft_student = F.log_softmax(student_logits / temperature, dim=-1)
    
    # KL divergence
    kl = F.kl_div(log_soft_student, soft_teacher, reduction='batchmean')
    
    # Scale by TÂ² (standard practice)
    return kl * (temperature ** 2)
```

---

## ğŸ“ DETAILED MATHEMATICAL PROOFS

### 1. Gibbs' Inequality: D_KL(P||Q) â‰¥ 0

**Theorem:** For any distributions P and Q, D_KL(P||Q) â‰¥ 0, with equality iff P = Q.

**Proof:**

**Method 1: Jensen's Inequality**

```
Step 1: Define D_KL
D_KL(P||Q) = Î£â‚“ P(x) log(P(x)/Q(x))
           = ğ”¼_P[log(P(x)/Q(x))]

Step 2: Use log concavity
log is a strictly concave function, so -log is strictly convex.

By Jensen's inequality for convex functions:
ğ”¼[Ï†(X)] â‰¥ Ï†(ğ”¼[X])   (with equality iff X is constant)

Step 3: Apply Jensen
-D_KL(P||Q) = ğ”¼_P[-log(P(x)/Q(x))]
            = ğ”¼_P[log(Q(x)/P(x))]
            â‰¤ log(ğ”¼_P[Q(x)/P(x)])          (Jensen)
            = log(Î£â‚“ P(x) Â· Q(x)/P(x))
            = log(Î£â‚“ Q(x))
            = log(1)                        (Q is a distribution)
            = 0

Therefore: D_KL(P||Q) â‰¥ 0

Step 4: Equality condition
Equality iff Q(x)/P(x) is constant for all x where P(x) > 0
âŸ¹ Q(x) = cÂ·P(x) for all x
Since both are distributions: c = 1
âŸ¹ P = Q
```

**Method 2: Direct calculation**

```
Let r(x) = Q(x)/P(x)

D_KL(P||Q) = -Î£â‚“ P(x) log r(x)
           â‰¥ -log(Î£â‚“ P(x)r(x))             (Jensen)
           = -log(Î£â‚“ Q(x))
           = -log(1)
           = 0  âˆ
```

---

### 2. KL Divergence for Gaussians (Closed Form)

**Problem:** Compute D_KL(N(Î¼â‚, Î£â‚) || N(Î¼â‚‚, Î£â‚‚))

**Theorem:**
```
D_KL(N(Î¼â‚,Î£â‚) || N(Î¼â‚‚,Î£â‚‚)) = 1/2 [tr(Î£â‚‚â»Â¹Î£â‚) + (Î¼â‚‚-Î¼â‚)áµ€Î£â‚‚â»Â¹(Î¼â‚‚-Î¼â‚) - d + log(det(Î£â‚‚)/det(Î£â‚))]
```

**Proof:**

```
Step 1: Write out D_KL definition
D_KL(pâ‚||pâ‚‚) = âˆ« pâ‚(x) log(pâ‚(x)/pâ‚‚(x)) dx
             = ğ”¼_{x~pâ‚}[log pâ‚(x) - log pâ‚‚(x)]
             = ğ”¼[log pâ‚(x)] - ğ”¼[log pâ‚‚(x)]

Step 2: Gaussian log-likelihood
log p(x) = -1/2[(x-Î¼)áµ€Î£â»Â¹(x-Î¼) + log|Î£| + dÂ·log(2Ï€)]

Step 3: Compute ğ”¼[log pâ‚(x)] where x ~ N(Î¼â‚, Î£â‚)
ğ”¼[log pâ‚(x)] = ğ”¼[-1/2[(x-Î¼â‚)áµ€Î£â‚â»Â¹(x-Î¼â‚) + log|Î£â‚| + dÂ·log(2Ï€)]]
             = -1/2[tr(Î£â‚â»Â¹Î£â‚) + log|Î£â‚| + dÂ·log(2Ï€)]
             = -1/2[d + log|Î£â‚| + dÂ·log(2Ï€)]

Step 4: Compute ğ”¼[log pâ‚‚(x)] where x ~ N(Î¼â‚, Î£â‚)
ğ”¼[log pâ‚‚(x)] = ğ”¼[-1/2[(x-Î¼â‚‚)áµ€Î£â‚‚â»Â¹(x-Î¼â‚‚) + log|Î£â‚‚| + dÂ·log(2Ï€)]]

For the quadratic term:
ğ”¼[(x-Î¼â‚‚)áµ€Î£â‚‚â»Â¹(x-Î¼â‚‚)] = ğ”¼[xáµ€Î£â‚‚â»Â¹x] - 2Î¼â‚‚áµ€Î£â‚‚â»Â¹ğ”¼[x] + Î¼â‚‚áµ€Î£â‚‚â»Â¹Î¼â‚‚

Since x ~ N(Î¼â‚, Î£â‚):
ğ”¼[x] = Î¼â‚
ğ”¼[xáµ€Î£â‚‚â»Â¹x] = tr(Î£â‚‚â»Â¹ğ”¼[xxáµ€])
           = tr(Î£â‚‚â»Â¹(Î£â‚ + Î¼â‚Î¼â‚áµ€))
           = tr(Î£â‚‚â»Â¹Î£â‚) + Î¼â‚áµ€Î£â‚‚â»Â¹Î¼â‚

Therefore:
ğ”¼[(x-Î¼â‚‚)áµ€Î£â‚‚â»Â¹(x-Î¼â‚‚)] = tr(Î£â‚‚â»Â¹Î£â‚) + Î¼â‚áµ€Î£â‚‚â»Â¹Î¼â‚ - 2Î¼â‚‚áµ€Î£â‚‚â»Â¹Î¼â‚ + Î¼â‚‚áµ€Î£â‚‚â»Â¹Î¼â‚‚
                      = tr(Î£â‚‚â»Â¹Î£â‚) + (Î¼â‚-Î¼â‚‚)áµ€Î£â‚‚â»Â¹(Î¼â‚-Î¼â‚‚)

So:
ğ”¼[log pâ‚‚(x)] = -1/2[tr(Î£â‚‚â»Â¹Î£â‚) + (Î¼â‚-Î¼â‚‚)áµ€Î£â‚‚â»Â¹(Î¼â‚-Î¼â‚‚) + log|Î£â‚‚| + dÂ·log(2Ï€)]

Step 5: Compute D_KL
D_KL = ğ”¼[log pâ‚(x)] - ğ”¼[log pâ‚‚(x)]
     = -1/2[d + log|Î£â‚|] + 1/2[tr(Î£â‚‚â»Â¹Î£â‚) + (Î¼â‚-Î¼â‚‚)áµ€Î£â‚‚â»Â¹(Î¼â‚-Î¼â‚‚) + log|Î£â‚‚|]
     = 1/2[tr(Î£â‚‚â»Â¹Î£â‚) + (Î¼â‚‚-Î¼â‚)áµ€Î£â‚‚â»Â¹(Î¼â‚‚-Î¼â‚) - d + log(|Î£â‚‚|/|Î£â‚|)]  âˆ
```

**Special case: D_KL(N(Î¼, Î£) || N(0, I))**

```
Î£â‚‚ = I, Î¼â‚‚ = 0:

D_KL = 1/2[tr(Î£) + Î¼áµ€Î¼ - d - log|Î£|]
     = 1/2[tr(Î£) + ||Î¼||Â² - d - log|Î£|]

If Î£ = diag(Ïƒâ‚Â², ..., Ïƒâ‚Â²):
D_KL = 1/2 Î£áµ¢[Ïƒáµ¢Â² + Î¼áµ¢Â² - 1 - log(Ïƒáµ¢Â²)]
```

This is the KL term in VAE loss!

---

### 3. Information-Theoretic Interpretation

**KL as expected log-likelihood ratio:**

```
D_KL(P||Q) = ğ”¼_P[log(P(x)/Q(x))]

Interpretation:
For x ~ P, we expect log P(x) to be larger than log Q(x) by D_KL(P||Q).

In coding theory:
- Optimal code for P: uses -log P(x) bits
- Using code for Q: uses -log Q(x) bits
- Extra bits needed: log(P(x)/Q(x))
- Expected extra bits: D_KL(P||Q)
```

**KL as information gain:**

```
D_KL(P||Q) = H(P, Q) - H(P)

H(P):      Entropy of P (inherent uncertainty)
H(P, Q):   Cross-entropy (coding cost using Q)
D_KL:      Extra cost of using wrong distribution Q
```

---

### 4. Forward vs Reverse KL: Mathematical Analysis

**Forward KL: D_KL(P||Q)**
```
= Î£â‚“ P(x) log(P(x)/Q(x))

Behavior when P(x) > 0 but Q(x) â‰ˆ 0:
  log(P(x)/Q(x)) â†’ âˆ
  Heavily penalized!
  â†’ Q must cover all of P (zero-forcing)
  â†’ Q spreads to cover all modes
```

**Reverse KL: D_KL(Q||P)**
```
= Î£â‚“ Q(x) log(Q(x)/P(x))

Behavior when Q(x) > 0 but P(x) â‰ˆ 0:
  log(Q(x)/P(x)) â†’ âˆ
  Heavily penalized!
  â†’ Q avoids regions where P is small (zero-avoiding)
  â†’ Q concentrates on one mode
```

**Example: Bimodal P**

```
P(x) = 0.5Â·N(-2, 1) + 0.5Â·N(2, 1)  (two modes)
Q(x) = N(Î¼, ÏƒÂ²)                     (single Gaussian)

Minimizing D_KL(P||Q):
  Q* â‰ˆ N(0, ÏƒÂ²) with large Ïƒ
  Covers both modes, mass between them

Minimizing D_KL(Q||P):
  Q* â‰ˆ N(-2, 1) or N(2, 1)
  Picks one mode, ignores the other
```

**Code to visualize:**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.optimize import minimize

def bimodal_p(x):
    """True distribution: mixture of two Gaussians"""
    return 0.5 * norm.pdf(x, -2, 1) + 0.5 * norm.pdf(x, 2, 1)

def gaussian_q(x, mu, sigma):
    """Approximate distribution: single Gaussian"""
    return norm.pdf(x, mu, sigma)

def forward_kl(params):
    """D_KL(P||Q): minimize this"""
    mu, log_sigma = params
    sigma = np.exp(log_sigma)
    x = np.linspace(-6, 6, 1000)
    p = bimodal_p(x)
    q = gaussian_q(x, mu, sigma) + 1e-10
    return np.trapz(p * np.log(p / q), x)

def reverse_kl(params):
    """D_KL(Q||P): minimize this"""
    mu, log_sigma = params
    sigma = np.exp(log_sigma)
    x = np.linspace(-6, 6, 1000)
    p = bimodal_p(x) + 1e-10
    q = gaussian_q(x, mu, sigma)
    return np.trapz(q * np.log(q / p), x)

# Optimize
result_forward = minimize(forward_kl, [0, 0], method='BFGS')
result_reverse = minimize(reverse_kl, [-2, 0], method='BFGS')

print("Forward KL (P||Q): Q spreads to cover both modes")
print(f"  Î¼ = {result_forward.x[0]:.2f}, Ïƒ = {np.exp(result_forward.x[1]):.2f}")

print("\nReverse KL (Q||P): Q concentrates on one mode")
print(f"  Î¼ = {result_reverse.x[0]:.2f}, Ïƒ = {np.exp(result_reverse.x[1]):.2f}")
```

---

### 5. VAE Loss Derivation

**Goal:** Maximize log p(x) (data likelihood)

**Problem:** p(x) = âˆ« p(x|z)p(z) dz is intractable

**Solution:** Variational inference with q(z|x)

```
Step 1: Write log p(x)
log p(x) = log âˆ« p(x,z) dz
         = log âˆ« p(x,z) Â· q(z|x)/q(z|x) dz
         = log ğ”¼_q[p(x,z)/q(z|x)]

Step 2: Apply Jensen's inequality
log ğ”¼[X] â‰¥ ğ”¼[log X]  (log is concave)

log p(x) â‰¥ ğ”¼_q[log p(x,z)/q(z|x)]
         = ğ”¼_q[log p(x|z)] + ğ”¼_q[log p(z)/q(z|x)]
         = ğ”¼_q[log p(x|z)] - ğ”¼_q[log q(z|x)/p(z)]
         = ğ”¼_q[log p(x|z)] - D_KL(q(z|x) || p(z))

This is the ELBO (Evidence Lower Bound)!

Step 3: Gap analysis
log p(x) - ELBO = D_KL(q(z|x) || p(z|x)) â‰¥ 0

So: log p(x) = ELBO + D_KL(q(z|x) || p(z|x))

Maximizing ELBO âŸº Minimizing D_KL(q(z|x) || p(z|x))
```

**VAE objective:**
```
L = ğ”¼_q[log p(x|z)] - D_KL(q(z|x) || p(z))
    +----------+       +--------------+
    Reconstruction     Regularization
    (decoder)          (KL to prior)

If q(z|x) = N(Î¼(x), Î£(x)) and p(z) = N(0, I):
D_KL = 1/2 Î£áµ¢[Î¼áµ¢Â² + Ïƒáµ¢Â² - 1 - log Ïƒáµ¢Â²]
```

---

### 6. Research Papers Using KL Divergence

| Paper | Year | How KL is Used | Equation |
|-------|------|----------------|----------|
| **VAE** (Kingma & Welling) | 2013 | Regularize latent | L = ğ”¼[log p(x\|z)] - D_KL(q\|\|p) |
| **Î²-VAE** (Higgins et al.) | 2017 | Weighted regularization | L = ğ”¼[log p(x\|z)] - Î²D_KL(q\|\|p) |
| **TRPO** (Schulman et al.) | 2015 | Constrain policy | D_KL(Ï€_old\|\|Ï€_new) â‰¤ Î´ |
| **PPO** (Schulman et al.) | 2017 | Penalty term | L - Î²Â·D_KL(Ï€_old\|\|Ï€_new) |
| **RLHF** (Ouyang et al.) | 2022 | Prevent model drift | L - Î²Â·D_KL(Ï€_Î¸\|\|Ï€_ref) |
| **Distillation** (Hinton et al.) | 2015 | Transfer knowledge | L = D_KL(p_teacher\|\|p_student) |

---

### 7. Practical Tips for Research Papers

**When you see KL in a paper:**

1. **Check the direction!**
   ```
   D_KL(P||Q) vs D_KL(Q||P)
   
   Forward: Mode-covering (variational inference)
   Reverse: Mode-seeking (expectation propagation)
   ```

2. **Look for approximations:**
   ```
   Exact KL (Gaussians):      Closed form âœ“
   Approximate KL (general):  Monte Carlo estimation
                             D_KL â‰ˆ (1/N) Î£áµ¢ log(p(xáµ¢)/q(xáµ¢))
   ```

3. **Temperature scaling:**
   ```
   Knowledge distillation often uses:
   D_KL(softmax(z_teacher/T) || softmax(z_student/T))
   
   T > 1: Softens distributions, more information transfer
   ```

4. **KL vs Cross-Entropy:**
   ```
   In classification (P = true labels, Q = predictions):
   
   Minimizing H(P, Q) = Minimizing D_KL(P||Q) + H(P)
   
   Since H(P) is constant (one-hot labels), they're equivalent!
   ```

---

## ğŸ”— Related Quantities

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Entropy | [../entropy/](../entropy/) |
| ğŸ“– | Cross-Entropy | [../cross-entropy/](../cross-entropy/) |
| ğŸ“– | Mutual Information | [../mutual-information/](../mutual-information/) |
| ğŸ“„ | Cover & Thomas Ch. 2 | [Information Theory Book](https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959) |
| ğŸ‡¨ğŸ‡³ | KLæ•£åº¦è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/22464760) |
| ğŸ‡¨ğŸ‡³ | KLæ•£åº¦ä¸äº¤å‰ç†µ | [CSDN](https://blog.csdn.net/qq_37466121/article/details/89287585) |
| ğŸ‡¨ğŸ‡³ | ä¿¡æ¯è®º-KLæ•£åº¦ | [Bç«™](https://www.bilibili.com/video/BV1VW411M7PW) |
| ğŸ‡¨ğŸ‡³ | VAEä¸­çš„KLæ•£åº¦ | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2018-12-24-2) |
| ğŸ‡¨ğŸ‡³ | çŸ¥è¯†è’¸é¦åŸç† | [PaperWeekly](https://www.paperweekly.site/papers/notes/2367)

---

## ğŸ”— Where KL Divergence Is Used

| Application | How It's Applied |
|-------------|------------------|
| **VAE Training** | Regularize latent to match prior: KL(q(z\|x) \|\| p(z)) |
| **Knowledge Distillation** | Transfer knowledge: KL(p_teacher \|\| p_student) |
| **PPO/TRPO** | Constrain policy updates: KL(Ï€_old \|\| Ï€_new) â‰¤ Î´ |
| **RLHF** | Penalty: KL(Ï€_Î¸ \|\| Ï€_ref) prevents drift from base model |
| **Variational Inference** | Approximate intractable posteriors |
| **Generative Models** | GAN discriminator estimates f-divergence |
| **Information Bottleneck** | Compression vs prediction tradeoff |

---

â¬…ï¸ [Back: Entropy](../02_entropy/) | â¡ï¸ [Next: Mutual Information](../04_mutual_information/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=9C27B0&height=80&section=footer" width="100%"/>
</p>
