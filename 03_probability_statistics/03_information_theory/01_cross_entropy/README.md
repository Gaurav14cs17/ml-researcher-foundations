<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=9C27B0&height=120&section=header&text=Cross-Entropy&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-03-9C27B0?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ðŸŽ¯ Visual Overview

<img src="./images/cross-entropy.svg" width="100%">

*Caption: Cross-entropy measures the "distance" between predicted probabilities and true labels. It heavily penalizes confident wrong predictions. The loss approaches 0 when predictions match the true labels.*

---

## ðŸ“‚ Overview

Cross-entropy is the standard loss function for classification. It combines softmax activation with negative log-likelihood into a stable, differentiable objective.

---

## ðŸ”‘ Formulas

| Type | Formula | Use Case |
|------|---------|----------|
| **Binary** | -[yÂ·log(p) + (1-y)Â·log(1-p)] | Binary classification |
| **Categorical** | -Î£áµ¢ yáµ¢Â·log(páµ¢) | Multi-class |
| **Sparse** | -log(p_true) | Multi-class (label indices) |

---

## ðŸ“ Why Cross-Entropy?

```
MSE for classification:  L = (y - p)Â²  â†’ gradients vanish when p â‰ˆ 0 or 1

Cross-entropy:           L = -log(p)   â†’ strong gradient when wrong
                                       â†’ approaches 0 when correct

Gradient: âˆ‚L/âˆ‚logit = p - y  (elegant!)

```

---

## ðŸ’» Code

```python
import torch.nn.functional as F

# Multi-class classification (most common)
loss = F.cross_entropy(logits, labels)  # logits: (B, C), labels: (B,)

# Binary classification
loss = F.binary_cross_entropy_with_logits(logits, labels.float())

# With label smoothing (regularization)
loss = F.cross_entropy(logits, labels, label_smoothing=0.1)

```

---

## ðŸ“ DETAILED MATHEMATICAL THEORY

### 1. Cross-Entropy: Complete Definition

**Information-Theoretic Definition:**

```
Cross-entropy H(P, Q) measures the expected number of bits 
needed to encode data from distribution P using a code optimized for Q.

Discrete:
  H(P, Q) = -Î£â‚“ P(x)Â·log Q(x)
          = ð”¼_P[-log Q(x)]

Continuous:
  H(P, Q) = -âˆ« p(x)Â·log q(x) dx

Relationship to KL divergence:
  H(P, Q) = H(P) + D_KL(P||Q)
  
where H(P) is the entropy of P.

```

**Key Properties:**

```
1. Non-negative: H(P, Q) â‰¥ 0
2. Minimum at equality: H(P, Q) â‰¥ H(P), with equality iff P = Q

3. Not symmetric: H(P, Q) â‰  H(Q, P) in general

4. Lower bounded by entropy: H(P, Q) â‰¥ H(P)

```

---

### 2. Cross-Entropy for Classification

**Binary Classification:**

```
True label: y âˆˆ {0, 1}
Prediction: p âˆˆ [0, 1]  (probability of class 1)

Binary cross-entropy:
  L(y, p) = -[yÂ·log(p) + (1-y)Â·log(1-p)]

Cases:
  â€¢ y = 1: L = -log(p)     (penalize if p small)
  â€¢ y = 0: L = -log(1-p)   (penalize if p large)

Properties:
  â€¢ L â†’ 0 when prediction correct (p â†’ y)
  â€¢ L â†’ âˆž when prediction wrong with high confidence
  â€¢ Gradient: âˆ‚L/âˆ‚p = (p-y)/(p(1-p))

```

**Multi-Class Classification (Categorical Cross-Entropy):**

```
True label: y âˆˆ {1,...,C}  (class index)
Prediction: p = [pâ‚,...,p_C] where Î£áµ¢páµ¢ = 1  (probability distribution)

Cross-entropy loss:
  L(y, p) = -log(p_y)
          = -Î£áµ¢â‚Œâ‚á¶œ ðŸ™{y=i}Â·log(páµ¢)

One-hot encoding (y as vector):
  y = [0,...,0,1,0,...,0]  (1 at position y)
  L = -Î£áµ¢ yáµ¢Â·log(páµ¢)

Properties:
  â€¢ Focuses on probability of correct class
  â€¢ Heavily penalizes confident wrong predictions
  â€¢ Gradient: âˆ‚L/âˆ‚páµ¢ = -yáµ¢/páµ¢

```

---

### 3. Softmax + Cross-Entropy: The Perfect Pair

**Why Softmax?**

```
Raw network output (logits): z = [zâ‚,...,z_C] âˆˆ â„á¶œ

Softmax:
  páµ¢ = exp(záµ¢) / Î£â±¼ exp(zâ±¼)

Properties:
  â€¢ Î£áµ¢ páµ¢ = 1 (valid probability distribution)
  â€¢ páµ¢ > 0 (no zero probabilities)
  â€¢ Differentiable everywhere
  â€¢ max(záµ¢) â†’ p â‰ˆ one-hot (confident prediction)

```

**Combined Gradient (The Magic!):**

```
Loss: L = -log(p_y) where páµ¢ = softmax(z)áµ¢

Direct computation of âˆ‚L/âˆ‚záµ¢:
  âˆ‚L/âˆ‚záµ¢ = âˆ‚L/âˆ‚pâ±¼ Â· âˆ‚pâ±¼/âˆ‚záµ¢  (sum over j)

Softmax derivative:
  âˆ‚pâ±¼/âˆ‚záµ¢ = pâ±¼(Î´áµ¢â±¼ - páµ¢)
  
where Î´áµ¢â±¼ = 1 if i=j, 0 otherwise

Result (beautiful simplification!):
  âˆ‚L/âˆ‚záµ¢ = páµ¢ - yáµ¢
  
where yáµ¢ = ðŸ™{class = i} (one-hot)

Interpretation:
  Gradient = prediction - truth
  
  Example:
    True class: 2 (out of 3 classes)
    y = [0, 1, 0]
    p = [0.1, 0.7, 0.2]
    âˆ‡L = [0.1, -0.3, 0.2]
    
    Gradient pushes probability toward true class!

```

**Proof of âˆ‚L/âˆ‚záµ¢ = páµ¢ - yáµ¢:**

```
Step 1: Write loss
  L = -log(p_y) = -log(exp(z_y)/Î£â±¼exp(zâ±¼))
    = -z_y + log(Î£â±¼exp(zâ±¼))

Step 2: Differentiate
  Case i = y (true class):
    âˆ‚L/âˆ‚záµ§ = -1 + exp(záµ§)/Î£â±¼exp(zâ±¼)
           = -1 + páµ§
           = páµ§ - 1
           = páµ§ - yáµ§  (since yáµ§=1)
  
  Case i â‰  y (other classes):
    âˆ‚L/âˆ‚záµ¢ = 0 + exp(záµ¢)/Î£â±¼exp(zâ±¼)
           = páµ¢
           = páµ¢ - 0
           = páµ¢ - yáµ¢  (since yáµ¢=0)

Both cases: âˆ‚L/âˆ‚záµ¢ = páµ¢ - yáµ¢ âœ“  QED

```

---

### 4. Numerical Stability: LogSumExp Trick

**Problem: Softmax Can Overflow**

```
páµ¢ = exp(záµ¢) / Î£â±¼ exp(zâ±¼)

If záµ¢ large (e.g., z = [1000, 1001, 999]):
  exp(1000) â‰ˆ 10â´Â³â´ â†’ inf in float32!

```

**Solution: Subtract Maximum**

```
Mathematically equivalent:
  páµ¢ = exp(záµ¢ - max(z)) / Î£â±¼ exp(zâ±¼ - max(z))

Why it works:
  exp(záµ¢)/Î£â±¼exp(zâ±¼) = exp(záµ¢ - c)/Î£â±¼exp(zâ±¼ - c)  for any c
  
  Choose c = max(z):
    â€¢ Largest value becomes exp(0) = 1
    â€¢ All other values â‰¤ 1
    â€¢ No overflow!

Example:
  z = [1000, 1001, 999]
  z' = z - 1001 = [-1, 0, -2]
  p = [eâ»Â¹, eâ°, eâ»Â²] / (eâ»Â¹ + eâ° + eâ»Â²)
    â‰ˆ [0.268, 0.665, 0.099]

```

**Log-Softmax (Even More Stable):**

```
For cross-entropy, we need log(páµ¢):
  log páµ¢ = log(exp(záµ¢)/Î£â±¼exp(zâ±¼))
         = záµ¢ - log(Î£â±¼exp(zâ±¼))
         = záµ¢ - LogSumExp(z)

LogSumExp(z) = log(Î£â±¼exp(zâ±¼))
             = max(z) + log(Î£â±¼exp(zâ±¼ - max(z)))  (stable!)

Then cross-entropy:
  L = -log(p_y)
    = -(z_y - LogSumExp(z))
    = LogSumExp(z) - z_y

PyTorch: F.log_softmax(z) computes this stably

```

---

### 5. Cross-Entropy vs Mean Squared Error

**Why Not MSE for Classification?**

```
MSE: L = (1/2)||y - p||Â²

Problem 1: Saturating gradients
  For sigmoid activation Ïƒ(z):
    âˆ‚L/âˆ‚z = (p - y)Â·pÂ·(1-p)
            â†‘       â†‘
         error   derivative
    
  When p â‰ˆ 0 or 1: Ïƒ'(z) â‰ˆ 0 â†’ vanishing gradient!
  Even if prediction is very wrong

Cross-entropy: L = -[yÂ·log(p) + (1-y)Â·log(1-p)]
  âˆ‚L/âˆ‚z = p - y
  
  No multiplication by Ïƒ'! Gradient doesn't vanish.

Problem 2: Wrong assumptions
  MSE assumes Gaussian noise: p(y|z) = N(y; f(z), ÏƒÂ²)
  But y âˆˆ {0,1} is discrete! Not Gaussian.
  
  Cross-entropy assumes Bernoulli: p(y|z) = Bernoulli(Ïƒ(z))
  Correct model for binary outcomes.

```

**Comparison:**

```
                MSE                     Cross-Entropy
Loss:           (y-p)Â²                  -yÂ·log(p) - (1-y)Â·log(1-p)
Gradient:       (p-y)Â·Ïƒ'(z)             p - y
Saturation:     Yes (at extremes)       No
Convergence:    Slow                    Fast
Interpretation: L2 distance             Log-likelihood
Best for:       Regression              Classification

```

---

### 6. Probabilistic Interpretation: Maximum Likelihood

**Cross-Entropy = Negative Log-Likelihood**

```
Dataset: D = {(xâ‚,yâ‚), ..., (xâ‚™,yâ‚™)}

Model: p(y|x; Î¸) = predicted probability

Log-likelihood:
  â„“(Î¸) = Î£áµ¢â‚Œâ‚â¿ log p(yáµ¢|xáµ¢; Î¸)

Maximum likelihood estimation:
  Î¸* = argmax_Î¸ â„“(Î¸)
     = argmin_Î¸ -â„“(Î¸)
     = argmin_Î¸ Î£áµ¢â‚Œâ‚â¿ -log p(yáµ¢|xáµ¢; Î¸)

For classification:
  -log p(yáµ¢|xáµ¢; Î¸) = cross-entropy loss!

Therefore:
  Minimizing cross-entropy = Maximum likelihood estimation âœ“

```

**Multi-Class Case:**

```
Model: p(y=c|x; Î¸) = p_c(x; Î¸)  (softmax output)

One sample:
  L(y, p) = -log p_y
  
Dataset:
  L = -(1/n)Î£áµ¢â‚Œâ‚â¿ log p_{yáµ¢}(xáµ¢; Î¸)
    = average negative log-likelihood
    = cross-entropy loss

Interpretation:
  Find parameters Î¸ that maximize probability of observing the data

```

---

### 7. Label Smoothing: Regularized Cross-Entropy

**Problem: Overconfidence**

```
Model predicts: p = [0.001, 0.998, 0.001]
True label: y = [0, 1, 0]

Loss is low, but model is overconfident!
May not generalize well.

```

**Label Smoothing (Szegedy et al., 2016):**

```
Smooth the hard targets:
  y_smooth = (1 - Î±)Â·y + Î±/C
  
where:
  Î± âˆˆ [0,1] = smoothing parameter (typically 0.1)
  C = number of classes

Example (C=3, Î±=0.1):
  Original: y = [0, 1, 0]
  Smoothed: y = [0.033, 0.933, 0.033]

Effect:
  â€¢ True class: 1 â†’ 0.9 + 0.1/3 = 0.933
  â€¢ Other classes: 0 â†’ 0.1/3 = 0.033

```

**Why It Helps:**

```
1. Prevents overconfidence
   Model can't push p_y â†’ 1 (would increase loss on other classes)

2. Implicit regularization
   Equivalent to adding entropy regularization:
     L_total = L_CE - Î±Â·H(p)
   
   Encourages higher entropy (less confident) predictions

3. Better generalization
   Empirically shown to improve test accuracy
   Used in: Inception, ResNet, Transformers

4. Calibration
   Model probabilities better match true frequencies

```

**Implementation:**

```python
def cross_entropy_with_label_smoothing(logits, labels, alpha=0.1):
    """
    logits: (batch_size, num_classes)
    labels: (batch_size,)  integer labels
    """
    num_classes = logits.size(-1)
    
    # Create smoothed labels
    smoothed_labels = torch.full_like(logits, alpha / num_classes)
    smoothed_labels.scatter_(-1, labels.unsqueeze(-1), 1.0 - alpha + alpha / num_classes)
    
    # Compute cross-entropy with soft labels
    log_probs = F.log_softmax(logits, dim=-1)
    loss = -(smoothed_labels * log_probs).sum(dim=-1).mean()
    
    return loss

# PyTorch built-in (since 1.10):
loss = F.cross_entropy(logits, labels, label_smoothing=0.1)

```

---

### 8. Focal Loss: Handling Class Imbalance

**Problem: Easy Examples Dominate**

```
In object detection:
  â€¢ Background patches: 99.9% (easy, low loss)
  â€¢ Object patches: 0.1% (hard, high loss)

Standard CE: L = -log(p_t)
  Easy examples (p_t â‰ˆ 1): Still contribute to loss
  Model spends time on easy examples

```

**Focal Loss (Lin et al., 2017):**

```
FL(p_t) = -(1 - p_t)^Î³ Â· log(p_t)

where:
  p_t = p if y=1, else 1-p  (probability of true class)
  Î³ â‰¥ 0 = focusing parameter (typically 2)

Behavior:
  â€¢ Easy examples (p_t â†’ 1): (1-p_t)^Î³ â†’ 0, down-weighted!
  â€¢ Hard examples (p_t â†’ 0): (1-p_t)^Î³ â†’ 1, full weight

Example (Î³=2):
  p_t = 0.9 (easy):   FL = 0.01 Â· (-log 0.9) â‰ˆ 0.001
  p_t = 0.5 (medium): FL = 0.25 Â· (-log 0.5) â‰ˆ 0.17
  p_t = 0.1 (hard):   FL = 0.81 Â· (-log 0.1) â‰ˆ 1.87
  
  Hard example gets 1870Ã— more weight than easy!

```

**With Class Balancing:**

```
FL(p_t) = -Î±_tÂ·(1 - p_t)^Î³ Â· log(p_t)

where Î±_t is the weight for class t (e.g., inverse frequency)

Applications:
  â€¢ One-stage object detection (RetinaNet)
  â€¢ Semantic segmentation with imbalance
  â€¢ Any classification with hard negatives

```

---

### 9. Temperature Scaling in Cross-Entropy

**Knowledge Distillation Temperature:**

```
Softmax with temperature:
  páµ¢ = exp(záµ¢/T) / Î£â±¼exp(zâ±¼/T)

where T > 0 is temperature

Effect:
  â€¢ T = 1: Standard softmax
  â€¢ T > 1: "Softer" probabilities (more uniform)
  â€¢ T < 1: "Sharper" probabilities (more confident)

Example (z = [1, 2, 3]):
  T=1:   p = [0.09, 0.24, 0.67]
  T=2:   p = [0.16, 0.29, 0.55]  (softer)
  T=10:  p = [0.30, 0.33, 0.37]  (almost uniform)
  T=0.5: p = [0.02, 0.12, 0.86]  (sharper)

```

**Knowledge Distillation:**

```
Teacher model: p_T = softmax(z_T/T)
Student model: p_S = softmax(z_S/T)

Distillation loss:
  L_distill = TÂ² Â· H(p_T, p_S)
           = TÂ² Â· (-Î£áµ¢ p_T,i Â· log p_S,i)

Factor TÂ²: Compensates for gradient scaling
  âˆ‚/âˆ‚z (log softmax(z/T)) scales as 1/T
  So loss scales as 1/TÂ²
  Multiply by TÂ² to get O(1) gradients

Why it works:
  â€¢ High temperature â†’ soft targets reveal "dark knowledge"
  â€¢ Example: [0.9, 0.05, 0.05] vs [0.9, 0.08, 0.02]
    Both have same hard label, but soft labels differ!
  â€¢ Student learns relative confidences, not just top-1

```

---

### 10. Relationship to Other Losses

**Cross-Entropy âŸ· KL Divergence:**

```
H(P, Q) = H(P) + D_KL(P||Q)

For classification:
  P = true distribution (one-hot)
  Q = predicted distribution
  
  H(P) = 0  (one-hot has zero entropy)
  
Therefore:
  Minimizing H(P,Q) = Minimizing D_KL(P||Q)
  
  Cross-entropy and KL divergence are equivalent!

```

**Cross-Entropy âŸ· Maximum Likelihood:**

```
L_CE = -(1/n)Î£áµ¢ log p(yáµ¢|xáµ¢)
     = -log likelihood / n

Minimizing cross-entropy = Maximum likelihood estimation

```

**Binary CE âŸ· Logistic Regression:**

```
Logistic regression:
  p(y=1|x) = Ïƒ(wáµ€x) where Ïƒ(z) = 1/(1+eâ»á¶»)

Likelihood:
  L = Î áµ¢ p(yáµ¢|xáµ¢)
    = Î áµ¢ Ïƒ(wáµ€xáµ¢)^yáµ¢ Â· (1-Ïƒ(wáµ€xáµ¢))^{1-yáµ¢}

Log-likelihood:
  log L = Î£áµ¢ [yáµ¢Â·log Ïƒ(wáµ€xáµ¢) + (1-yáµ¢)Â·log(1-Ïƒ(wáµ€xáµ¢))]

Negative log-likelihood:
  -log L = Î£áµ¢ -[yáµ¢Â·log páµ¢ + (1-yáµ¢)Â·log(1-páµ¢)]
         = Î£áµ¢ BCE(yáµ¢, páµ¢)

Binary cross-entropy is logistic regression loss!

```

---

### 11. Practical Implementation Tips

**Numerical Stability Checklist:**

```
âœ“ Use log_softmax instead of log(softmax(Â·))
âœ“ Use F.cross_entropy (combines log_softmax + nll_loss)
âœ“ Use F.binary_cross_entropy_with_logits (not sigmoid + BCE)
âœ“ Avoid manual softmax + log operations
âœ“ Clip probabilities away from 0/1 if manual: p = clip(p, 1e-7, 1-1e-7)

```

**Memory Optimization:**

```
# Bad: Stores intermediate probabilities
probs = F.softmax(logits, dim=-1)
loss = -torch.log(probs[range(len(probs)), labels]).mean()

# Good: Fused operation, less memory
loss = F.cross_entropy(logits, labels)

For large batch/vocabulary:
  Use F.cross_entropy with reduction='sum', then divide
  Avoids storing per-example losses

```

**Multi-GPU Training:**

```
When using DataParallel/DistributedDataParallel:
  
  # Gather labels across GPUs
  # Then compute loss

  Or use reduction='mean' (default) and PyTorch handles it

```

---

## ðŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Image Classification** | Primary loss function (ImageNet, CIFAR) |
| **Language Modeling** | Next token prediction (GPT, BERT) |
| **Object Detection** | Classification head (with focal loss) |
| **Machine Translation** | Sequence-to-sequence models |
| **Speech Recognition** | CTC loss uses cross-entropy |
| **Knowledge Distillation** | Teacher-student training |
| **Reinforcement Learning** | Policy gradient (actor loss) |

## ðŸ“š References

| Type | Resource | Link |
|------|----------|------|
| ðŸ“„ | Label Smoothing Paper | [Rethinking Inception](https://arxiv.org/abs/1512.00567) |
| ðŸ“„ | Focal Loss Paper | [RetinaNet](https://arxiv.org/abs/1708.02002) |
| ðŸ“„ | Knowledge Distillation | [Hinton et al.](https://arxiv.org/abs/1503.02531) |
| ðŸ“– | Deep Learning Book | Chapter 5 (ML Basics) |
| ðŸ‡¨ðŸ‡³ | äº¤å‰ç†µè¯¦è§£ | [çŸ¥ä¹Ž](https://zhuanlan.zhihu.com/p/35709485) |
| ðŸ‡¨ðŸ‡³ | Softmaxä¸Žäº¤å‰ç†µ | [CSDN](https://blog.csdn.net/u014380165/article/details/77284921) |

---

â¬…ï¸ [Back: Information Theory](../) | âž¡ï¸ [Next: Entropy](../02_entropy/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=9C27B0&height=80&section=footer" width="100%"/>
</p>
