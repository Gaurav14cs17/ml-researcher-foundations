<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=SVD%20Deep%20Dive&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=32&desc=The%20Most%20Important%20Decomposition%20in%20ML&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-01.07_SVD-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-SVD_LoRA_PCA_Compression-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **SVD is the Swiss Army knife of linear algebra.** It works for ANY matrix, provides optimal low-rank approximation, and powers LoRA, PCA, recommendation systems, and pseudoinverse computation.

- ğŸ“ **Exists for any matrix**: $A = U\Sigma V^T$ (no restrictions!)

- ğŸ¯ **Eckart-Young**: $A_k$ is the BEST rank-$k$ approximation

- ğŸ”§ **LoRA**: Low-rank fine-tuning uses SVD insight

- ğŸ“Š **PCA**: Actually implemented via SVD (more stable)

---

## ğŸ“‘ Table of Contents

1. [Complete Theory](#1-complete-theory)

2. [Existence Proof](#2-existence-proof)

3. [Eckart-Young Theorem](#3-eckart-young-theorem)

4. [Computing SVD](#4-computing-svd)

5. [Applications](#5-applications)

6. [Code Implementation](#6-code-implementation)

7. [Resources](#-resources)

---

## ğŸ¨ Visual Overview

<img src="./images/svd-complete.svg" width="100%">

```
+-----------------------------------------------------------------------------+

|                    SINGULAR VALUE DECOMPOSITION                              |
+-----------------------------------------------------------------------------+
|                                                                              |
|      A          =        U        Ã—        Î£        Ã—        Váµ€             |
|    (mÃ—n)              (mÃ—m)            (mÃ—n)            (nÃ—n)               |
|                                                                              |
|   +-------+       +-------+       +-------+       +-------+                |
|   |       |       |       |       |Ïƒâ‚     |       |       |                |
|   | Input |   =   |Rotate |   Ã—   |  Ïƒâ‚‚   |   Ã—   |Rotate |                |
|   |Matrix |       | in â„áµ |       |   â‹±   |       | in â„â¿ |                |
|   |       |       |       |       |    Ïƒáµ£ |       |       |                |
|   +-------+       +-------+       +-------+       +-------+                |
|                                                                              |
|   GEOMETRIC INTERPRETATION:                                                  |
|   ------------------------                                                   |
|   1. Váµ€ rotates input space (aligns with principal directions)             |
|   2. Î£ scales along each axis (by singular values)                          |
|   3. U rotates output space (aligns with output directions)                 |
|                                                                              |
|   LOW-RANK APPROXIMATION:                                                    |
|   ------------------------                                                   |
|   Aâ‚– = Î£áµ¢â‚Œâ‚áµ Ïƒáµ¢ uáµ¢ váµ¢áµ€  (sum of k rank-1 matrices)                        |
|                                                                              |
|   Error: â€–A - Aâ‚–â€–Â²_F = ÏƒÂ²â‚–â‚Šâ‚ + ÏƒÂ²â‚–â‚Šâ‚‚ + ... + ÏƒÂ²áµ£                          |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. Complete Theory

### ğŸ“Œ Theorem (SVD)

For **any** matrix $A \in \mathbb{R}^{m \times n}$, there exist:

```math
A = U\Sigma V^T

```

where:

- $U \in \mathbb{R}^{m \times m}$: orthogonal ($U^TU = UU^T = I$)

- $\Sigma \in \mathbb{R}^{m \times n}$: diagonal with $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$

- $V \in \mathbb{R}^{n \times n}$: orthogonal ($V^TV = VV^T = I$)

- $r = \text{rank}(A)$: number of non-zero singular values

### ğŸ“ Relationship to Eigendecomposition

```
Aáµ€A = (UÎ£Váµ€)áµ€(UÎ£Váµ€)
    = VÎ£áµ€Uáµ€UÎ£Váµ€
    = VÎ£áµ€Î£Váµ€
    = VÎ£Â²Váµ€  (eigendecomposition of Aáµ€A!)

Similarly:
AAáµ€ = UÎ£Â²Uáµ€  (eigendecomposition of AAáµ€!)

Therefore:
â€¢ Right singular vectors (V) = eigenvectors of Aáµ€A
â€¢ Left singular vectors (U) = eigenvectors of AAáµ€
â€¢ Singular values = âˆš(eigenvalues of Aáµ€A) = âˆš(eigenvalues of AAáµ€)

```

### ğŸ“ Key Properties

| Property | Formula | Significance |
|----------|---------|--------------|
| Rank | $\text{rank}(A) = \#\{\sigma_i > 0\}$ | Count non-zero singular values |
| Frobenius norm | $\|A\|_F = \sqrt{\sum_i \sigma_i^2}$ | Total "energy" |
| Spectral norm | $\|A\|_2 = \sigma_1$ | Maximum stretching |
| Nuclear norm | $\|A\|_* = \sum_i \sigma_i$ | Low-rank regularization |
| Condition | $\kappa(A) = \sigma_1/\sigma_r$ | Numerical stability |
| Pseudoinverse | $A^+ = V\Sigma^+U^T$ | Generalized inverse |

---

## 2. Existence Proof

### ğŸ” Complete Proof of SVD Existence

```
THEOREM: For any A âˆˆ â„áµË£â¿, there exists SVD A = UÎ£Váµ€.

PROOF:

Step 1: Show Aáµ€A is symmetric positive semi-definite
----------------------------------------------------
(Aáµ€A)áµ€ = Aáµ€A  âœ“ (symmetric)

For any x âˆˆ â„â¿:
  xáµ€(Aáµ€A)x = (Ax)áµ€(Ax) = â€–Axâ€–Â² â‰¥ 0  âœ“ (PSD)

Step 2: Apply Spectral Theorem to Aáµ€A
----------------------------------------
By spectral theorem (Aáµ€A symmetric):
  Aáµ€A = VÎ›Váµ€

where:
  V = [vâ‚|vâ‚‚|...|vâ‚™] orthogonal (eigenvectors)
  Î› = diag(Î»â‚, Î»â‚‚, ..., Î»â‚™) with Î»áµ¢ â‰¥ 0 (eigenvalues)

Reorder so Î»â‚ â‰¥ Î»â‚‚ â‰¥ ... â‰¥ Î»áµ£ > 0 = Î»áµ£â‚Šâ‚ = ... = Î»â‚™
where r = rank(Aáµ€A) = rank(A).

Define: Ïƒáµ¢ = âˆšÎ»áµ¢ (singular values)

Step 3: Construct left singular vectors U
--------------------------------------------
For i = 1, ..., r (non-zero singular values):
  Define uáµ¢ = (1/Ïƒáµ¢)Aváµ¢

Verify orthonormality:
  uáµ¢áµ€uâ±¼ = (1/Ïƒáµ¢Ïƒâ±¼)(Aváµ¢)áµ€(Avâ±¼)
        = (1/Ïƒáµ¢Ïƒâ±¼)váµ¢áµ€Aáµ€Avâ±¼
        = (1/Ïƒáµ¢Ïƒâ±¼)váµ¢áµ€(Î»â±¼vâ±¼)     (since Aáµ€Avâ±¼ = Î»â±¼vâ±¼)
        = (Î»â±¼/Ïƒáµ¢Ïƒâ±¼)(váµ¢áµ€vâ±¼)
        = (Ïƒâ±¼Â²/Ïƒáµ¢Ïƒâ±¼)Î´áµ¢â±¼        (since Î»â±¼ = Ïƒâ±¼Â² and váµ¢áµ€vâ±¼ = Î´áµ¢â±¼)
        = (Ïƒâ±¼/Ïƒáµ¢)Î´áµ¢â±¼
        = Î´áµ¢â±¼  âœ“

For i = r+1, ..., m:
  Complete {uâ‚, ..., uáµ£} to an orthonormal basis of â„áµ
  (using Gram-Schmidt on any extension)

Step 4: Verify A = UÎ£Váµ€
------------------------
For j = 1, ..., r:
  By construction: uâ±¼ = (1/Ïƒâ±¼)Avâ±¼
  Therefore: Avâ±¼ = Ïƒâ±¼uâ±¼

For j = r+1, ..., n:
  Aáµ€Avâ±¼ = Î»â±¼vâ±¼ = 0Â·vâ±¼ = 0
  â€–Avâ±¼â€–Â² = vâ±¼áµ€Aáµ€Avâ±¼ = 0
  Therefore: Avâ±¼ = 0 = 0Â·uâ±¼

In matrix form:
  A[vâ‚|...|vâ‚™] = [Avâ‚|...|Avâ‚™]
               = [Ïƒâ‚uâ‚|...|Ïƒáµ£uáµ£|0|...|0]
               = [uâ‚|...|uâ‚˜][Ïƒâ‚ 0  ... 0  0 ... 0]
                            [0  Ïƒâ‚‚ ... 0  0 ... 0]
                            [...           â‹±     ]
                            [0  0  ... Ïƒáµ£ 0 ... 0]
                            [0  0  ... 0  0 ... 0]
                            [         â‹®          ]
  AV = UÎ£
  A = UÎ£Váµ€  (since V is orthogonal, Váµ€ = Vâ»Â¹)  âˆ

```

---

## 3. Eckart-Young Theorem

### ğŸ“Œ Theorem

The best rank-$k$ approximation to $A$ (in Frobenius or spectral norm) is:

```math
A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^T

```

**Error**: $\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2$

### ğŸ” Proof

```
THEOREM: Among all rank-k matrices, Aâ‚– minimizes â€–A - Bâ€–_F.

PROOF:

Step 1: Compute error for Aâ‚–
-----------------------------
  A - Aâ‚– = Î£áµ¢â‚Œâ‚Ê³ Ïƒáµ¢uáµ¢váµ¢áµ€ - Î£áµ¢â‚Œâ‚áµ Ïƒáµ¢uáµ¢váµ¢áµ€
         = Î£áµ¢â‚Œâ‚–â‚Šâ‚Ê³ Ïƒáµ¢uáµ¢váµ¢áµ€

  â€–A - Aâ‚–â€–Â²_F = â€–Î£áµ¢â‚Œâ‚–â‚Šâ‚Ê³ Ïƒáµ¢uáµ¢váµ¢áµ€â€–Â²_F

Since {uáµ¢váµ¢áµ€} are orthonormal under Frobenius inner product:
  tr((uáµ¢váµ¢áµ€)áµ€(uâ±¼vâ±¼áµ€)) = tr(váµ¢uáµ¢áµ€uâ±¼vâ±¼áµ€) = tr(váµ¢Î´áµ¢â±¼vâ±¼áµ€) = Î´áµ¢â±¼

Therefore:
  â€–A - Aâ‚–â€–Â²_F = Î£áµ¢â‚Œâ‚–â‚Šâ‚Ê³ Ïƒáµ¢Â²

Step 2: Show no rank-k matrix does better
------------------------------------------
Let B be ANY rank-k matrix.
Since rank(B) = k, the null space of B has dimension n - k.
The space span{vâ‚, ..., vâ‚–â‚Šâ‚} has dimension k + 1.

By dimension counting (Grassmann formula):
  dim(null(B) âˆ© span{vâ‚,...,vâ‚–â‚Šâ‚}) â‰¥ (n-k) + (k+1) - n = 1

So âˆƒ unit vector z âˆˆ span{vâ‚,...,vâ‚–â‚Šâ‚} with Bz = 0.

Write z = Î£áµ¢â‚Œâ‚áµâºÂ¹ Î±áµ¢váµ¢ with Î£áµ¢Î±áµ¢Â² = 1.

Then:
  â€–A - Bâ€–Â²_F â‰¥ â€–(A - B)zâ€–Â²    (Frobenius â‰¥ spectral action)
             = â€–Az - Bzâ€–Â²
             = â€–Azâ€–Â²          (since Bz = 0)
             = â€–Î£áµ¢â‚Œâ‚áµâºÂ¹ Î±áµ¢Ïƒáµ¢uáµ¢â€–Â²  (since Aváµ¢ = Ïƒáµ¢uáµ¢)
             = Î£áµ¢â‚Œâ‚áµâºÂ¹ Î±áµ¢Â²Ïƒáµ¢Â²
             â‰¥ Ïƒâ‚–â‚Šâ‚Â² Â· Î£áµ¢â‚Œâ‚áµâºÂ¹ Î±áµ¢Â²  (since Ïƒáµ¢ â‰¥ Ïƒâ‚–â‚Šâ‚ for i â‰¤ k+1)
             = Ïƒâ‚–â‚Šâ‚Â²

Step 3: Extend to full error bound
-----------------------------------
A more careful argument (applying the same technique iteratively)
shows:
  â€–A - Bâ€–Â²_F â‰¥ Î£áµ¢â‚Œâ‚–â‚Šâ‚Ê³ Ïƒáµ¢Â² = â€–A - Aâ‚–â€–Â²_F

Therefore Aâ‚– is optimal.  âˆ

```

---

## 4. Computing SVD

### ğŸ“ Algorithms

**Method 1: Via Eigendecomposition**

```
1. Form Aáµ€A (nÃ—n, symmetric)

2. Compute eigendecomposition: Aáµ€A = VÎ›Váµ€

3. Singular values: Ïƒáµ¢ = âˆšÎ»áµ¢

4. Left singular vectors: uáµ¢ = Aváµ¢/Ïƒáµ¢

Complexity: O(mnÂ² + nÂ³)
Issue: Forms Aáµ€A explicitly â†’ numerical issues

```

**Method 2: Golub-Kahan Bidiagonalization (Standard)**

```
1. Reduce A to bidiagonal form B via orthogonal transforms

2. Apply implicit QR to B to diagonalize

3. Accumulate transforms into U and V

Complexity: O(mnÂ²) for m â‰¥ n
More numerically stable than eigendecomposition

```

**Method 3: Randomized SVD (for Large Matrices)**

```python
def randomized_svd(A, k, p=10, q=2):
    """
    Randomized SVD for computing top-k singular values/vectors.
    
    Parameters:
    - A: matrix (m Ã— n)
    - k: target rank
    - p: oversampling (k + p random projections)
    - q: power iterations (for better accuracy)
    """
    m, n = A.shape
    
    # Step 1: Random projection
    Omega = np.random.randn(n, k + p)
    Y = A @ Omega
    
    # Step 2: Power iteration for accuracy
    for _ in range(q):
        Y = A @ (A.T @ Y)
    
    # Step 3: Orthonormalize
    Q, _ = np.linalg.qr(Y)
    
    # Step 4: Reduce to small problem
    B = Q.T @ A  # (k+p) Ã— n
    
    # Step 5: SVD of small matrix
    U_B, S, Vt = np.linalg.svd(B, full_matrices=False)
    
    # Step 6: Recover left singular vectors
    U = Q @ U_B
    
    return U[:, :k], S[:k], Vt[:k, :]

```

---

## 5. Applications

### ğŸ¤– Application 1: LoRA (Low-Rank Adaptation)

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    """
    LoRA: Instead of fine-tuning W directly, learn low-rank update:
    W' = W + BA where B âˆˆ â„áµˆË£Ê³, A âˆˆ â„Ê³Ë£áµ, r << min(d,k)
    
    Connection to SVD:
    - Fine-tuning update Î”W often has low intrinsic rank
    - SVD shows Î”W â‰ˆ Î£áµ¢Ïƒáµ¢uáµ¢váµ¢áµ€ decays quickly
    - LoRA approximates: Î”W â‰ˆ BA â‰ˆ Î£áµ¢â‚Œâ‚Ê³ Ïƒáµ¢uáµ¢váµ¢áµ€
    """
    def __init__(self, in_features, out_features, rank=4, alpha=1.0):
        super().__init__()
        self.rank = rank
        self.scaling = alpha / rank
        
        # Frozen pre-trained weights
        self.W = nn.Parameter(torch.randn(out_features, in_features), 
                              requires_grad=False)
        
        # Trainable low-rank factors
        self.A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.B = nn.Parameter(torch.zeros(out_features, rank))
    
    def forward(self, x):
        return x @ self.W.T + self.scaling * (x @ self.A.T @ self.B.T)
    
    def merge_weights(self):
        """Merge LoRA into base weights (for inference)."""
        return self.W + self.scaling * (self.B @ self.A)

# Parameter comparison
d, k, r = 768, 768, 4
full_params = d * k  # 589,824
lora_params = d * r + r * k  # 6,144
print(f"Compression: {full_params / lora_params:.0f}Ã—")  # 96Ã—

```

### ğŸ¤– Application 2: PCA via SVD

```python
def pca_via_svd(X, n_components):
    """
    PCA implemented via SVD (the standard method).
    
    Why SVD instead of eigendecomposition?
    - SVD of X directly (don't form Xáµ€X)
    - More numerically stable
    - Handles m â‰  n naturally
    
    Math:
    X = UÎ£Váµ€ (SVD of centered X)
    Xáµ€X = VÎ£Â²Váµ€ (covariance up to scaling)
    Principal components = columns of V
    Projected data = UÎ£ = XV
    """
    # Center the data
    mean = X.mean(axis=0)
    X_centered = X - mean
    
    # SVD (economy form)
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)
    
    # Principal components = rows of Vt (columns of V)
    components = Vt[:n_components]
    
    # Projected data
    X_projected = U[:, :n_components] * S[:n_components]
    
    # Explained variance
    explained_var = S[:n_components]**2 / (len(X) - 1)
    explained_ratio = explained_var / explained_var.sum()
    
    return X_projected, components, explained_ratio

# Example
X = np.random.randn(1000, 100)
X_pca, components, var_ratio = pca_via_svd(X, n_components=10)
print(f"Variance explained: {var_ratio.sum():.1%}")

```

### ğŸ¤– Application 3: Image Compression

```python
def compress_image_svd(image, k):
    """
    Compress grayscale image using rank-k SVD approximation.
    
    For RGB: apply to each channel separately.
    """
    U, S, Vt = np.linalg.svd(image, full_matrices=False)
    
    # Keep top-k components
    compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
    
    # Compression metrics
    m, n = image.shape
    original_size = m * n
    compressed_size = m * k + k + k * n  # U_k + S_k + Vt_k
    compression_ratio = original_size / compressed_size
    
    # Reconstruction error
    error = np.linalg.norm(image - compressed) / np.linalg.norm(image)
    
    return compressed, compression_ratio, error

# Example with different ranks
image = np.random.rand(512, 512)  # Simulate grayscale image
for k in [5, 10, 20, 50, 100]:
    _, ratio, error = compress_image_svd(image, k)
    print(f"k={k:3d}: {ratio:.1f}Ã— compression, {error:.2%} error")

```

### ğŸ¤– Application 4: Pseudoinverse

```python
def pseudoinverse_svd(A, tol=1e-10):
    """
    Compute Moore-Penrose pseudoinverse via SVD.
    
    Aâº = VÎ£âºUáµ€ where Î£âºáµ¢áµ¢ = 1/Ïƒáµ¢ if Ïƒáµ¢ > 0, else 0
    """
    U, S, Vt = np.linalg.svd(A, full_matrices=False)
    
    # Invert non-zero singular values
    S_inv = np.zeros_like(S)
    S_inv[S > tol] = 1.0 / S[S > tol]
    
    # Aâº = V Î£âº Uáµ€
    A_pinv = Vt.T @ np.diag(S_inv) @ U.T
    
    return A_pinv

# Least squares solution
A = np.random.randn(5, 3)  # Overdetermined
b = np.random.randn(5)

A_pinv = pseudoinverse_svd(A)
x = A_pinv @ b

# Verify: x minimizes ||Ax - b||
print(f"Residual: {np.linalg.norm(A @ x - b):.6f}")
print(f"Via lstsq: {np.linalg.norm(A @ np.linalg.lstsq(A, b, rcond=None)[0] - b):.6f}")

```

---

## 6. Code Implementation

```python
import numpy as np
import torch

class SVDAnalysis:
    """Comprehensive SVD analysis toolkit."""
    
    def __init__(self, A):
        self.A = A
        self.U, self.S, self.Vt = np.linalg.svd(A, full_matrices=False)
        self.rank = np.sum(self.S > 1e-10)
    
    def low_rank_approximation(self, k):
        """Compute best rank-k approximation."""
        A_k = self.U[:, :k] @ np.diag(self.S[:k]) @ self.Vt[:k, :]
        error = np.sqrt(np.sum(self.S[k:]**2))
        return A_k, error
    
    def explained_variance(self, k):
        """Fraction of variance captured by top-k singular values."""
        return np.sum(self.S[:k]**2) / np.sum(self.S**2)
    
    def optimal_rank(self, threshold=0.95):
        """Find rank needed to capture threshold variance."""
        cumulative = np.cumsum(self.S**2) / np.sum(self.S**2)
        return np.searchsorted(cumulative, threshold) + 1
    
    def condition_number(self):
        """Compute condition number."""
        return self.S[0] / self.S[self.rank - 1]
    
    def pseudoinverse(self):
        """Compute Moore-Penrose pseudoinverse."""
        S_inv = np.zeros_like(self.S)
        S_inv[self.S > 1e-10] = 1.0 / self.S[self.S > 1e-10]
        return self.Vt.T @ np.diag(S_inv) @ self.U.T
    
    def nuclear_norm(self):
        """Compute nuclear norm (sum of singular values)."""
        return np.sum(self.S)
    
    def spectral_norm(self):
        """Compute spectral norm (max singular value)."""
        return self.S[0]
    
    def frobenius_norm(self):
        """Compute Frobenius norm (sqrt sum of squared singular values)."""
        return np.sqrt(np.sum(self.S**2))

# Demo
A = np.random.randn(100, 50)
svd = SVDAnalysis(A)

print("=== SVD Analysis ===")
print(f"Matrix shape: {A.shape}")
print(f"Rank: {svd.rank}")
print(f"Condition number: {svd.condition_number():.2f}")
print(f"Frobenius norm: {svd.frobenius_norm():.2f}")
print(f"Spectral norm: {svd.spectral_norm():.2f}")
print(f"Nuclear norm: {svd.nuclear_norm():.2f}")

print("\n=== Low-Rank Approximation ===")
for k in [1, 5, 10, 25]:
    A_k, error = svd.low_rank_approximation(k)
    var = svd.explained_variance(k)
    print(f"k={k:2d}: error={error:.4f}, variance={var:.2%}")

print(f"\nRank for 95% variance: {svd.optimal_rank(0.95)}")

```

---

## ğŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ğŸ“– | [Matrix Computations](https://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm) | Golub & Van Loan Ch.2 |
| ğŸ“„ | [LoRA Paper](https://arxiv.org/abs/2106.09685) | Low-rank adaptation |
| ğŸ¥ | [Steve Brunton SVD](https://www.youtube.com/watch?v=nbBvuuNVfco) | Visual explanation |
| ğŸ“„ | [Randomized SVD](https://arxiv.org/abs/0909.4061) | Halko et al. |

---

## ğŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Matrix Properties](../06_matrix_properties/README.md) | [Linear Algebra](../README.md) | [Transformations](../08_transformations/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
