<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Eigenvalues%20%26%20Eigenvectors&fontSize=38&fontColor=fff&animation=twinkling&fontAlignY=32&desc=The%20DNA%20of%20Linear%20Transformations&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-01.03_Eigen-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-Eigenvalues_Eigenvectors_Spectral-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **Eigenvalues and eigenvectors reveal the intrinsic behavior of linear transformations.** An eigenvector is a direction that only gets scaled (not rotated) when transformed.

- ğŸ“ **Definition**: $A\mathbf{v} = \lambda\mathbf{v}$ â€” eigenvector $\mathbf{v}$ only scales by factor $\lambda$

- ğŸ” **Finding them**: Solve $\det(A - \lambda I) = 0$ (characteristic polynomial)

- ğŸ“Š **Key facts**: $\sum \lambda\_i = \text{tr}(A)$, $\prod \lambda\_i = \det(A)$

- ğŸ¤– **ML Uses**: PCA, PageRank, stability analysis, spectral clustering

---

## ğŸ“‘ Table of Contents

1. [Visual Overview](#-visual-overview)
2. [Definition and Intuition](#1-definition-and-intuition)
3. [Finding Eigenvalues](#2-finding-eigenvalues-characteristic-polynomial)
4. [The Spectral Theorem](#3-the-spectral-theorem-complete-proof)
5. [Power Iteration Algorithm](#4-power-iteration-algorithm)
6. [Properties and Theorems](#5-key-properties-and-theorems)
7. [Examples](#6-worked-examples)
8. [Code Implementation](#7-code-implementation)
9. [ML Applications](#8-ml-applications)
10. [Common Mistakes](#-common-mistakes)
11. [Resources](#-resources)

---

## ğŸ¨ Visual Overview

<img src="./images/eigenvalue-visual.svg" width="100%">

```
+-----------------------------------------------------------------------------+

|                     EIGENVALUE / EIGENVECTOR INTUITION                       |
+-----------------------------------------------------------------------------+
|                                                                              |
|   GENERAL VECTOR                      EIGENVECTOR                            |
|   -------------                       -----------                            |
|                                                                              |
|   Before A:        After A:           Before A:        After A:              |
|       â†—               â†‘                   â†’               â†’â†’â†’                |
|      /                |                   v               Î»v                 |
|     x                Ax                                                      |
|                   (rotated!)              (only scaled by Î», same direction) |
|                                                                              |
|   Most vectors change direction        Eigenvectors ONLY scale              |
|   when multiplied by A                 They are "fixed directions" of A     |
|                                                                              |
+-----------------------------------------------------------------------------+

|                                                                              |
|   EIGENVALUE INTERPRETATION                                                  |
|   -------------------------                                                  |
|                                                                              |
|   Î» > 1:  Stretch along eigenvector direction                               |
|   Î» = 1:  No change (identity behavior)                                     |
|   0 < Î» < 1: Compress along eigenvector direction                           |
|   Î» = 0:  Collapse to zero (singular matrix)                                |
|   Î» < 0:  Flip and scale (reflection + scaling)                             |
|   Î» âˆˆ â„‚:  Rotation + scaling (complex eigenvalues)                          |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. Definition and Intuition

### ğŸ“Œ Formal Definition

For a square matrix $A \in \mathbb{R}^{n \times n}$:

```math
A\mathbf{v} = \lambda\mathbf{v}

```

where:

- $\lambda \in \mathbb{C}$ is an **eigenvalue**
- $\mathbf{v} \neq \mathbf{0}$ is the corresponding **eigenvector**

### ğŸ” Geometric Interpretation

```
When matrix A acts on vector v:
  â€¢ Most vectors: Change direction AND magnitude
  â€¢ Eigenvector v: ONLY changes magnitude (by factor Î»)

Example:
  A = [2  1]    v = [1]    Av = [2  1][1] = [3] = 3[1] = 3v
      [0  3]        [1]         [0  3][1]   [3]    [1]
  
  v is an eigenvector with eigenvalue Î» = 3

```

### Why "Eigen"?

The word "eigen" is German for "own" or "characteristic". Eigenvectors are the matrix's "own" special directions â€” the directions intrinsic to the transformation.

---

## 2. Finding Eigenvalues: Characteristic Polynomial

### ğŸ“ Derivation

```
Step 1: Start with the definition
        Av = Î»v

Step 2: Rearrange
        Av - Î»v = 0
        Av - Î»Iv = 0
        (A - Î»I)v = 0

Step 3: Non-trivial solution exists iff (A - Î»I) is singular
        det(A - Î»I) = 0

This is the CHARACTERISTIC POLYNOMIAL

```

### The Characteristic Polynomial

```math
p(\lambda) = \det(A - \lambda I)

```

This is a polynomial of degree $n$ in $\lambda$:

```math
p(\lambda) = (-1)^n \lambda^n + (-1)^{n-1}\text{tr}(A)\lambda^{n-1} + \cdots + \det(A)

```

### ğŸ’¡ Example: 2Ã—2 Matrix

For $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$:

```
det(A - Î»I) = det([a-Î»   b  ])
                 ([c    d-Î»])
            = (a-Î»)(d-Î») - bc
            = Î»Â² - (a+d)Î» + (ad-bc)
            = Î»Â² - tr(A)Â·Î» + det(A)

Quadratic formula:
Î» = (tr(A) Â± âˆš(tr(A)Â² - 4det(A))) / 2

```

---

## 3. The Spectral Theorem (Complete Proof)

### ğŸ“Œ Theorem Statement

**Spectral Theorem**: For a real symmetric matrix $A = A^T$:

1. **All eigenvalues are real**
2. **Eigenvectors can be chosen orthonormal**
3. **$A = Q\Lambda Q^T$** where $Q$ is orthogonal

### ğŸ” Complete Proof

**Part 1: Eigenvalues are Real**

```
Step 1: Let Î» be an eigenvalue with eigenvector v (possibly complex)
        Av = Î»v

Step 2: Take complex conjugate transpose of both sides
        (Av)* = (Î»v)*
        v*A* = Î»Ì„v*
        
        Since A is real and symmetric: A* = Aáµ€ = A
        So: v*A = Î»Ì„v*

Step 3: Multiply the original equation on the left by v*
        v*Av = v*(Î»v) = Î»(v*v) = Î»â€–vâ€–Â²

Step 4: Multiply Step 2's result on the right by v
        v*Av = (Î»Ì„v*)v = Î»Ì„(v*v) = Î»Ì„â€–vâ€–Â²

Step 5: From Steps 3 and 4:
        Î»â€–vâ€–Â² = Î»Ì„â€–vâ€–Â²

Step 6: Since v â‰  0, we have â€–vâ€–Â² > 0, therefore:
        Î» = Î»Ì„
        
        This means Î» is real!  âˆ

```

**Part 2: Eigenvectors of Distinct Eigenvalues are Orthogonal**

```
Step 1: Let Avâ‚ = Î»â‚vâ‚ and Avâ‚‚ = Î»â‚‚vâ‚‚ with Î»â‚ â‰  Î»â‚‚

Step 2: Compute vâ‚áµ€Avâ‚‚ in two ways

        Way 1 (use Avâ‚‚ = Î»â‚‚vâ‚‚):
        vâ‚áµ€Avâ‚‚ = vâ‚áµ€(Î»â‚‚vâ‚‚) = Î»â‚‚(vâ‚áµ€vâ‚‚)

        Way 2 (use Aáµ€ = A and Avâ‚ = Î»â‚vâ‚):
        vâ‚áµ€Avâ‚‚ = (Aáµ€vâ‚)áµ€vâ‚‚ = (Avâ‚)áµ€vâ‚‚ = (Î»â‚vâ‚)áµ€vâ‚‚ = Î»â‚(vâ‚áµ€vâ‚‚)

Step 3: Equate the two expressions:
        Î»â‚‚(vâ‚áµ€vâ‚‚) = Î»â‚(vâ‚áµ€vâ‚‚)
        (Î»â‚‚ - Î»â‚)(vâ‚áµ€vâ‚‚) = 0

Step 4: Since Î»â‚ â‰  Î»â‚‚, we must have:
        vâ‚áµ€vâ‚‚ = 0
        
        The eigenvectors are orthogonal!  âˆ

```

**Part 3: Eigendecomposition $A = Q\Lambda Q^T$**

```
Step 1: Collect orthonormal eigenvectors as columns of Q
        Q = [vâ‚ | vâ‚‚ | ... | vâ‚™]

Step 2: By definition of eigenvectors:
        Avâ‚ = Î»â‚vâ‚
        Avâ‚‚ = Î»â‚‚vâ‚‚
        ...
        Avâ‚™ = Î»â‚™vâ‚™

Step 3: In matrix form:
        A[vâ‚|vâ‚‚|...|vâ‚™] = [Î»â‚vâ‚|Î»â‚‚vâ‚‚|...|Î»â‚™vâ‚™]
        AQ = [vâ‚|vâ‚‚|...|vâ‚™][Î»â‚    0  ]
                           [   Î»â‚‚    ]
                           [      â‹±  ]
                           [0      Î»â‚™]
        AQ = QÎ›

Step 4: Since Q is orthogonal (Qáµ€Q = I):
        A = QÎ›Qâ»Â¹ = QÎ›Qáµ€  âˆ

```

### ğŸ“ Corollary: Spectral Decomposition

```math
A = \sum_{i=1}^{n} \lambda_i \mathbf{v}_i \mathbf{v}_i^T

```

Each term $\lambda\_i \mathbf{v}\_i \mathbf{v}\_i^T$ is a rank-1 projection matrix!

---

## 4. Power Iteration Algorithm

### ğŸ“Œ Algorithm

```python
def power_iteration(A, num_iterations=100):
    """
    Find the dominant eigenvalue and eigenvector.
    
    Convergence rate: |Î»â‚‚/Î»â‚|^k (geometric)
    """
    n = A.shape[0]
    v = np.random.randn(n)
    v = v / np.linalg.norm(v)
    
    for _ in range(num_iterations):
        Av = A @ v
        v = Av / np.linalg.norm(Av)
    
    # Rayleigh quotient gives eigenvalue
    eigenvalue = v @ A @ v
    return eigenvalue, v

```

### ğŸ” Proof of Convergence

```
Step 1: Express initial vector in eigenbasis
        vâ‚€ = Î±â‚vâ‚ + Î±â‚‚vâ‚‚ + ... + Î±â‚™vâ‚™
        (assuming Î±â‚ â‰  0)

Step 2: After k iterations:
        Aáµvâ‚€ = Î±â‚Î»â‚áµvâ‚ + Î±â‚‚Î»â‚‚áµvâ‚‚ + ... + Î±â‚™Î»â‚™áµvâ‚™

Step 3: Factor out Î»â‚áµ:
        Aáµvâ‚€ = Î»â‚áµ[Î±â‚vâ‚ + Î±â‚‚(Î»â‚‚/Î»â‚)áµvâ‚‚ + ... + Î±â‚™(Î»â‚™/Î»â‚)áµvâ‚™]

Step 4: If |Î»â‚| > |Î»â‚‚| â‰¥ ... â‰¥ |Î»â‚™| (dominant eigenvalue):
        As k â†’ âˆ: (Î»áµ¢/Î»â‚)áµ â†’ 0 for i â‰¥ 2

Step 5: Therefore:
        Aáµvâ‚€/â€–Aáµvâ‚€â€– â†’ Â±vâ‚  (dominant eigenvector)
        
        Convergence rate: O(|Î»â‚‚/Î»â‚|áµ)  âˆ

```

### Inverse Iteration (Find Smallest Eigenvalue)

```python
def inverse_iteration(A, shift=0, num_iterations=100):
    """
    Find eigenvalue closest to 'shift'.
    Uses (A - shift*I)â»Â¹ which has largest eigenvalue = 1/(Î» - shift)
    """
    n = A.shape[0]
    v = np.random.randn(n)
    v = v / np.linalg.norm(v)
    
    A_shifted = A - shift * np.eye(n)
    
    for _ in range(num_iterations):
        w = np.linalg.solve(A_shifted, v)  # More stable than inverse
        v = w / np.linalg.norm(w)
    
    eigenvalue = v @ A @ v
    return eigenvalue, v

```

---

## 5. Key Properties and Theorems

### ğŸ“Š Fundamental Properties

| Property | Formula | Proof |
|----------|---------|-------|
| Sum of eigenvalues | $\sum\_i \lambda\_i = \text{tr}(A)$ | Coefficient of $\lambda^{n-1}$ in char. poly |
| Product of eigenvalues | $\prod\_i \lambda\_i = \det(A)$ | $p(0) = \det(A)$ and $p(\lambda) = \prod\_i(\lambda\_i - \lambda)$ |
| Eigenvalues of $A^k$ | $\lambda\_i^k$ | $Av = \lambda v \Rightarrow A^k v = \lambda^k v$ |
| Eigenvalues of $A^{-1}$ | $1/\lambda\_i$ | $Av = \lambda v \Rightarrow v = \lambda A^{-1}v$ |
| Eigenvalues of $A + cI$ | $\lambda\_i + c$ | $(A+cI)v = Av + cv = (\lambda + c)v$ |

### ğŸ” Proof: tr(A) = Sum of Eigenvalues

```
The characteristic polynomial is:
  p(Î») = det(A - Î»I) = (-1)â¿Î»â¿ + (-1)â¿â»Â¹tr(A)Î»â¿â»Â¹ + ... + det(A)

Also, by the Fundamental Theorem of Algebra:
  p(Î») = (-1)â¿(Î» - Î»â‚)(Î» - Î»â‚‚)...(Î» - Î»â‚™)

Expanding the product:
  = (-1)â¿[Î»â¿ - (Î»â‚+Î»â‚‚+...+Î»â‚™)Î»â¿â»Â¹ + ...]
  = (-1)â¿Î»â¿ + (-1)â¿â»Â¹(Î£áµ¢Î»áµ¢)Î»â¿â»Â¹ + ...

Comparing coefficients of Î»â¿â»Â¹:
  (-1)â¿â»Â¹tr(A) = (-1)â¿â»Â¹(Î£áµ¢Î»áµ¢)
  tr(A) = Î£áµ¢Î»áµ¢  âˆ

```

### ğŸ“ Cayley-Hamilton Theorem

**Theorem**: Every matrix satisfies its own characteristic polynomial.

```math
p(A) = A^n - \text{tr}(A)A^{n-1} + \cdots + (-1)^n\det(A)I = 0

```

**Application**: Express $A^{-1}$ as polynomial in $A$:

```
For 2Ã—2: AÂ² - tr(A)Â·A + det(A)Â·I = 0
         AÂ² = tr(A)Â·A - det(A)Â·I
         Aâ»Â¹ = (tr(A)Â·I - A) / det(A)

```

---

## 6. Worked Examples

### Example 1: Complete 2Ã—2 Eigenanalysis

```
A = [4  2]
    [1  3]

Step 1: Characteristic polynomial
det(A - Î»I) = det([4-Î»   2 ])
                 ([ 1   3-Î»])
            = (4-Î»)(3-Î») - 2
            = Î»Â² - 7Î» + 12 - 2
            = Î»Â² - 7Î» + 10
            = (Î» - 5)(Î» - 2)

Step 2: Eigenvalues
Î»â‚ = 5,  Î»â‚‚ = 2

Check: Î»â‚ + Î»â‚‚ = 7 = tr(A) âœ“
       Î»â‚ Ã— Î»â‚‚ = 10 = det(A) âœ“

Step 3: Eigenvectors
For Î»â‚ = 5:
  (A - 5I)v = 0
  [-1  2][vâ‚]   [0]
  [ 1 -2][vâ‚‚] = [0]
  
  -vâ‚ + 2vâ‚‚ = 0  âŸ¹  vâ‚ = 2vâ‚‚
  vâ‚ = [2, 1]áµ€  (or normalized: [2/âˆš5, 1/âˆš5]áµ€)

For Î»â‚‚ = 2:
  (A - 2I)v = 0
  [2  2][vâ‚]   [0]
  [1  1][vâ‚‚] = [0]
  
  vâ‚ + vâ‚‚ = 0  âŸ¹  vâ‚‚ = -vâ‚
  vâ‚‚ = [1, -1]áµ€  (or normalized: [1/âˆš2, -1/âˆš2]áµ€)

Step 4: Verify
Avâ‚ = [4  2][2] = [10] = 5[2] = 5vâ‚ âœ“
      [1  3][1]   [ 5]    [1]

Avâ‚‚ = [4  2][ 1] = [2]  = 2[ 1] = 2vâ‚‚ âœ“
      [1  3][-1]   [-2]    [-1]

```

### Example 2: Symmetric Matrix (Orthogonal Eigenvectors)

```
A = [3  1]
    [1  3]

Characteristic polynomial:
det(A - Î»I) = (3-Î»)Â² - 1 = Î»Â² - 6Î» + 8 = (Î»-4)(Î»-2)

Eigenvalues: Î»â‚ = 4, Î»â‚‚ = 2

Eigenvectors:
For Î»â‚ = 4: vâ‚ = [1, 1]áµ€/âˆš2
For Î»â‚‚ = 2: vâ‚‚ = [1, -1]áµ€/âˆš2

Orthogonality check:
vâ‚áµ€vâ‚‚ = (1)(1) + (1)(-1) = 0 âœ“  (as guaranteed by Spectral Theorem)

Eigendecomposition:
A = QÎ›Qáµ€ = [1/âˆš2   1/âˆš2][4  0][1/âˆš2   1/âˆš2]
           [1/âˆš2  -1/âˆš2][0  2][1/âˆš2  -1/âˆš2]

```

### Example 3: Complex Eigenvalues (Rotation Matrix)

```
R(Î¸) = [cos Î¸  -sin Î¸]   (rotation by angle Î¸)
       [sin Î¸   cos Î¸]

Characteristic polynomial:
det(R - Î»I) = (cos Î¸ - Î»)Â² + sinÂ²Î¸
            = Î»Â² - 2cos(Î¸)Î» + cosÂ²Î¸ + sinÂ²Î¸
            = Î»Â² - 2cos(Î¸)Î» + 1

Eigenvalues (using quadratic formula):
Î» = (2cos Î¸ Â± âˆš(4cosÂ²Î¸ - 4)) / 2
  = cos Î¸ Â± âˆš(cosÂ²Î¸ - 1)
  = cos Î¸ Â± âˆš(-sinÂ²Î¸)
  = cos Î¸ Â± iÂ·sin Î¸
  = e^{Â±iÎ¸}

Complex eigenvalues! No real eigenvectors exist (for Î¸ â‰  0, Ï€).
This makes sense: rotation doesn't preserve any direction.

```

### Example 4: Matrix Power via Eigendecomposition

```
Compute AÂ¹â° where A = [2  1]
                      [0  3]

Step 1: Eigenvalues (diagonal, so obvious)
Î»â‚ = 2, Î»â‚‚ = 3

Step 2: Eigenvectors
For Î»â‚ = 2: [0  1][vâ‚] = [0]  âŸ¹  vâ‚ = [1, 0]áµ€
            [0  1][vâ‚‚]   [0]

For Î»â‚‚ = 3: [-1  1][vâ‚] = [0]  âŸ¹  vâ‚‚ = [1, 1]áµ€
            [ 0  0][vâ‚‚]   [0]

Step 3: Form P and Pâ»Â¹
P = [1  1]    Pâ»Â¹ = [1  -1]
    [0  1]          [0   1]

Step 4: Compute AÂ¹â°
AÂ¹â° = PÎ›Â¹â°Pâ»Â¹ = [1  1][2Â¹â°    0 ][1  -1]
                 [0  1][ 0   3Â¹â°][0   1]
     
     = [1  1][1024     0  ][1  -1]
       [0  1][   0  59049][0   1]
     
     = [1024   59049-1024] = [1024  58025]
       [   0        59049]   [   0  59049]

```

---

## 7. Code Implementation

```python
import numpy as np
import torch

def eigenanalysis(A):
    """Complete eigenvalue analysis of a matrix"""
    n = A.shape[0]
    
    # Check if symmetric
    is_symmetric = np.allclose(A, A.T)
    
    if is_symmetric:
        # Use eigh for symmetric (faster, more stable, guaranteed real)
        eigenvalues, eigenvectors = np.linalg.eigh(A)
    else:
        # General case (may have complex eigenvalues)
        eigenvalues, eigenvectors = np.linalg.eig(A)
    
    print(f"Matrix is {'symmetric' if is_symmetric else 'non-symmetric'}")
    print(f"Eigenvalues: {eigenvalues}")
    print(f"Sum of eigenvalues: {eigenvalues.sum():.6f}")
    print(f"Trace of A: {np.trace(A):.6f}")
    print(f"Product of eigenvalues: {np.prod(eigenvalues):.6f}")
    print(f"Determinant of A: {np.linalg.det(A):.6f}")
    
    return eigenvalues, eigenvectors

def verify_eigen(A, eigenvalues, eigenvectors):
    """Verify eigenvalue equation Av = Î»v"""
    for i in range(len(eigenvalues)):
        v = eigenvectors[:, i]
        lam = eigenvalues[i]
        
        Av = A @ v
        lam_v = lam * v
        
        error = np.linalg.norm(Av - lam_v)
        print(f"Î»_{i+1} = {lam:.4f}, error = {error:.2e}")

def power_iteration(A, num_iterations=100, tol=1e-10):
    """
    Find dominant eigenvalue and eigenvector.
    Returns: (eigenvalue, eigenvector, num_iterations_used)
    """
    n = A.shape[0]
    v = np.random.randn(n)
    v = v / np.linalg.norm(v)
    
    for k in range(num_iterations):
        v_new = A @ v
        v_new = v_new / np.linalg.norm(v_new)
        
        # Check convergence
        if np.linalg.norm(v_new - v) < tol:
            eigenvalue = v_new @ A @ v_new
            return eigenvalue, v_new, k + 1
        
        v = v_new
    
    eigenvalue = v @ A @ v
    return eigenvalue, v, num_iterations

def spectral_decomposition(A):
    """
    Compute A = Î£áµ¢ Î»áµ¢ váµ¢váµ¢áµ€ for symmetric A
    Returns list of (eigenvalue, rank-1 matrix) tuples
    """
    assert np.allclose(A, A.T), "Matrix must be symmetric"
    
    eigenvalues, eigenvectors = np.linalg.eigh(A)
    
    components = []
    for i, lam in enumerate(eigenvalues):
        v = eigenvectors[:, i:i+1]  # Column vector
        component = lam * (v @ v.T)
        components.append((lam, component))
    
    # Verify reconstruction
    A_reconstructed = sum(comp for _, comp in components)
    error = np.linalg.norm(A - A_reconstructed)
    print(f"Reconstruction error: {error:.2e}")
    
    return components

def matrix_function(A, f):
    """
    Compute f(A) using eigendecomposition.
    f(A) = QÂ·f(Î›)Â·Qâ»Â¹
    
    Examples: f = np.exp (matrix exponential)
              f = np.sqrt (matrix square root)
    """
    eigenvalues, eigenvectors = np.linalg.eig(A)
    
    # Apply f to eigenvalues
    f_eigenvalues = f(eigenvalues)
    
    # Reconstruct
    f_A = eigenvectors @ np.diag(f_eigenvalues) @ np.linalg.inv(eigenvectors)
    
    return f_A

# Example usage
if __name__ == "__main__":
    # Symmetric matrix example
    A = np.array([[3, 1], [1, 3]])
    
    print("=" * 50)
    print("Eigenvalue Analysis")
    print("=" * 50)
    eigenvalues, eigenvectors = eigenanalysis(A)
    
    print("\n" + "=" * 50)
    print("Verification")
    print("=" * 50)
    verify_eigen(A, eigenvalues, eigenvectors)
    
    print("\n" + "=" * 50)
    print("Power Iteration")
    print("=" * 50)
    lam, v, iters = power_iteration(A)
    print(f"Dominant eigenvalue: {lam:.6f} (found in {iters} iterations)")
    print(f"True dominant eigenvalue: {eigenvalues.max():.6f}")
    
    print("\n" + "=" * 50)
    print("Matrix Exponential")
    print("=" * 50)
    exp_A = matrix_function(A, np.exp)
    print(f"exp(A) = \n{exp_A}")

```

### PyTorch GPU Implementation

```python
import torch

def eigen_torch(A_tensor):
    """Eigendecomposition with GPU support"""
    # For symmetric matrices
    if torch.allclose(A_tensor, A_tensor.T):
        eigenvalues, eigenvectors = torch.linalg.eigh(A_tensor)
    else:
        eigenvalues, eigenvectors = torch.linalg.eig(A_tensor)
    
    return eigenvalues, eigenvectors

# GPU example
device = 'cuda' if torch.cuda.is_available() else 'cpu'
A_gpu = torch.randn(1000, 1000, device=device)
A_gpu = A_gpu @ A_gpu.T  # Make symmetric
eigenvalues, eigenvectors = torch.linalg.eigh(A_gpu)

```

---

## 8. ML Applications

### ğŸ¤– Application 1: Principal Component Analysis (PCA)

```python
def pca_via_eigen(X, n_components):
    """
    PCA using eigendecomposition of covariance matrix.
    Principal components = eigenvectors with largest eigenvalues.
    """
    # Center the data
    X_centered = X - X.mean(axis=0)
    
    # Compute covariance matrix
    n = len(X)
    cov = X_centered.T @ X_centered / (n - 1)
    
    # Eigendecomposition (cov is symmetric PSD)
    eigenvalues, eigenvectors = np.linalg.eigh(cov)
    
    # Sort by eigenvalue (descending)
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # Select top components
    components = eigenvectors[:, :n_components]
    
    # Project data
    X_projected = X_centered @ components
    
    # Explained variance ratio
    explained_var_ratio = eigenvalues[:n_components] / eigenvalues.sum()
    
    return X_projected, components, explained_var_ratio

```

### ğŸ¤– Application 2: PageRank

```python
def pagerank(adjacency_matrix, damping=0.85, max_iter=100):
    """
    PageRank: Find dominant eigenvector of Google matrix.
    
    G = dÂ·P + (1-d)/nÂ·J  where P = normalized adjacency
    PageRank = principal eigenvector of G
    """
    n = adjacency_matrix.shape[0]
    
    # Normalize columns (make stochastic)
    out_degree = adjacency_matrix.sum(axis=0)
    out_degree[out_degree == 0] = 1  # Handle dangling nodes
    P = adjacency_matrix / out_degree
    
    # Google matrix
    J = np.ones((n, n)) / n
    G = damping * P + (1 - damping) * J
    
    # Power iteration for dominant eigenvector
    rank = np.ones(n) / n
    for _ in range(max_iter):
        rank = G @ rank
        rank = rank / rank.sum()
    
    return rank

```

### ğŸ¤– Application 3: RNN Gradient Stability

```python
def check_rnn_stability(W_hh):
    """
    Check RNN stability via eigenvalues of hidden-to-hidden weight.
    
    For stable training:
    - All |Î»áµ¢| < 1: Vanishing gradients
    - Any |Î»áµ¢| > 1: Exploding gradients
    - All |Î»áµ¢| â‰ˆ 1: Ideal (hard to achieve)
    """
    eigenvalues = np.linalg.eigvals(W_hh)
    spectral_radius = np.max(np.abs(eigenvalues))
    
    if spectral_radius > 1:
        print(f"âš ï¸ Spectral radius = {spectral_radius:.4f} > 1")
        print("Risk of EXPLODING gradients!")
    elif spectral_radius < 0.9:
        print(f"âš ï¸ Spectral radius = {spectral_radius:.4f} < 0.9")
        print("Risk of VANISHING gradients!")
    else:
        print(f"âœ“ Spectral radius = {spectral_radius:.4f}")
        print("Gradients should be stable.")
    
    return spectral_radius

```

### ğŸ¤– Application 4: Spectral Clustering

```python
def spectral_clustering(adjacency, n_clusters):
    """
    Spectral clustering using graph Laplacian eigenvectors.
    """
    # Degree matrix
    D = np.diag(adjacency.sum(axis=1))
    
    # Laplacian L = D - A
    L = D - adjacency
    
    # Normalized Laplacian (more stable)
    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))
    L_normalized = D_inv_sqrt @ L @ D_inv_sqrt
    
    # Smallest eigenvectors (excluding the trivial one)
    eigenvalues, eigenvectors = np.linalg.eigh(L_normalized)
    
    # Use k smallest non-trivial eigenvectors
    embedding = eigenvectors[:, 1:n_clusters+1]
    
    # Cluster in this embedding space (e.g., k-means)
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embedding)
    
    return labels, embedding

```

---

## âš ï¸ Common Mistakes

### âŒ Mistake 1: Confusing Eigenvalues with Singular Values

```python
# Eigenvalues: only for SQUARE matrices
# Singular values: for ANY matrix

A = np.random.randn(5, 3)  # Rectangular
# eigenvalues = np.linalg.eig(A)  # ERROR!
U, singular_values, Vt = np.linalg.svd(A)  # Correct

# For square A: singular values = |eigenvalues| only if A is normal (AAáµ€ = Aáµ€A)

```

### âŒ Mistake 2: Expecting Real Eigenvalues for Non-Symmetric Matrices

```python
# Non-symmetric matrices can have complex eigenvalues
A = np.array([[0, -1], [1, 0]])  # 90Â° rotation
eigenvalues = np.linalg.eigvals(A)
print(eigenvalues)  # [0+1j, 0-1j] - complex!

```

### âŒ Mistake 3: Not Checking for Defective Matrices

```python
# Not all matrices are diagonalizable!
A = np.array([[1, 1], [0, 1]])  # Jordan block
eigenvalues, eigenvectors = np.linalg.eig(A)
# Only ONE eigenvector exists, but we get two (numerically corrupted)

```

---

## ğŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ğŸ¥ | [3Blue1Brown: Eigenvectors](https://www.youtube.com/watch?v=PFDu9oVAE-g) | Visual intuition |
| ğŸ“– | [Linear Algebra Done Right](https://linear.axler.net/) | Theoretical treatment |
| ğŸ¥ | [MIT 18.06](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/) | Strang's lectures |

---

## ğŸ—ºï¸ Navigation

<p align="center">

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Dimensionality Reduction](../02_dimensionality_reduction/README.md) | [Linear Algebra](../README.md) | [Eigenvalues Advanced](../04_eigenvalues/README.md) |

</p>

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
