<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Matrix%20Factorization&fontSize=38&fontColor=fff&animation=twinkling&fontAlignY=32&desc=NMF%20Â·%20Recommender%20Systems%20Â·%20Topic%20Modeling&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ðŸ“š_Section-01.05_Matrix_Factorization-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ðŸ“Š_Topics-NMF_Recommenders-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ðŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **Matrix factorization decomposes a matrix into simpler factors.** This is the foundation of recommender systems, topic modeling, and signal separation.

- ðŸŽ¬ **Recommenders**: User-item ratings â‰ˆ User preferences Ã— Item features

- ðŸ“° **Topic Modeling**: Document-word matrix â‰ˆ Document-topic Ã— Topic-word

- âž• **NMF**: Non-negative factorization for interpretable parts

---

## ðŸ“‘ Table of Contents

1. [Overview](#1-overview)

2. [Low-Rank Matrix Factorization](#2-low-rank-matrix-factorization)

3. [Non-Negative Matrix Factorization](#3-non-negative-matrix-factorization-nmf)

4. [Recommender Systems](#4-recommender-systems)

5. [Code Implementation](#5-code-implementation)

6. [Resources](#-resources)

---

## ðŸŽ¨ Visual Overview

<img src="./images/matrix-factorization-recommender.svg" width="100%">

```
+-----------------------------------------------------------------------------+
|                     MATRIX FACTORIZATION FOR RECOMMENDATIONS                 |
+-----------------------------------------------------------------------------+
|                                                                              |
|   RATINGS MATRIX R        â‰ˆ      USER FACTORS U    Ã—    ITEM FACTORS V      |
|   (n_users Ã— n_items)           (n_users Ã— k)          (k Ã— n_items)        |
|                                                                              |
|   +-----------------+       +-----------+       +-----------------+        |
|   | ? 5 ? 3 ? 1 ? |       | uâ‚áµ€      |       | vâ‚ vâ‚‚ ... vâ‚˜   |        |
|   | 4 ? ? ? 2 ? 5 |   â‰ˆ   | uâ‚‚áµ€      |   Ã—   |                 |        |
|   | ? ? 3 ? ? 4 ? |       | ...       |       |    k Ã— m        |        |
|   | 1 ? ? 5 ? ? 2 |       | uâ‚™áµ€      |       |                 |        |
|   +-----------------+       +-----------+       +-----------------+        |
|      Known ratings           User latent         Item latent                |
|      ? = unknown             features            features                   |
|                                                                              |
|   PREDICTION:                                                                |
|   -------------                                                              |
|   rÌ‚áµ¢â±¼ = uáµ¢áµ€ vâ±¼ = Î£â‚– uáµ¢â‚– vâ‚–â±¼                                                |
|                                                                              |
|   User i's preference for item j = dot product of their latent vectors      |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. Overview

### ðŸ“Œ General Problem

Given matrix $R \in \mathbb{R}^{m \times n}$, find factors:

```math
R \approx UV^T

```

where $U \in \mathbb{R}^{m \times k}$ and $V \in \mathbb{R}^{n \times k}$ with $k \ll \min(m, n)$.

### ðŸ“ Relation to SVD

SVD gives the optimal low-rank approximation:

```math
A = U\Sigma V^T \approx U_k \Sigma_k V_k^T

```

But SVD requires:

- Complete matrix (no missing entries)

- No constraints (allows negative values)

Matrix factorization methods relax these.

---

## 2. Low-Rank Matrix Factorization

### ðŸ“Œ Problem Formulation

```math
\min_{U, V} \|R - UV^T\|_F^2 + \lambda(\|U\|_F^2 + \|V\|_F^2)

```

- Frobenius norm: $\|A\|_F = \sqrt{\sum_{ij} A_{ij}^2}$

- Regularization $\lambda$ prevents overfitting

### ðŸ” Alternating Least Squares (ALS)

```
Initialize U, V randomly

Repeat until convergence:
    # Fix V, solve for U
    For each user i:
        uáµ¢ = (Váµ€V + Î»I)â»Â¹ Váµ€ ráµ¢
        where ráµ¢ = ratings by user i
    
    # Fix U, solve for V
    For each item j:
        vâ±¼ = (Uáµ€U + Î»I)â»Â¹ Uáµ€ râ±¼
        where râ±¼ = ratings of item j

```

**Why ALS?**
- With one factor fixed, problem is convex (quadratic)

- Each sub-problem has closed-form solution

- Easily parallelizable across users/items

---

## 3. Non-Negative Matrix Factorization (NMF)

### ðŸ“Œ Problem

```math
\min_{U \geq 0, V \geq 0} \|R - UV^T\|_F^2

```

subject to $U_{ij} \geq 0$ and $V_{ij} \geq 0$.

### ðŸ” Why Non-Negativity?

```
Non-negativity gives INTERPRETABLE parts-based representation:

Example: Face recognition
  R = images (pixels Ã— images)
  U = basis images (parts: eyes, nose, mouth)
  V = coefficients (how much of each part)

  Face â‰ˆ 0.3Ã—(eyes) + 0.5Ã—(nose) + 0.2Ã—(mouth)

With negative values (like SVD), parts could "cancel out"
â†’ Less interpretable

```

### ðŸ“ Multiplicative Update Rules

```
Initialize U, V with random positive values

Repeat until convergence:
    # Update U
    Uáµ¢â‚– â† Uáµ¢â‚– Ã— (RV)áµ¢â‚– / (UV^TV)áµ¢â‚–
    
    # Update V
    Vâ±¼â‚– â† Vâ±¼â‚– Ã— (R^TU)â±¼â‚– / (VU^TU)â±¼â‚–

These updates:

1. Keep values non-negative (positive Ã— positive)

2. Decrease the objective function

3. Converge to a local minimum

```

### ðŸ” Proof: Updates Decrease Objective

```
The objective D = â€–R - UV^Tâ€–Â²_F

For the update rule Uáµ¢â‚– â† Uáµ¢â‚– Ã— (RV)áµ¢â‚– / (UV^TV)áµ¢â‚–:

Using auxiliary function technique (Lee & Seung, 2001):

1. Define G(U, U') â‰¥ D(U) with equality at U = U'

2. New U minimizes G(U, U^old)

3. D(U^new) â‰¤ G(U^new, U^old) â‰¤ G(U^old, U^old) = D(U^old)

Therefore objective decreases (or stays same) at each step.  âˆŽ

```

---

## 4. Recommender Systems

### ðŸ“Œ Problem Setup

- Users: $m$ users

- Items: $n$ items

- Ratings: $R_{ij}$ = rating user $i$ gave item $j$ (only some known)

### ðŸ“ Matrix Factorization Model

```math
\hat{r}_{ij} = b + b_i + b_j + \mathbf{u}_i^T \mathbf{v}_j

```

where:

- $b$: global bias

- $b_i$: user bias

- $b_j$: item bias

- $\mathbf{u}_i$: user latent factors

- $\mathbf{v}_j$: item latent factors

### ðŸ” Optimization

Minimize over observed ratings $\Omega$:

```math
\min \sum_{(i,j) \in \Omega} (r_{ij} - \hat{r}_{ij})^2 + \lambda(\|U\|_F^2 + \|V\|_F^2 + \sum_i b_i^2 + \sum_j b_j^2)

```

### ðŸ’¡ Example: Netflix

```
User 1 watches: Action, Sci-Fi movies (high ratings)
User 2 watches: Romance, Comedy movies (high ratings)

Latent factors might capture:
  Dimension 1: Action vs Romance preference
  Dimension 2: Old vs New movies
  Dimension 3: Serious vs Light tone
  ...

User vectors encode preferences along these dimensions
Movie vectors encode characteristics along same dimensions

```

---

## 5. Code Implementation

```python
import numpy as np
from sklearn.decomposition import NMF

class MatrixFactorization:
    """Matrix factorization for recommender systems."""
    
    def __init__(self, n_factors=10, learning_rate=0.01, reg=0.01, n_epochs=100):
        self.n_factors = n_factors
        self.lr = learning_rate
        self.reg = reg
        self.n_epochs = n_epochs
    
    def fit(self, R):
        """
        Fit the model using Stochastic Gradient Descent.
        R: Rating matrix (0 = missing)
        """
        self.n_users, self.n_items = R.shape
        
        # Initialize factors
        self.U = np.random.normal(0, 0.1, (self.n_users, self.n_factors))
        self.V = np.random.normal(0, 0.1, (self.n_items, self.n_factors))
        
        # Biases
        self.b = np.mean(R[R > 0])
        self.b_u = np.zeros(self.n_users)
        self.b_i = np.zeros(self.n_items)
        
        # Get observed indices
        observed = np.where(R > 0)
        
        for epoch in range(self.n_epochs):
            for i, j in zip(*observed):
                # Prediction
                pred = self.predict_single(i, j)
                error = R[i, j] - pred
                
                # Update biases
                self.b_u[i] += self.lr * (error - self.reg * self.b_u[i])
                self.b_i[j] += self.lr * (error - self.reg * self.b_i[j])
                
                # Update factors
                self.U[i] += self.lr * (error * self.V[j] - self.reg * self.U[i])
                self.V[j] += self.lr * (error * self.U[i] - self.reg * self.V[j])
            
            if epoch % 10 == 0:
                loss = self.compute_loss(R, observed)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def predict_single(self, i, j):
        """Predict rating for user i, item j."""
        return self.b + self.b_u[i] + self.b_i[j] + np.dot(self.U[i], self.V[j])
    
    def predict(self):
        """Predict all ratings."""
        return self.b + self.b_u[:, np.newaxis] + self.b_i + self.U @ self.V.T
    
    def compute_loss(self, R, observed):
        """Compute regularized loss."""
        loss = 0
        for i, j in zip(*observed):
            loss += (R[i, j] - self.predict_single(i, j))**2
        loss += self.reg * (np.sum(self.U**2) + np.sum(self.V**2))
        return loss

def nmf_topic_modeling(document_term_matrix, n_topics=10):
    """
    NMF for topic modeling.
    
    Document-Term Matrix â‰ˆ Document-Topic Ã— Topic-Term
    """
    nmf = NMF(n_components=n_topics, init='nndsvd', random_state=42)
    
    # W: document-topic matrix
    # H: topic-term matrix
    W = nmf.fit_transform(document_term_matrix)
    H = nmf.components_
    
    return W, H, nmf

# Example usage
np.random.seed(42)

# Simulate ratings (0 = missing)
R = np.random.randint(0, 6, (100, 50))
R[np.random.rand(*R.shape) < 0.7] = 0  # 70% missing

mf = MatrixFactorization(n_factors=10, n_epochs=100)
mf.fit(R)

# Predict missing ratings
predictions = mf.predict()
print(f"Predicted ratings range: [{predictions.min():.2f}, {predictions.max():.2f}]")

```

---

## ðŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ðŸ“„ | [Matrix Factorization for Recommenders](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf) | Netflix paper |
| ðŸ“„ | [NMF (Lee & Seung)](https://www.nature.com/articles/44565) | Original NMF |
| ðŸŽ¥ | [Recommender Systems](https://www.coursera.org/learn/recommender-systems) | Coursera |

---

## ðŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ðŸ  Home | âž¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Eigenvalues](../04_eigenvalues/README.md) | [Linear Algebra](../README.md) | [Matrix Properties](../06_matrix_properties/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
