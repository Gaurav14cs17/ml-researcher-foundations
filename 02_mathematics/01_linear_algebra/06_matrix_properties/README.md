<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Matrix%20Properties&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=32&desc=Rank%20Â·%20Determinant%20Â·%20Trace%20Â·%20Condition%20Number&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-01.06_Matrix_Properties-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-Rank_Det_Trace_Cond-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **Matrix properties reveal essential information for debugging and analysis.** Rank tells you about solvability, condition number about numerical stability.

- ğŸ“Š **Rank**: Dimension of column/row space â€” is the system solvable?

- ğŸ”¢ **Determinant**: Volume scaling â€” is the matrix invertible?

- â• **Trace**: Sum of eigenvalues â€” quick diagnostic

- âš ï¸ **Condition Number**: Numerical stability â€” will my algorithm fail?

---

## ğŸ“‘ Table of Contents

1. [Rank](#1-rank)

2. [Determinant](#2-determinant)

3. [Trace](#3-trace)

4. [Condition Number](#4-condition-number)

5. [Positive Definiteness](#5-positive-definiteness)

6. [Code Implementation](#6-code-implementation)

7. [Resources](#-resources)

---

## ğŸ¨ Visual Overview

<img src="./images/matrix-properties.svg" width="100%">

---

## 1. Rank

### ğŸ“Œ Definition

```math
\text{rank}(A) = \dim(\text{column space}) = \dim(\text{row space})

```

Equivalently: number of non-zero singular values.

### ğŸ“ Key Properties

| Property | Formula | Significance |
|----------|---------|--------------|
| Max rank | $\text{rank}(A) \leq \min(m, n)$ | Bounded by smaller dimension |
| Product | $\text{rank}(AB) \leq \min(\text{rank}(A), \text{rank}(B))$ | Can't increase rank |
| Sum | $\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B)$ | Subadditivity |
| Transpose | $\text{rank}(A) = \text{rank}(A^T)$ | Row rank = column rank |

### ğŸ” Proof: Row Rank = Column Rank

```
Let r = dim(row space of A) and c = dim(column space of A).

Step 1: Row operations don't change row space
        Reduce A to row echelon form R
        R has r non-zero rows
        These r rows are linearly independent
        So row space of R (= row space of A) has dimension r

Step 2: Column operations don't change column space
        Non-zero rows of R each have leading 1 in different column
        The c corresponding columns of A are linearly independent
        (They reduce to the standard basis vectors)
        So column space has dimension â‰¥ r

Step 3: Similarly, column space dimension â‰¤ r
        The r pivot columns span the column space

Therefore: row rank = column rank = r  âˆ

```

### ğŸ’¡ Examples

**Example 1**: Compute Rank

```
A = [1  2  3]
    [2  4  6]
    [0  1  2]

Row reduce:
Râ‚‚ - 2Râ‚: [1  2  3]
          [0  0  0]
          [0  1  2]

Swap Râ‚‚, Râ‚ƒ: [1  2  3]
             [0  1  2]
             [0  0  0]

Two non-zero rows â†’ rank(A) = 2

```

**Example 2**: Rank in ML (LoRA)

```
Weight matrix W: (768 Ã— 768) â†’ rank up to 768
LoRA update Î”W = BA where B: (768 Ã— 4), A: (4 Ã— 768)

rank(Î”W) = rank(BA) â‰¤ min(rank(B), rank(A)) â‰¤ 4

Only 4 degrees of freedom for update! (vs 589K for full W)

```

---

## 2. Determinant

### ğŸ“Œ Definition

The determinant is the unique function $\det: \mathbb{R}^{n \times n} \to \mathbb{R}$ satisfying:

1. Multilinear in columns

2. Alternating (swapping columns negates)

3. $\det(I) = 1$

### ğŸ“ Key Properties

| Property | Formula |
|----------|---------|
| Multiplicative | $\det(AB) = \det(A)\det(B)$ |
| Transpose | $\det(A^T) = \det(A)$ |
| Inverse | $\det(A^{-1}) = 1/\det(A)$ |
| Scaling | $\det(cA) = c^n \det(A)$ |
| Eigenvalues | $\det(A) = \prod_i \lambda_i$ |

### ğŸ” Geometric Interpretation

```
|det(A)| = volume scaling factor

For 2Ã—2 matrix A = [a b]:
                   [c d]

det(A) = ad - bc

â€¢ |det(A)| = area of parallelogram with sides (a,c) and (b,d)
â€¢ det(A) > 0: preserves orientation
â€¢ det(A) < 0: reverses orientation (reflection)
â€¢ det(A) = 0: collapses to lower dimension (singular)

```

### ğŸ’¡ Examples

**Example 1**: 2Ã—2 Determinant

```
A = [3  1]
    [2  4]

det(A) = 3Ã—4 - 1Ã—2 = 12 - 2 = 10

Geometric: Area of parallelogram = 10 square units

```

**Example 2**: Determinant via Eigenvalues

```
A = [2  1]
    [1  2]

Eigenvalues: Î»â‚ = 3, Î»â‚‚ = 1

det(A) = Î»â‚ Ã— Î»â‚‚ = 3 Ã— 1 = 3

Verify: det(A) = 2Ã—2 - 1Ã—1 = 4 - 1 = 3 âœ“

```

---

## 3. Trace

### ğŸ“Œ Definition

```math
\text{tr}(A) = \sum_{i=1}^{n} A_{ii}

```

### ğŸ“ Key Properties

| Property | Formula | Proof Sketch |
|----------|---------|--------------|
| Linearity | $\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$ | Sum of sums |
| Cyclic | $\text{tr}(ABC) = \text{tr}(CAB) = \text{tr}(BCA)$ | Index manipulation |
| Transpose | $\text{tr}(A) = \text{tr}(A^T)$ | Diagonal unchanged |
| Eigenvalues | $\text{tr}(A) = \sum_i \lambda_i$ | Characteristic poly |
| Frobenius | $\|A\|_F^2 = \text{tr}(A^TA)$ | By definition |

### ğŸ” Proof: Cyclic Property

```
tr(AB) = Î£áµ¢ (AB)áµ¢áµ¢ = Î£áµ¢ Î£â±¼ Aáµ¢â±¼Bâ±¼áµ¢

tr(BA) = Î£â±¼ (BA)â±¼â±¼ = Î£â±¼ Î£áµ¢ Bâ±¼áµ¢Aáµ¢â±¼

These are the same double sum, just indexed differently!

For three matrices:
tr(ABC) = tr((AB)C) = tr(C(AB)) = tr(CAB)  âˆ

```

### ğŸ’¡ Applications

**Efficient Gradient Computation**:

```python
# Goal: Compute tr(ABáµ€C)
# Naive: O(nÂ³) to form products, O(n) for trace

# Efficient: tr(ABáµ€C) = Î£áµ¢â±¼â‚– Aáµ¢â±¼ Bâ‚–â±¼ Câ‚–áµ¢
# Use: tr(ABáµ€C) = tr((ABáµ€)C) = Î£áµ¢ ((ABáµ€)C)áµ¢áµ¢

# Even better with element-wise:
# tr(ABáµ€) = Î£áµ¢â±¼ Aáµ¢â±¼Báµ¢â±¼ = sum(A * B)

```

---

## 4. Condition Number

### ğŸ“Œ Definition

```math
\kappa(A) = \|A\| \cdot \|A^{-1}\| = \frac{\sigma_{\max}}{\sigma_{\min}}

```

### ğŸ“ Interpretation

```
Condition number measures sensitivity to perturbation:

Solving Ax = b with perturbed b + Î´b:
  A(x + Î´x) = b + Î´b
  AÎ´x = Î´b
  Î´x = Aâ»Â¹Î´b

Relative error:
  â€–Î´xâ€–/â€–xâ€– â‰¤ Îº(A) Ã— â€–Î´bâ€–/â€–bâ€–

So:
â€¢ Îº(A) â‰ˆ 1: Well-conditioned (errors don't amplify)
â€¢ Îº(A) >> 1: Ill-conditioned (small errors â†’ large errors)
â€¢ Îº(A) = âˆ: Singular (non-invertible)

Rule of thumb: Lose logâ‚â‚€(Îº) digits of precision
  Îº = 10â¶ â†’ lose 6 digits (from 16 to 10 significant digits)

```

### ğŸ’¡ Examples

**Example 1**: Hilbert Matrix (Notoriously Ill-Conditioned)

```
H_{ij} = 1/(i+j-1)

Hâ‚ƒ = [1    1/2  1/3]
     [1/2  1/3  1/4]
     [1/3  1/4  1/5]

Îº(Hâ‚ƒ) â‰ˆ 524
Îº(Hâ‚…) â‰ˆ 476,000
Îº(Hâ‚â‚€) â‰ˆ 10Â¹Â³

Almost impossible to invert numerically for n > 10!

```

**Example 2**: Orthogonal Matrix (Perfectly Conditioned)

```
Q is orthogonal âŸ¹ Ïƒáµ¢ = 1 for all i
âŸ¹ Îº(Q) = 1/1 = 1

This is why QR decomposition is numerically stable!

```

---

## 5. Positive Definiteness

### ğŸ“Œ Definition

A symmetric matrix $A$ is:

- **Positive Definite** ($A \succ 0$) if $\mathbf{x}^TA\mathbf{x} > 0$ for all $\mathbf{x} \neq 0$

- **Positive Semi-Definite** ($A \succeq 0$) if $\mathbf{x}^TA\mathbf{x} \geq 0$ for all $\mathbf{x}$

### ğŸ“ Equivalent Conditions

| Condition | Test |
|-----------|------|
| Definition | $\mathbf{x}^TA\mathbf{x} > 0$ for all $\mathbf{x} \neq 0$ |
| Eigenvalues | All $\lambda_i > 0$ |
| Cholesky | $A = LL^T$ exists (with positive diagonal $L$) |
| Sylvester | All leading principal minors $> 0$ |
| Factorization | $A = B^TB$ for some full-rank $B$ |

### ğŸ” Proof: PD âŸº All Eigenvalues > 0

```
(âŸ¹) Suppose A â‰» 0
For any eigenvalue Î» with eigenvector v:
  Av = Î»v
  váµ€Av = Î»váµ€v = Î»â€–vâ€–Â²

Since A â‰» 0: váµ€Av > 0
Since v â‰  0: â€–vâ€–Â² > 0
Therefore: Î» > 0  âœ“

(âŸ¸) Suppose all Î»áµ¢ > 0
For any x â‰  0, write x = Î£áµ¢ Î±áµ¢váµ¢ (eigenbasis)
  xáµ€Ax = (Î£áµ¢ Î±áµ¢váµ¢)áµ€ A (Î£â±¼ Î±â±¼vâ±¼)
       = Î£áµ¢ Î£â±¼ Î±áµ¢Î±â±¼ váµ¢áµ€Avâ±¼
       = Î£áµ¢ Î±áµ¢Â² Î»áµ¢  (since váµ¢áµ€vâ±¼ = Î´áµ¢â±¼ and Avâ±¼ = Î»â±¼vâ±¼)
       > 0  (all Î»áµ¢ > 0 and not all Î±áµ¢ = 0)  âœ“

```

### ğŸ’¡ Applications

**Covariance Matrices are PSD**:

```
Î£ = E[(X - Î¼)(X - Î¼)áµ€]

For any vector a:
  aáµ€Î£a = aáµ€E[(X - Î¼)(X - Î¼)áµ€]a
       = E[aáµ€(X - Î¼)(X - Î¼)áµ€a]
       = E[(aáµ€(X - Î¼))Â²]
       â‰¥ 0

So covariance matrices are always PSD!

```

**Hessian Analysis for Optimization**:

```
At critical point x* where âˆ‡f(x*) = 0:

â€¢ H(x*) â‰» 0 â†’ x* is strict local minimum
â€¢ H(x*) â‰º 0 â†’ x* is strict local maximum
â€¢ H(x*) indefinite â†’ x* is saddle point

```

---

## 6. Code Implementation

```python
import numpy as np

def matrix_properties(A):
    """Compute comprehensive matrix properties."""
    results = {}
    
    # Basic properties
    results['shape'] = A.shape
    results['rank'] = np.linalg.matrix_rank(A)
    results['trace'] = np.trace(A)
    
    if A.shape[0] == A.shape[1]:  # Square matrix
        results['determinant'] = np.linalg.det(A)
        results['is_singular'] = np.abs(results['determinant']) < 1e-10
        
        # Eigenvalues
        eigenvalues = np.linalg.eigvals(A)
        results['eigenvalues'] = eigenvalues
        results['spectral_radius'] = np.max(np.abs(eigenvalues))
        
        # Condition number
        if not results['is_singular']:
            results['condition_number'] = np.linalg.cond(A)
        else:
            results['condition_number'] = np.inf
        
        # Symmetry check
        results['is_symmetric'] = np.allclose(A, A.T)
        
        if results['is_symmetric']:
            # Positive definiteness
            eigenvalues_real = np.linalg.eigvalsh(A)
            results['is_positive_definite'] = np.all(eigenvalues_real > 0)
            results['is_positive_semidefinite'] = np.all(eigenvalues_real >= -1e-10)
    
    return results

def check_invertibility(A, tol=1e-10):
    """Check various invertibility criteria."""
    if A.shape[0] != A.shape[1]:
        return {"invertible": False, "reason": "Not square"}
    
    det = np.linalg.det(A)
    rank = np.linalg.matrix_rank(A)
    n = A.shape[0]
    
    if np.abs(det) < tol:
        return {"invertible": False, "reason": f"det â‰ˆ 0 ({det:.2e})"}
    if rank < n:
        return {"invertible": False, "reason": f"rank {rank} < {n}"}
    
    cond = np.linalg.cond(A)
    if cond > 1e15:
        return {"invertible": True, "reason": f"âš ï¸ Very ill-conditioned (Îº={cond:.2e})"}
    
    return {"invertible": True, "condition_number": cond}

def stability_analysis(A):
    """Analyze numerical stability of operations with A."""
    cond = np.linalg.cond(A)
    
    print(f"Condition number: {cond:.2e}")
    print(f"Expected precision loss: ~{np.log10(cond):.1f} digits")
    
    if cond < 10:
        print("âœ“ Excellent conditioning")
    elif cond < 1e4:
        print("âœ“ Good conditioning")
    elif cond < 1e8:
        print("âš ï¸ Moderate conditioning - use caution")
    elif cond < 1e15:
        print("âš ï¸ Poor conditioning - results may be inaccurate")
    else:
        print("â›” Severe ill-conditioning - consider regularization")
    
    return cond

# Example
A = np.array([[4, 2, 1],
              [2, 5, 3],
              [1, 3, 6]])

print("=== Matrix Properties ===")
props = matrix_properties(A)
for key, value in props.items():
    if isinstance(value, np.ndarray):
        print(f"{key}: {value}")
    else:
        print(f"{key}: {value}")

print("\n=== Stability Analysis ===")
stability_analysis(A)

```

---

## ğŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ğŸ“– | [Matrix Computations](https://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm) | Golub & Van Loan |
| ğŸ¥ | [3Blue1Brown](https://www.youtube.com/watch?v=Ip3X9LOh2dk) | Determinant |
| ğŸ“– | [Numerical Recipes](http://numerical.recipes/) | Conditioning |

---

## ğŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Matrix Factorization](../05_matrix_factorization/README.md) | [Linear Algebra](../README.md) | [SVD](../07_svd/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
