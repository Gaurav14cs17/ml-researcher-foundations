<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Matrix%20Decompositions&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=32&desc=SVD%20Â·%20Eigendecomposition%20Â·%20QR%20Â·%20Cholesky%20Â·%20LU&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-01.01_Decompositions-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-5_Decompositions-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **Matrix decompositions break complex matrices into simpler, structured components.** They are the computational backbone of ML - enabling PCA, LoRA, solving linear systems, and much more.

- ğŸ”¥ **SVD** (`A = UÎ£Váµ€`): Works for ANY matrix, enables low-rank approximation (LoRA!)

- ğŸ“ **Eigendecomposition** (`A = QÎ›Qâ»Â¹`): Square matrices, powers, PCA

- ğŸ”„ **QR** (`A = QR`): Numerically stable least squares

- ğŸ“Š **Cholesky** (`A = LLáµ€`): Fast for positive definite matrices

- ğŸ§® **LU** (`A = LU`): General linear system solving

---

## ğŸ“‘ Table of Contents

1. [Visual Overview](#-visual-overview)

2. [Singular Value Decomposition (SVD)](#1-singular-value-decomposition-svd)

3. [Eigendecomposition](#2-eigendecomposition)

4. [QR Decomposition](#3-qr-decomposition)

5. [Cholesky Decomposition](#4-cholesky-decomposition)

6. [LU Decomposition](#5-lu-decomposition)

7. [Comparison Table](#-comparison-which-decomposition-to-use)

8. [Code Examples](#-complete-code-examples)

9. [Common Mistakes](#-common-mistakes)

10. [Resources](#-resources)

---

## ğŸ¨ Visual Overview

<img src="./images/svd-decomposition-visual.svg" width="100%">

```
+-----------------------------------------------------------------------------+
|                     MATRIX DECOMPOSITION LANDSCAPE                           |
+-----------------------------------------------------------------------------+
|                                                                              |
|                              ANY MATRIX                                      |
|                                  |                                           |
|                    +-------------+-------------+                            |
|                    â–¼             â–¼             â–¼                            |
|               +--------+   +--------+   +--------+                          |
|               |  SVD   |   |   QR   |   |   LU   |                          |
|               | A=UÎ£Váµ€ |   | A=QR   |   | A=LU   |                          |
|               +--------+   +--------+   +--------+                          |
|                    |                         |                               |
|                    |         SQUARE          |                               |
|                    |           |             |                               |
|                    |     +-----+-----+       |                               |
|                    |     â–¼           â–¼       |                               |
|                    | +-------+  +--------+   |                               |
|                    | | Eigen |  |Schur   |   |                               |
|                    | |A=QÎ›Qâ»Â¹|  |A=QTQ*  |   |                               |
|                    | +-------+  +--------+   |                               |
|                    |     |                   |                               |
|                    |     |    SYMMETRIC PD   |                               |
|                    |     |         |         |                               |
|                    |     |    +----+----+    |                               |
|                    |     |    â–¼         |    |                               |
|                    |     | +--------+   |    |                               |
|                    |     | |Cholesky|   |    |                               |
|                    |     | |A=LLáµ€   |   |    |                               |
|                    |     | +--------+   |    |                               |
|                    |     |              |    |                               |
|                    â–¼     â–¼              â–¼    â–¼                               |
|               +----------------------------------+                          |
|               |         ML APPLICATIONS          |                          |
|               |  PCA, LoRA, Regression, GPs...   |                          |
|               +----------------------------------+                          |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. Singular Value Decomposition (SVD)

### ğŸ“Œ Theorem

**For ANY matrix** $A \in \mathbb{R}^{m \times n}$:

```math
A = U\Sigma V^T

```

where:

- $U \in \mathbb{R}^{m \times m}$: orthogonal matrix (left singular vectors)

- $\Sigma \in \mathbb{R}^{m \times n}$: diagonal with $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$

- $V \in \mathbb{R}^{n \times n}$: orthogonal matrix (right singular vectors)

```
     A           =        U        Ã—        Î£        Ã—        Váµ€
   (mÃ—n)              (mÃ—m)            (mÃ—n)            (nÃ—n)
   
+---------+       +---------+     +---------+     +---------+

|         |       |         |     |Ïƒâ‚       |     |         |
|  Input  |   =   | Rotate  |  Ã—  |  Ïƒâ‚‚     |  Ã—  | Rotate  |
| Matrix  |       |         |     |    â‹±    |     |         |
|         |       |         |     |      Ïƒáµ£ |     |         |
+---------+       +---------+     +---------+     +---------+

```

### ğŸ” Complete Proof of SVD Existence

**Step 1**: Consider $A^TA \in \mathbb{R}^{n \times n}$

$A^TA$ is symmetric positive semi-definite:

```
â€¢ Symmetric: (Aáµ€A)áµ€ = Aáµ€A  âœ“
â€¢ PSD: xáµ€(Aáµ€A)x = (Ax)áµ€(Ax) = â€–Axâ€–Â² â‰¥ 0  âœ“

```

**Step 2**: Apply Spectral Theorem to $A^TA$

```
Since Aáµ€A is symmetric:
  Aáµ€A = VÎ›Váµ€

where:
  V orthogonal (eigenvectors vâ‚, ..., vâ‚™)
  Î› diagonal with Î»áµ¢ â‰¥ 0 (eigenvalues)

Define singular values: Ïƒáµ¢ = âˆšÎ»áµ¢

```

**Step 3**: Construct $U$

```
For each non-zero Ïƒáµ¢, define:
  uáµ¢ = (1/Ïƒáµ¢)Aváµ¢

Verify orthonormality:
  uáµ¢áµ€uâ±¼ = (1/Ïƒáµ¢Ïƒâ±¼)(Aváµ¢)áµ€(Avâ±¼)
        = (1/Ïƒáµ¢Ïƒâ±¼)váµ¢áµ€(Aáµ€A)vâ±¼
        = (1/Ïƒáµ¢Ïƒâ±¼)váµ¢áµ€(Î»â±¼vâ±¼)
        = (Ïƒâ±¼/Ïƒáµ¢)(váµ¢áµ€vâ±¼)
        = Î´áµ¢â±¼  âœ“

Complete {uáµ¢} to orthonormal basis of â„áµ using Gram-Schmidt.

```

**Step 4**: Verify $A = U\Sigma V^T$

```
For each vâ±¼:
  Avâ±¼ = Ïƒâ±¼uâ±¼  (by construction of uâ±¼)

In matrix form:
  AV = UÎ£
  AVVáµ€ = UÎ£Váµ€  (multiply right by Váµ€)
  A = UÎ£Váµ€     (since VVáµ€ = I)  âˆ

```

### ğŸ“ Key Properties

| Property | Formula | Significance |
|----------|---------|--------------|
| Rank | $\text{rank}(A) = \#\{\sigma_i > 0\}$ | Non-zero singular values |
| Frobenius norm | $\|A\|_F = \sqrt{\sum_i \sigma_i^2}$ | Matrix "size" |
| Spectral norm | $\|A\|_2 = \sigma_1$ | Largest singular value |
| Condition number | $\kappa(A) = \sigma_1/\sigma_r$ | Numerical stability |
| Pseudoinverse | $A^+ = V\Sigma^+ U^T$ | Generalized inverse |

### ğŸ” Eckart-Young Theorem (Low-Rank Approximation)

**Theorem**: The best rank-$k$ approximation to $A$ (in Frobenius norm) is:

```math
A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^T

```

**Error**: $\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2$

**Proof Sketch**:

```
Step 1: Any rank-k matrix B has null space of dimension n-k

Step 2: span{vâ‚,...,vâ‚–â‚Šâ‚} has dimension k+1

Step 3: By dimension counting, these spaces intersect
        âˆƒ unit z âˆˆ span{vâ‚,...,vâ‚–â‚Šâ‚} with Bz = 0

Step 4: Then â€–A-Bâ€–Â²_F â‰¥ â€–(A-B)zâ€–Â² = â€–Azâ€–Â² â‰¥ ÏƒÂ²â‚–â‚Šâ‚

Step 5: Aâ‚– achieves this lower bound  âˆ

```

### ğŸ’¡ Examples

**Example 1**: 2Ã—2 Matrix SVD

```
A = [3  0]
    [0  2]

Already diagonal! So:
  Ïƒâ‚ = 3, Ïƒâ‚‚ = 2
  U = I, V = I
  
A = [1 0] [3 0] [1 0]
    [0 1] [0 2] [0 1]

```

**Example 2**: Rank-1 Matrix

```
A = [2  4]     (rank 1: row 2 = 2 Ã— row 1)
    [1  2]

SVD: A = uÏƒváµ€ where
  Ïƒ = âˆš(4+16+1+4) = 5
  v = [2,4]áµ€/âˆš20 = [1,2]áµ€/âˆš5
  u = Av/Ïƒ = [10,5]áµ€/(5âˆš5) = [2,1]áµ€/âˆš5

Verify: A = 5 Ã— [2/âˆš5] Ã— [1/âˆš5, 2/âˆš5]
             [1/âˆš5]

```

**Example 3**: Low-Rank Approximation

```
A = [1  0  0]
    [0  3  0]
    [0  0  0.1]

Singular values: Ïƒâ‚=3, Ïƒâ‚‚=1, Ïƒâ‚ƒ=0.1

Rank-2 approximation (k=2):
Aâ‚‚ = [1  0  0]    (just zero out smallest singular value)
     [0  3  0]
     [0  0  0]

Error: â€–A-Aâ‚‚â€–_F = Ïƒâ‚ƒ = 0.1 (only 10% of smallest component lost)

```

### ğŸ¤– ML Application: LoRA (Low-Rank Adaptation)

```python
# LoRA exploits Eckart-Young: weight updates often have low intrinsic rank
#
# Instead of updating full W (dÃ—k parameters):
#   W' = W + Î”W
#
# LoRA uses low-rank factorization (rÃ—(d+k) parameters, r << min(d,k)):
#   W' = W + BA    where Bâˆˆâ„áµˆË£Ê³, Aâˆˆâ„Ê³Ë£áµ

import torch
import torch.nn as nn

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4, alpha=1.0):
        super().__init__()
        self.rank = rank
        self.scaling = alpha / rank
        
        # Frozen original weights
        self.W = nn.Parameter(torch.randn(out_features, in_features), 
                              requires_grad=False)
        
        # Trainable low-rank matrices
        self.A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.B = nn.Parameter(torch.zeros(out_features, rank))
    
    def forward(self, x):
        # W' = W + scaling * B @ A
        return x @ self.W.T + self.scaling * (x @ self.A.T @ self.B.T)

# Parameter savings:
# Original: 768 Ã— 768 = 589,824 params
# LoRA (r=4): 768Ã—4 + 4Ã—768 = 6,144 params (96Ã— reduction!)

```

---

## 2. Eigendecomposition

### ğŸ“Œ Definition

For a **square matrix** $A \in \mathbb{R}^{n \times n}$:

```math
A\mathbf{v} = \lambda\mathbf{v}

```

If $A$ has $n$ linearly independent eigenvectors:

```math
A = Q\Lambda Q^{-1}

```

For **symmetric** $A$: $A = Q\Lambda Q^T$ (orthogonal $Q$)

### ğŸ” Proof: Symmetric Matrices Have Real Eigenvalues and Orthogonal Eigenvectors

**Part 1: Eigenvalues are Real**

```
Step 1: Let Î» be eigenvalue with eigenvector v (possibly complex)
        Av = Î»v

Step 2: Take conjugate transpose
        v*A = Î»Ì„v*  (since A is real symmetric: A* = Aáµ€ = A)

Step 3: Left-multiply eigenvalue equation by v*
        v*Av = Î»(v*v) = Î»â€–vâ€–Â²

Step 4: Right-multiply Step 2 by v
        v*Av = Î»Ì„(v*v) = Î»Ì„â€–vâ€–Â²

Step 5: Equate: Î»â€–vâ€–Â² = Î»Ì„â€–vâ€–Â²
        Since â€–vâ€–Â² > 0: Î» = Î»Ì„  âŸ¹ Î» is real  âˆ

```

**Part 2: Eigenvectors of Distinct Eigenvalues are Orthogonal**

```
Step 1: Let Avâ‚ = Î»â‚vâ‚ and Avâ‚‚ = Î»â‚‚vâ‚‚ with Î»â‚ â‰  Î»â‚‚

Step 2: Compute vâ‚áµ€Avâ‚‚ two ways:
        Way 1: vâ‚áµ€Avâ‚‚ = vâ‚áµ€(Î»â‚‚vâ‚‚) = Î»â‚‚(vâ‚áµ€vâ‚‚)
        Way 2: vâ‚áµ€Avâ‚‚ = (Avâ‚)áµ€vâ‚‚ = Î»â‚(vâ‚áµ€vâ‚‚)  (using Aáµ€ = A)

Step 3: Equate: Î»â‚‚(vâ‚áµ€vâ‚‚) = Î»â‚(vâ‚áµ€vâ‚‚)
        (Î»â‚‚ - Î»â‚)(vâ‚áµ€vâ‚‚) = 0

Step 4: Since Î»â‚ â‰  Î»â‚‚: vâ‚áµ€vâ‚‚ = 0  (orthogonal)  âˆ

```

### ğŸ“ Key Properties

| Property | Formula | Application |
|----------|---------|-------------|
| Sum of eigenvalues | $\sum_i \lambda_i = \text{tr}(A)$ | Quick verification |
| Product | $\prod_i \lambda_i = \det(A)$ | Invertibility check |
| Powers | $A^n = Q\Lambda^n Q^{-1}$ | Fast matrix powers |
| Functions | $f(A) = Qf(\Lambda)Q^{-1}$ | Matrix exponential |
| Inverse | $A^{-1} = Q\Lambda^{-1}Q^{-1}$ | If all $\lambda_i \neq 0$ |

### ğŸ’¡ Examples

**Example 1**: Finding Eigenvalues (2Ã—2)

```
A = [4  1]
    [2  3]

Step 1: Characteristic polynomial
det(A - Î»I) = det([4-Î»   1 ])
                 ([2   3-Î»])
            = (4-Î»)(3-Î») - 2
            = Î»Â² - 7Î» + 10
            = (Î»-5)(Î»-2)

Step 2: Eigenvalues: Î»â‚ = 5, Î»â‚‚ = 2

Step 3: Eigenvectors
For Î»â‚ = 5: (A-5I)v = 0
  [-1  1][vâ‚]   [0]
  [ 2 -2][vâ‚‚] = [0]  âŸ¹ vâ‚ = [1, 1]áµ€

For Î»â‚‚ = 2: (A-2I)v = 0
  [2  1][vâ‚]   [0]
  [2  1][vâ‚‚] = [0]  âŸ¹ vâ‚‚ = [1, -2]áµ€

```

**Example 2**: Symmetric Matrix (Orthogonal Eigenvectors)

```
A = [2  1]
    [1  2]

Eigenvalues: Î»â‚ = 3, Î»â‚‚ = 1

Eigenvectors (normalized):
  vâ‚ = [1/âˆš2, 1/âˆš2]áµ€
  vâ‚‚ = [1/âˆš2, -1/âˆš2]áµ€

Check orthogonality: vâ‚Â·vâ‚‚ = 1/2 - 1/2 = 0 âœ“

```

**Example 3**: Matrix Power via Eigendecomposition

```
Compute AÂ¹â°â° where A = [2  1]
                       [1  2]

Using A = QÎ›Qáµ€:
  AÂ¹â°â° = QÎ›Â¹â°â°Qáµ€
  
  Î›Â¹â°â° = [3Â¹â°â°    0  ]
         [  0    1Â¹â°â°]
       = [3Â¹â°â°   0]
         [0      1]

Much faster than 100 matrix multiplications!

```

### âš ï¸ When Eigendecomposition Fails

```
Not all matrices are diagonalizable!

Defective matrix example:
A = [1  1]
    [0  1]

Eigenvalue: Î» = 1 (multiplicity 2)
But only ONE eigenvector: v = [1, 0]áµ€

For such matrices, use:
â€¢ Jordan Normal Form: A = PJPâ»Â¹
â€¢ Schur Decomposition: A = QTQ* (always exists)
â€¢ SVD (always exists for any matrix)

```

---

## 3. QR Decomposition

### ğŸ“Œ Theorem

For any matrix $A \in \mathbb{R}^{m \times n}$ with $m \geq n$:

```math
A = QR

```

where:

- $Q \in \mathbb{R}^{m \times n}$: orthonormal columns ($Q^TQ = I$)

- $R \in \mathbb{R}^{n \times n}$: upper triangular

### ğŸ” Gram-Schmidt Algorithm (Constructive Proof)

```
Input: Matrix A = [aâ‚ | aâ‚‚ | ... | aâ‚™] (columns)
Output: Q (orthonormal columns), R (upper triangular)

For j = 1, 2, ..., n:
    # Start with original column
    Å©â±¼ = aâ±¼
    
    # Subtract projections onto previous q vectors
    For i = 1 to j-1:
        ráµ¢â±¼ = qáµ¢áµ€ aâ±¼           # Projection coefficient
        Å©â±¼ = Å©â±¼ - ráµ¢â±¼ qáµ¢       # Remove component along qáµ¢
    
    # Normalize
    râ±¼â±¼ = â€–Å©â±¼â€–
    qâ±¼ = Å©â±¼ / râ±¼â±¼

Result: A = QR where Q = [qâ‚|...|qâ‚™] and R has ráµ¢â±¼ entries

```

**Verification**:

```
We need to show: aâ±¼ = Î£áµ¢â‚Œâ‚Ê² ráµ¢â±¼qáµ¢  (j-th column of QR)

By construction:
  Å©â±¼ = aâ±¼ - Î£áµ¢â‚Œâ‚Ê²â»Â¹ ráµ¢â±¼qáµ¢
  qâ±¼ = Å©â±¼/râ±¼â±¼  âŸ¹  Å©â±¼ = râ±¼â±¼qâ±¼

Therefore:
  aâ±¼ = Å©â±¼ + Î£áµ¢â‚Œâ‚Ê²â»Â¹ ráµ¢â±¼qáµ¢
     = râ±¼â±¼qâ±¼ + Î£áµ¢â‚Œâ‚Ê²â»Â¹ ráµ¢â±¼qáµ¢
     = Î£áµ¢â‚Œâ‚Ê² ráµ¢â±¼qáµ¢  âœ“

```

### ğŸ“ Modified Gram-Schmidt (Numerically Stable)

```
Classical GS: Compute all projections from original vectors
Modified GS:  Update vector after each projection

For j = 1 to n:
    For i = 1 to j-1:
        ráµ¢â±¼ = qáµ¢áµ€ qâ±¼          # Use CURRENT qâ±¼, not original aâ±¼
        qâ±¼ = qâ±¼ - ráµ¢â±¼ qáµ¢      # Update qâ±¼ immediately
    râ±¼â±¼ = â€–qâ±¼â€–
    qâ±¼ = qâ±¼ / râ±¼â±¼

Numerical advantage:
â€¢ Classical GS error: O(ÎºÂ²(A)Îµ) where Îµ = machine precision
â€¢ Modified GS error:  O(Îº(A)Îµ)

```

### ğŸ’¡ Examples

**Example 1**: 2Ã—2 QR Decomposition

```
A = [1  1]
    [1  0]

Step 1: First column
  aâ‚ = [1, 1]áµ€
  râ‚â‚ = â€–aâ‚â€– = âˆš2
  qâ‚ = aâ‚/râ‚â‚ = [1/âˆš2, 1/âˆš2]áµ€

Step 2: Second column
  aâ‚‚ = [1, 0]áµ€
  râ‚â‚‚ = qâ‚áµ€aâ‚‚ = 1/âˆš2
  Å©â‚‚ = aâ‚‚ - râ‚â‚‚qâ‚ = [1, 0]áµ€ - (1/âˆš2)[1/âˆš2, 1/âˆš2]áµ€ = [1/2, -1/2]áµ€
  râ‚‚â‚‚ = â€–Å©â‚‚â€– = 1/âˆš2
  qâ‚‚ = Å©â‚‚/râ‚‚â‚‚ = [1/âˆš2, -1/âˆš2]áµ€

Result:
Q = [1/âˆš2   1/âˆš2]    R = [âˆš2    1/âˆš2]
    [1/âˆš2  -1/âˆš2]        [0     1/âˆš2]

```

### ğŸ¤– ML Applications

**1. Least Squares (More Stable than Normal Equations)**

```
Problem: min â€–Ax - bâ€–Â²

Normal equations: x = (Aáµ€A)â»Â¹Aáµ€b  (can be ill-conditioned!)

QR approach:
  A = QR
  Ax = b  âŸ¹  QRx = b  âŸ¹  Rx = Qáµ€b
  
  Solve triangular system Rx = Qáµ€b (stable back-substitution)

```

**2. QR Algorithm for Eigenvalues**

```
Aâ‚€ = A
For k = 1, 2, ...:
    Aâ‚–â‚‹â‚ = Qâ‚–Râ‚–     (QR factorization)
    Aâ‚– = Râ‚–Qâ‚–       (Reverse multiply)

As k â†’ âˆ: Aâ‚– â†’ upper triangular (eigenvalues on diagonal)

```

---

## 4. Cholesky Decomposition

### ğŸ“Œ Theorem

For a **symmetric positive definite** matrix $A$:

```math
A = LL^T

```

where $L$ is lower triangular with positive diagonal entries.

### ğŸ” Proof of Existence

```
Proof by strong induction on matrix size n:

Base case (n=1): A = [a] where a > 0 (positive definite)
  L = [âˆša], and LÂ·Láµ€ = a = A âœ“

Inductive step: Assume true for (n-1)Ã—(n-1). For nÃ—n:

Partition A:
  A = [a    báµ€]  where a > 0 (A is PD âŸ¹ diagonal entries > 0)
      [b    C ]

Let L have the form:
  L = [â„“     0 ]
      [m    Lâ‚‚]

Then LLáµ€ = [â„“Â²           â„“máµ€        ]
           [â„“m    mmáµ€ + Lâ‚‚Lâ‚‚áµ€]

Matching with A:
  â„“ = âˆša
  m = b/â„“ = b/âˆša
  Lâ‚‚Lâ‚‚áµ€ = C - mmáµ€ = C - bbáµ€/a

Need to show: C - bbáµ€/a is positive definite
  For any x â‰  0:
    xáµ€(C - bbáµ€/a)x = xáµ€Cx - (báµ€x)Â²/a
    
  Consider y = [-báµ€x/a, xáµ€]áµ€ (nonzero since x â‰  0)
    yáµ€Ay = (báµ€x)Â²/a - 2(báµ€x)Â²/a + xáµ€Cx = xáµ€Cx - (báµ€x)Â²/a > 0
    
  So C - bbáµ€/a is PD, apply induction to get Lâ‚‚.  âˆ

```

### ğŸ“ Algorithm

```python
def cholesky(A):
    """
    Compute Cholesky decomposition A = LLáµ€
    Time: O(nÂ³/3) - about 2Ã— faster than LU
    """
    n = A.shape[0]
    L = np.zeros((n, n))
    
    for j in range(n):
        # Diagonal element
        L[j, j] = np.sqrt(A[j, j] - np.sum(L[j, :j]**2))
        
        # Below diagonal
        for i in range(j+1, n):
            L[i, j] = (A[i, j] - np.sum(L[i, :j] * L[j, :j])) / L[j, j]
    
    return L

```

### ğŸ’¡ Example

```
A = [4  2]
    [2  5]

Step 1: Lâ‚â‚ = âˆš4 = 2

Step 2: Lâ‚‚â‚ = Aâ‚‚â‚/Lâ‚â‚ = 2/2 = 1

Step 3: Lâ‚‚â‚‚ = âˆš(Aâ‚‚â‚‚ - Lâ‚‚â‚Â²) = âˆš(5 - 1) = 2

Result:
L = [2  0]
    [1  2]

Verify: LLáµ€ = [2  0][2  1] = [4  2] = A âœ“
              [1  2][0  2]   [2  5]

```

### ğŸ¤– ML Applications

**Gaussian Processes**: Computing $\mathcal{N}(\mu, K)$

```python
# Sampling from multivariate Gaussian
L = np.linalg.cholesky(K)  # K = covariance matrix
z = np.random.randn(n)      # Standard normal
sample = mu + L @ z         # Transform to N(Î¼, K)

# Computing log-likelihood efficiently
# log|K| = 2 Ã— sum of log(diagonal of L)
log_det = 2 * np.sum(np.log(np.diag(L)))

```

---

## 5. LU Decomposition

### ğŸ“Œ Theorem

For a square matrix $A$ (with partial pivoting):

```math
PA = LU

```

where:

- $P$: permutation matrix

- $L$: lower triangular with 1s on diagonal

- $U$: upper triangular

### ğŸ” Algorithm (Gaussian Elimination)

```
For k = 1 to n-1:
    # Pivot: swap rows if needed for stability
    Find i â‰¥ k with max |Aáµ¢â‚–|
    Swap rows k and i
    
    # Eliminate below diagonal
    For i = k+1 to n:
        Láµ¢â‚– = Aáµ¢â‚– / Aâ‚–â‚–           # Store multiplier
        Aáµ¢,â‚–â‚Šâ‚:â‚™ -= Láµ¢â‚– Ã— Aâ‚–,â‚–â‚Šâ‚:â‚™  # Eliminate

```

### ğŸ’¡ Application: Solving $Ax = b$

```
Given PA = LU:

1. Apply permutation: Pb = b'

2. Forward substitution: Solve Ly = b' for y

3. Back substitution: Solve Ux = y for x

Total: O(nÂ³/3) for factorization + O(nÂ²) per solve
       (Factorize once, solve many times!)

```

---

## ğŸ“Š Comparison: Which Decomposition to Use?

| Decomposition | Matrix Type | Complexity | Best For |
|---------------|-------------|------------|----------|
| **SVD** | Any $m \times n$ | $O(mn \cdot \min(m,n))$ | Low-rank approximation, pseudoinverse |
| **Eigen** | Square $n \times n$ | $O(n^3)$ | PCA, matrix powers, spectral analysis |
| **QR** | Any $m \times n$ | $O(mn^2)$ | Least squares, numerical stability |
| **Cholesky** | Symmetric PD | $O(n^3/3)$ | **Fastest** for PD systems, GPs |
| **LU** | Square $n \times n$ | $O(n^3/3)$ | General linear systems |

### Decision Flowchart

```
Is matrix square?
+- No â†’ Use SVD or QR
+- Yes â†’ Is matrix symmetric?
         +- No â†’ Use LU or SVD
         +- Yes â†’ Is matrix positive definite?
                  +- No â†’ Use Eigen or SVD
                  +- Yes â†’ Use Cholesky (fastest!)

```

---

## ğŸ’» Complete Code Examples

```python
import numpy as np
import torch
from scipy import linalg

def demonstrate_all_decompositions():
    """Complete examples of all matrix decompositions"""
    
    # Create test matrices
    np.random.seed(42)
    A_rect = np.random.randn(5, 3)           # Rectangular
    A_square = np.random.randn(4, 4)         # Square
    A_symmetric = A_square @ A_square.T + np.eye(4)  # Symmetric PD
    
    print("=" * 60)
    print("1. SINGULAR VALUE DECOMPOSITION (Any matrix)")
    print("=" * 60)
    U, S, Vt = np.linalg.svd(A_rect, full_matrices=False)
    print(f"Matrix shape: {A_rect.shape}")
    print(f"U: {U.shape}, S: {S.shape}, Vt: {Vt.shape}")
    print(f"Singular values: {S}")
    print(f"Reconstruction error: {np.linalg.norm(A_rect - U @ np.diag(S) @ Vt):.2e}")
    
    # Low-rank approximation
    k = 2
    A_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
    print(f"Rank-{k} approximation error: {np.linalg.norm(A_rect - A_k):.4f}")
    print(f"Theoretical error (Ïƒâ‚ƒ): {S[k]:.4f}")
    
    print("\n" + "=" * 60)
    print("2. EIGENDECOMPOSITION (Square symmetric)")
    print("=" * 60)
    eigenvalues, eigenvectors = np.linalg.eigh(A_symmetric)
    print(f"Eigenvalues: {eigenvalues}")
    print(f"Sum of eigenvalues: {eigenvalues.sum():.4f}")
    print(f"Trace of A: {np.trace(A_symmetric):.4f}")
    print(f"Product of eigenvalues: {np.prod(eigenvalues):.4f}")
    print(f"Determinant of A: {np.linalg.det(A_symmetric):.4f}")
    
    # Verify reconstruction
    Lambda = np.diag(eigenvalues)
    A_reconstructed = eigenvectors @ Lambda @ eigenvectors.T
    print(f"Reconstruction error: {np.linalg.norm(A_symmetric - A_reconstructed):.2e}")
    
    print("\n" + "=" * 60)
    print("3. QR DECOMPOSITION (Any matrix)")
    print("=" * 60)
    Q, R = np.linalg.qr(A_rect)
    print(f"Q shape: {Q.shape}, R shape: {R.shape}")
    print(f"Q orthonormal check (Qáµ€Q = I): {np.allclose(Q.T @ Q, np.eye(Q.shape[1]))}")
    print(f"Reconstruction error: {np.linalg.norm(A_rect - Q @ R):.2e}")
    
    # Solve least squares
    b = np.random.randn(5)
    x_qr = np.linalg.solve(R, Q.T @ b)
    x_lstsq = np.linalg.lstsq(A_rect, b, rcond=None)[0]
    print(f"QR vs lstsq difference: {np.linalg.norm(x_qr - x_lstsq):.2e}")
    
    print("\n" + "=" * 60)
    print("4. CHOLESKY DECOMPOSITION (Symmetric PD)")
    print("=" * 60)
    L = np.linalg.cholesky(A_symmetric)
    print(f"L shape: {L.shape}")
    print(f"L is lower triangular: {np.allclose(L, np.tril(L))}")
    print(f"Reconstruction error: {np.linalg.norm(A_symmetric - L @ L.T):.2e}")
    
    # Solve system using Cholesky
    b = np.random.randn(4)
    y = linalg.solve_triangular(L, b, lower=True)
    x = linalg.solve_triangular(L.T, y, lower=False)
    print(f"Solution error: {np.linalg.norm(A_symmetric @ x - b):.2e}")
    
    print("\n" + "=" * 60)
    print("5. LU DECOMPOSITION (Square)")
    print("=" * 60)
    P, L, U = linalg.lu(A_square)
    print(f"P: {P.shape}, L: {L.shape}, U: {U.shape}")
    print(f"Reconstruction error: {np.linalg.norm(A_square - P @ L @ U):.2e}")
    
    return U, S, Vt, eigenvalues, eigenvectors

# Run demo
demonstrate_all_decompositions()

```

### PyTorch GPU Implementation

```python
import torch

def svd_on_gpu(A):
    """SVD with GPU acceleration"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    A_tensor = torch.tensor(A, device=device, dtype=torch.float32)
    
    U, S, Vh = torch.linalg.svd(A_tensor)
    
    return U.cpu().numpy(), S.cpu().numpy(), Vh.cpu().numpy()

def low_rank_approximation_torch(A, k):
    """GPU-accelerated low-rank approximation"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    A_tensor = torch.tensor(A, device=device, dtype=torch.float32)
    
    U, S, Vh = torch.linalg.svd(A_tensor, full_matrices=False)
    
    # Keep top-k components
    A_k = U[:, :k] @ torch.diag(S[:k]) @ Vh[:k, :]
    
    return A_k.cpu().numpy()

```

---

## âš ï¸ Common Mistakes

### âŒ Mistake 1: Using Eigendecomposition on Non-Square Matrix

```python
# WRONG
A = np.random.randn(5, 3)
eigenvalues, eigenvectors = np.linalg.eig(A)  # ERROR!

# CORRECT: Use SVD for rectangular matrices
U, S, Vt = np.linalg.svd(A)

```

### âŒ Mistake 2: Assuming All Matrices are Diagonalizable

```python
# This matrix is defective (not diagonalizable)
A = np.array([[1, 1], [0, 1]])

# Use Schur decomposition instead
T, Q = linalg.schur(A)

```

### âŒ Mistake 3: Using Cholesky on Non-PD Matrix

```python
# WRONG: Matrix must be positive definite
A = np.array([[1, 2], [2, 1]])  # Eigenvalues: 3, -1 (not PD!)
L = np.linalg.cholesky(A)  # LinAlgError!

# Check first
eigenvalues = np.linalg.eigvalsh(A)
if np.all(eigenvalues > 0):
    L = np.linalg.cholesky(A)

```

### âŒ Mistake 4: Ignoring Numerical Stability

```python
# FRAGILE: Normal equations
x = np.linalg.inv(A.T @ A) @ A.T @ b

# STABLE: Use QR
Q, R = np.linalg.qr(A)
x = np.linalg.solve(R, Q.T @ b)

```

---

## ğŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ğŸ“– | [Matrix Computations](https://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm) | Golub & Van Loan (Bible of numerical LA) |
| ğŸ“– | [Numerical Linear Algebra](https://people.maths.ox.ac.uk/trefethen/text.html) | Trefethen & Bau |
| ğŸ¥ | [Steve Brunton SVD Series](https://www.youtube.com/watch?v=nbBvuuNVfco) | Visual explanations |
| ğŸ“„ | [LoRA Paper](https://arxiv.org/abs/2106.09685) | Low-rank adaptation |

---

## ğŸ—ºï¸ Navigation

<p align="center">

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Linear Algebra](../README.md) | [Mathematics](../../README.md) | [Dimensionality Reduction](../02_dimensionality_reduction/README.md) |

</p>

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
