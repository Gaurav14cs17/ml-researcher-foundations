<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Dimensionality%20Reduction&fontSize=38&fontColor=fff&animation=twinkling&fontAlignY=32&desc=PCA%20Â·%20t-SNE%20Â·%20UMAP%20Â·%20Autoencoders&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-01.02_Dim_Reduction-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-PCA_tSNE_UMAP-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **Dimensionality reduction compresses high-dimensional data while preserving important structure.** Essential for visualization, noise reduction, and computational efficiency.

- ğŸ“Š **PCA**: Linear, preserves variance, fast, interpretable

- ğŸŒ€ **t-SNE**: Non-linear, preserves local structure, for visualization

- ğŸ—ºï¸ **UMAP**: Non-linear, preserves global+local structure, faster than t-SNE

- ğŸ§  **Autoencoders**: Neural network-based, learns nonlinear manifolds

---

## ğŸ“‘ Table of Contents

1. [Visual Overview](#-visual-overview)

2. [PCA: Complete Theory](#1-pca-principal-component-analysis)

3. [t-SNE](#2-t-sne)

4. [UMAP](#3-umap)

5. [Comparison](#4-comparison)

6. [Code Implementation](#5-code-implementation)

7. [Resources](#-resources)

---

## ğŸ¨ Visual Overview

<img src="./images/pca-tsne-comparison.svg" width="100%">

```
+-----------------------------------------------------------------------------+
|               DIMENSIONALITY REDUCTION METHODS COMPARISON                    |
+-----------------------------------------------------------------------------+
|                                                                              |
|   HIGH-DIMENSIONAL DATA (e.g., 768D embeddings)                             |
|   -----------------------------------------------                           |
|                      |                                                       |
|        +-------------+-------------+-------------+                          |
|        â–¼             â–¼             â–¼             â–¼                          |
|   +--------+   +--------+   +--------+   +------------+                    |
|   |  PCA   |   | t-SNE  |   |  UMAP  |   |Autoencoder |                    |
|   +--------+   +--------+   +--------+   +------------+                    |
|        |             |             |             |                          |
|   Linear         Non-linear   Non-linear   Non-linear                       |
|   Global         Local        Local+Global  Learned                         |
|   Fast           Slow         Medium        Slow (training)                 |
|   Variance       Similarity   Topology      Reconstruction                  |
|                                                                              |
|   USE CASES:                                                                |
|   â€¢ Preprocessing    â€¢ Visualization  â€¢ Visualization  â€¢ Feature learning  |
|   â€¢ Noise reduction  â€¢ Clustering viz â€¢ Clustering     â€¢ Generative models |
|   â€¢ Feature extract  â€¢ 2D/3D plots    â€¢ Large datasets â€¢ Compression       |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. PCA: Principal Component Analysis

### ğŸ“Œ Goal

Find orthogonal directions (principal components) that maximize variance in the data.

### ğŸ“ Mathematical Formulation

Given centered data $X \in \mathbb{R}^{n \times d}$ (n samples, d features):

$$\text{Covariance matrix: } \Sigma = \frac{1}{n-1}X^TX$$

**Goal**: Find projection $W \in \mathbb{R}^{d \times k}$ that maximizes:

$$\text{Var}(XW) = W^T \Sigma W$$

subject to $W^TW = I$ (orthonormal columns)

### ğŸ” Complete Derivation

```
Step 1: First Principal Component
        Find wâ‚ that maximizes variance of Xwâ‚:
        max_{wâ‚} wâ‚áµ€Î£wâ‚  subject to â€–wâ‚â€– = 1

Step 2: Lagrangian
        L = wâ‚áµ€Î£wâ‚ - Î»(wâ‚áµ€wâ‚ - 1)

Step 3: Take derivative and set to zero
        âˆ‚L/âˆ‚wâ‚ = 2Î£wâ‚ - 2Î»wâ‚ = 0
        Î£wâ‚ = Î»wâ‚

Step 4: This is an eigenvalue equation!
        wâ‚ must be an eigenvector of Î£

Step 5: Which eigenvector maximizes variance?
        Variance = wâ‚áµ€Î£wâ‚ = wâ‚áµ€(Î»wâ‚) = Î»wâ‚áµ€wâ‚ = Î»
        
        Maximum variance when Î» is LARGEST eigenvalue!
        wâ‚ = eigenvector of largest eigenvalue

Step 6: Second Principal Component
        Maximize wâ‚‚áµ€Î£wâ‚‚ subject to â€–wâ‚‚â€– = 1 AND wâ‚‚áµ€wâ‚ = 0
        
        Solution: wâ‚‚ = eigenvector of second largest eigenvalue
        (Orthogonality follows from Spectral Theorem)

Step 7: General Solution
        Principal components = eigenvectors of Î£ sorted by eigenvalue
        PCâ‚, PCâ‚‚, ..., PCâ‚– = top k eigenvectors

```

### ğŸ“ PCA via SVD (Numerically Stable)

```
Instead of computing Î£ = Xáµ€X and then eigendecomposition,
use SVD of X directly:

X = UÎ£Váµ€

Then:
  Xáµ€X = VÎ£Â²Váµ€  (eigendecomposition of covariance!)

  Principal components = columns of V
  Singular valuesÂ² = eigenvalues of Xáµ€X

Projection:
  X_reduced = XÂ·V[:,:k] = U[:,:k]Â·Î£[:k,:k]

```

### ğŸ’¡ Examples

**Example 1**: 2D to 1D PCA

```
Data points: (1,2), (2,4), (3,6), (4,8)

Step 1: Center the data
  mean = (2.5, 5)
  centered = (-1.5,-3), (-0.5,-1), (0.5,1), (1.5,3)

Step 2: Covariance matrix
  Î£ = [1.67  3.33]
      [3.33  6.67]

Step 3: Eigenvalues and eigenvectors
  Î»â‚ â‰ˆ 8.33, vâ‚ â‰ˆ [0.45, 0.89]
  Î»â‚‚ â‰ˆ 0,    vâ‚‚ â‰ˆ [-0.89, 0.45]

Step 4: First PC explains 8.33/(8.33+0) = 100% of variance

Step 5: Project onto first PC
  The data lies exactly on the line y = 2x!

```

**Example 2**: Explained Variance Ratio

```
Given eigenvalues: [4.0, 2.0, 1.0, 0.5, 0.3, 0.2]

Total variance: 8.0

Explained variance ratios:
  PC1: 4.0/8.0 = 50%
  PC2: 2.0/8.0 = 25%
  PC3: 1.0/8.0 = 12.5%
  ...

Cumulative:
  PC1: 50%
  PC1+PC2: 75%
  PC1+PC2+PC3: 87.5%

To capture 95% variance, need first 5 components.

```

### ğŸ’» Code Implementation

```python
import numpy as np
from sklearn.decomposition import PCA

def pca_from_scratch(X, n_components):
    """
    PCA implementation from scratch.
    
    Steps:
    1. Center the data
    2. Compute covariance matrix
    3. Eigendecomposition
    4. Project onto top eigenvectors
    """
    # Center
    X_centered = X - X.mean(axis=0)
    
    # Covariance matrix
    n = X.shape[0]
    cov = X_centered.T @ X_centered / (n - 1)
    
    # Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eigh(cov)
    
    # Sort by eigenvalue (descending)
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # Select top k components
    components = eigenvectors[:, :n_components]
    
    # Project
    X_pca = X_centered @ components
    
    # Explained variance ratio
    explained_var = eigenvalues[:n_components] / eigenvalues.sum()
    
    return X_pca, components, explained_var

def pca_via_svd(X, n_components):
    """
    PCA via SVD (numerically more stable).
    """
    X_centered = X - X.mean(axis=0)
    
    # SVD
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)
    
    # Components = rows of Vt (or columns of V)
    components = Vt[:n_components].T
    
    # Projected data
    X_pca = U[:, :n_components] * S[:n_components]
    
    # Explained variance
    explained_var = (S[:n_components]**2) / (S**2).sum()
    
    return X_pca, components, explained_var

# Example usage
X = np.random.randn(1000, 100)
X_pca, components, explained_var = pca_from_scratch(X, n_components=10)
print(f"Shape: {X.shape} â†’ {X_pca.shape}")
print(f"Explained variance: {explained_var.sum():.2%}")

```

---

## 2. t-SNE

### ğŸ“Œ Goal

Preserve **local structure**: similar points in high-D should be similar in low-D.

### ğŸ“ Algorithm

```
Step 1: Compute pairwise similarities in high-D
        pâ±¼|áµ¢ = exp(-â€–xáµ¢-xâ±¼â€–Â²/2Ïƒáµ¢Â²) / Î£â‚–â‰ áµ¢ exp(-â€–xáµ¢-xâ‚–â€–Â²/2Ïƒáµ¢Â²)
        páµ¢â±¼ = (pâ±¼|áµ¢ + páµ¢|â±¼) / 2n  (symmetrized)

Step 2: Initialize low-D embedding Y randomly

Step 3: Compute similarities in low-D (Student-t with 1 df)
        qáµ¢â±¼ = (1 + â€–yáµ¢-yâ±¼â€–Â²)â»Â¹ / Î£â‚–â‰ â‚—(1 + â€–yâ‚–-yâ‚—â€–Â²)â»Â¹

Step 4: Minimize KL divergence
        KL(Pâ€–Q) = Î£áµ¢â±¼ páµ¢â±¼ log(páµ¢â±¼/qáµ¢â±¼)

Step 5: Gradient descent on Y
        âˆ‚C/âˆ‚yáµ¢ = 4Î£â±¼(páµ¢â±¼ - qáµ¢â±¼)(yáµ¢ - yâ±¼)(1 + â€–yáµ¢-yâ±¼â€–Â²)â»Â¹

```

### âš ï¸ Key Hyperparameter: Perplexity

```
Perplexity â‰ˆ effective number of neighbors

  Perplexity = 2^H(Páµ¢)  where H is entropy

  Typical values: 5-50
  Low perplexity â†’ tight clusters, may miss global structure
  High perplexity â†’ may merge distinct clusters

```

### ğŸ’» Code

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def tsne_visualization(X, labels=None, perplexity=30):
    """
    t-SNE for visualization.
    """
    tsne = TSNE(
        n_components=2,
        perplexity=perplexity,
        n_iter=1000,
        random_state=42
    )
    X_tsne = tsne.fit_transform(X)
    
    plt.figure(figsize=(10, 8))
    if labels is not None:
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7)
        plt.colorbar(scatter)
    else:
        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)
    plt.title(f't-SNE (perplexity={perplexity})')
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    
    return X_tsne

```

---

## 3. UMAP

### ğŸ“Œ Goal

Preserve **both local and global structure** using topological methods.

### ğŸ“ Key Idea

```
UMAP models data as a fuzzy topological structure:

1. Build fuzzy simplicial complex from high-D data
   (weighted graph where edge weights = similarity)

2. Find low-D representation with similar topology

3. Minimize cross-entropy between high-D and low-D graphs

```

### Advantages over t-SNE

| Aspect | t-SNE | UMAP |
|--------|-------|------|
| Speed | O(nÂ²) â†’ O(n log n) with approximations | O(n^1.14) |
| Global structure | Poor | Better preserved |
| Scalability | Struggles > 10K points | Handles millions |
| Theory | Similarity preservation | Topological foundation |

### ğŸ’» Code

```python
import umap

def umap_visualization(X, labels=None, n_neighbors=15, min_dist=0.1):
    """
    UMAP for visualization and clustering.
    """
    reducer = umap.UMAP(
        n_components=2,
        n_neighbors=n_neighbors,  # Similar to perplexity
        min_dist=min_dist,        # Controls clustering tightness
        random_state=42
    )
    X_umap = reducer.fit_transform(X)
    
    return X_umap

```

---

## 4. Comparison

| Method | Type | Preserves | Speed | Use Case |
|--------|------|-----------|-------|----------|
| **PCA** | Linear | Global variance | Fast O(ndÂ²) | Preprocessing, interpretable |
| **t-SNE** | Non-linear | Local | Slow O(nÂ²) | Visualization |
| **UMAP** | Non-linear | Local + Global | Medium O(n^1.14) | Visualization, clustering |
| **Autoencoder** | Non-linear | Learned | Slow | Feature learning |

### When to Use What

```
Use PCA when:
  âœ“ Need interpretability
  âœ“ Linear relationships sufficient
  âœ“ Preprocessing for other algorithms
  âœ“ Very high-D data (d >> 1000)

Use t-SNE when:
  âœ“ 2D/3D visualization only
  âœ“ Small-medium datasets (<10K)
  âœ“ Only care about local clusters

Use UMAP when:
  âœ“ Visualization + downstream tasks
  âœ“ Large datasets
  âœ“ Want global structure preserved
  âœ“ Need reproducibility

```

---

## 5. Code Implementation

### Complete Pipeline

```python
import numpy as np
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt

class DimensionalityReduction:
    """Complete dimensionality reduction toolkit."""
    
    def __init__(self, X):
        self.X = X
        self.X_centered = X - X.mean(axis=0)
    
    def pca(self, n_components=50):
        """PCA reduction"""
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(self.X)
        
        return {
            'embedding': X_pca,
            'explained_variance': pca.explained_variance_ratio_,
            'components': pca.components_
        }
    
    def tsne(self, n_components=2, perplexity=30, use_pca=True):
        """t-SNE reduction (with optional PCA preprocessing)"""
        X = self.X
        if use_pca and X.shape[1] > 50:
            X = PCA(n_components=50).fit_transform(X)
        
        tsne = TSNE(n_components=n_components, perplexity=perplexity)
        X_tsne = tsne.fit_transform(X)
        
        return {'embedding': X_tsne, 'kl_divergence': tsne.kl_divergence_}
    
    def umap_reduce(self, n_components=2, n_neighbors=15, min_dist=0.1):
        """UMAP reduction"""
        reducer = umap.UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist
        )
        X_umap = reducer.fit_transform(self.X)
        
        return {'embedding': X_umap, 'reducer': reducer}
    
    def plot_comparison(self, labels=None):
        """Compare all methods"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        methods = [
            ('PCA', self.pca(n_components=2)['embedding']),
            ('t-SNE', self.tsne()['embedding']),
            ('UMAP', self.umap_reduce()['embedding'])
        ]
        
        for ax, (name, embedding) in zip(axes, methods):
            if labels is not None:
                ax.scatter(embedding[:, 0], embedding[:, 1], 
                          c=labels, cmap='tab10', alpha=0.7)
            else:
                ax.scatter(embedding[:, 0], embedding[:, 1], alpha=0.7)
            ax.set_title(name)
            ax.set_xlabel('Dim 1')
            ax.set_ylabel('Dim 2')
        
        plt.tight_layout()
        return fig

# Usage
X = np.random.randn(1000, 100)
labels = np.random.randint(0, 5, 1000)

reducer = DimensionalityReduction(X)
fig = reducer.plot_comparison(labels)

```

---

## ğŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ğŸ“„ | [t-SNE Paper](https://www.jmlr.org/papers/v9/vandermaaten08a.html) | Original t-SNE |
| ğŸ“„ | [UMAP Paper](https://arxiv.org/abs/1802.03426) | Original UMAP |
| ğŸ¥ | [StatQuest: PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ) | Visual explanation |

---

## ğŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Decompositions](../01_decompositions/README.md) | [Linear Algebra](../README.md) | [Eigen](../03_eigen/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
