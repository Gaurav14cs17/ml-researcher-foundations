<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=00C853&height=120&section=header&text=Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-02-00C853?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“ Mathematical Foundations

### Optimality Conditions
```
First-order necessary condition:
âˆ‡f(x*) = 0

Second-order sufficient condition:
âˆ‡Â²f(x*) â‰» 0 (positive definite)
```

### Convexity
```
Function f is convex iff:
f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)  âˆ€Î» âˆˆ [0,1]

Equivalently:
âˆ‡Â²f(x) âª° 0 (positive semidefinite)

Strong convexity (Î¼ > 0):
f(y) â‰¥ f(x) + âˆ‡f(x)áµ€(y-x) + (Î¼/2)||y-x||Â²
```

### Lipschitz Smoothness
```
f has L-Lipschitz gradient if:
||âˆ‡f(x) - âˆ‡f(y)|| â‰¤ L||x - y||

Equivalently:
f(y) â‰¤ f(x) + âˆ‡f(x)áµ€(y-x) + (L/2)||y-x||Â²
```

### Convergence Rates
```
GD with step size Î± = 1/L:
f(xâ‚–) - f(x*) â‰¤ L||xâ‚€ - x*||Â² / (2k)   [O(1/k)]

Strongly convex (Îº = L/Î¼):
||xâ‚– - x*|| â‰¤ (1 - 1/Îº)^k ||xâ‚€ - x*||   [Linear]

Nesterov acceleration:
f(xâ‚–) - f(x*) â‰¤ O(L||xâ‚€ - x*||Â² / kÂ²)  [O(1/kÂ²)]
```

---

## ğŸ“‚ Topics in This Folder

| Folder | Topics | ML Application |
|--------|--------|----------------|
| [basics/](./basics/) | Objectives, constraints, optima | Problem formulation |
| [convex/](./convex/) | Convex sets, functions, strong convexity | ğŸ”¥ Theory foundation |
| [duality/](./duality/) | Lagrangian, KKT conditions | SVM, constrained problems |
| [first-order/](./first-order/) | Gradient descent, momentum | ğŸ”¥ Training NNs |
| [second-order/](./second-order/) | Newton, quasi-Newton | Natural gradient |
| [stochastic/](./stochastic/) | SGD, Adam, adaptive | ğŸ”¥ Deep learning |

---

## ğŸ¯ The Optimization Problem

```
minimize    f(x)           â† Objective function
subject to  gáµ¢(x) â‰¤ 0     â† Inequality constraints
            hâ±¼(x) = 0     â† Equality constraints
            x âˆˆ X          â† Domain

In ML:
â€¢ f(x) = Loss function (cross-entropy, MSE, ...)
â€¢ x = Model parameters (millions/billions of them!)
â€¢ Constraints often implicit (weight decay = soft constraint)
```

---

## ğŸŒ ML Optimization Landscape

```
                    Optimization Methods
                           |
          +----------------+----------------+
          |                |                |
    First-order      Second-order      Stochastic
    (gradient)       (Hessian)         (mini-batch)
          |                |                |
    +-----+-----+    +----+----+    +------+------+
    |           |    |         |    |             |
   GD        Momentum Newton  BFGS  SGD         Adam
    |           |              |     |             |
    |     Nesterov        L-BFGS   Variance     AdamW
    |                              reduction
    |                                            |
    +--------------------+-----------------------+
                         |
                         v
              Deep Learning Training
```

---

## ğŸ”‘ Key Convergence Results

| Method | Rate | Assumption | Paper |
|--------|------|------------|-------|
| GD (convex) | O(1/k) | Convex, L-smooth | - |
| GD (strongly convex) | O((1-Î¼/L)^k) | Î¼-strongly convex | - |
| Nesterov | O(1/kÂ²) | Convex | [Nesterov 1983] |
| Newton | O(log log(1/Îµ)) | Strongly convex | - |
| SGD (convex) | O(1/âˆšk) | Convex | - |

```
Convergence rate comparison:

Steps k | O(1/kÂ²) | O((1-1/Îº)^k) | O(1/k) | O(1/âˆšk)
--------+---------+--------------+--------+---------
     10 |  0.010  |     0.35     |  0.10  |   0.32
    100 |  0.0001 |     0.00     |  0.01  |   0.10
   1000 | 1e-06   |     0.00     |  0.001 |   0.03

(Lower is better)
```

---

## ğŸ’» Core Algorithms

```python

# Gradient Descent
def gradient_descent(f, grad_f, x0, lr=0.01, n_steps=100):
    x = x0
    for _ in range(n_steps):
        x = x - lr * grad_f(x)
    return x

# SGD with Momentum
def sgd_momentum(params, grads, velocities, lr=0.01, momentum=0.9):
    for p, g, v in zip(params, grads, velocities):
        v[:] = momentum * v + g
        p[:] = p - lr * v
    return params, velocities

# Adam (simplified)
def adam_step(param, grad, m, v, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
    m = beta1 * m + (1 - beta1) * grad
    v = beta2 * v + (1 - beta2) * grad**2
    m_hat = m / (1 - beta1**t)
    v_hat = v / (1 - beta2**t)
    param = param - lr * m_hat / (np.sqrt(v_hat) + eps)
    return param, m, v
```

---

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Convex Optimization | [Boyd (free)](https://web.stanford.edu/~boyd/cvxbook/) |
| ğŸ“– | Numerical Optimization | Nocedal & Wright |
| ğŸ“ | Stanford Convex Optimization | [YouTube](https://www.youtube.com/playlist?list=PL3940DD956CDF0622) |

## ğŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept for ML systems |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

---

â¬…ï¸ [Back: 02-Calculus](../02_calculus/)

---

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=00C853&height=80&section=footer" width="100%"/>
</p>
