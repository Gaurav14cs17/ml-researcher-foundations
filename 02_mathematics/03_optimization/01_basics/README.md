<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=00C853&height=120&section=header&text=Optimization%20Basics&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-02-00C853?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/optimization-basics.svg" width="100%">

*Caption: Optimization finds Î¸* = argmin f(Î¸). Minima: local vs global. Gradients point uphill; follow -âˆ‡f to descend. Critical points have âˆ‡f=0. Saddle points are critical but not minima.*

---

## ğŸ“‚ Overview

Optimization is the engine of ML - every trained model is the result of optimization. Understanding the landscape of loss functions helps design better training procedures.

---

## ğŸ“ Mathematical Definitions

### Optimization Problem
```
minimize f(Î¸)  subject to g(Î¸) â‰¤ 0, h(Î¸) = 0

Where:
â€¢ f(Î¸): objective function (loss)
â€¢ g(Î¸): inequality constraints
â€¢ h(Î¸): equality constraints
â€¢ Î¸*: optimal solution
```

### Critical Points
```
âˆ‡f(Î¸*) = 0  (necessary for local optimum)

Types:
â€¢ Local minimum: f(Î¸*) â‰¤ f(Î¸) in neighborhood
â€¢ Global minimum: f(Î¸*) â‰¤ f(Î¸) everywhere
â€¢ Saddle point: âˆ‡f = 0, not min/max
```

### Second-Order Conditions
```
At critical point Î¸* where âˆ‡f(Î¸*) = 0:

â€¢ Hessian H = âˆ‡Â²f(Î¸*)
â€¢ H â‰» 0 (positive definite) âŸ¹ local minimum
â€¢ H â‰º 0 (negative definite) âŸ¹ local maximum
â€¢ H indefinite âŸ¹ saddle point
```

### Gradient Descent
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î± âˆ‡f(Î¸â‚œ)

â€¢ Î±: learning rate (step size)
â€¢ Converges to local minimum (for smooth, convex f)
â€¢ Rate: O(1/t) for convex, O(e^(-t)) for strongly convex
```

---

## ğŸ’» Code Examples

```python
import torch
import numpy as np

# Simple gradient descent
def gradient_descent(f, grad_f, x0, lr=0.01, n_iters=100):
    x = x0.copy()
    for _ in range(n_iters):
        x = x - lr * grad_f(x)
    return x

# Example: minimize f(x) = xÂ² + 2yÂ²
f = lambda x: x[0]**2 + 2*x[1]**2
grad_f = lambda x: np.array([2*x[0], 4*x[1]])
x_opt = gradient_descent(f, grad_f, np.array([5.0, 3.0]))
# x_opt â‰ˆ [0, 0]

# PyTorch autograd
x = torch.tensor([5.0, 3.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.1)
for _ in range(100):
    loss = x[0]**2 + 2*x[1]**2
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Boyd: Convex Optimization | [Book](https://web.stanford.edu/~boyd/cvxbook/) |
| ğŸ“– | Nocedal: Numerical Opt | [Book](https://www.springer.com/gp/book/9780387303031) |
| ğŸ“ | Stanford EE364A | [Course](https://web.stanford.edu/class/ee364a/) |
| ğŸ‡¨ğŸ‡³ | ä¼˜åŒ–åŸºç¡€ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25383715) |
| ğŸ‡¨ğŸ‡³ | æ¢¯åº¦ä¸‹é™è¯¦è§£ | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88318597) |
| ğŸ‡¨ğŸ‡³ | å‡¸ä¼˜åŒ–è¯¾ç¨‹ | [Bç«™](https://www.bilibili.com/video/BV1Vt411G7nq) |

---

## ğŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Optimization Overview](../README.md) | [Mathematics](../../README.md) | [Constrained](../02_constrained/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=00C853&height=80&section=footer" width="100%"/>
</p>
