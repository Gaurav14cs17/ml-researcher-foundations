<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Constrained%20Optimization&fontSize=36&fontColor=fff&animation=twinkling&fontAlignY=32&desc=Lagrange%20Multipliers%20¬∑%20KKT%20Conditions%20¬∑%20Duality&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/üìö_Section-03.02_Constrained-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/üìä_Topics-Lagrange_KKT_SVM-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/‚úçÔ∏è_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/üìÖ_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ‚ö° TL;DR

> **Constrained optimization is essential for SVMs, max-entropy models, and safe RL.** The KKT conditions are necessary (and often sufficient) for optimality.

- üìê **Lagrange Multipliers**: Handle equality constraints $h(x) = 0$

- üîí **KKT Conditions**: Handle inequality constraints $g(x) \leq 0$

- ‚öñÔ∏è **Duality**: Convert hard primal problems to easier dual problems

- ü§ñ **Applications**: SVM, constrained RL, portfolio optimization

---

## üìë Table of Contents

1. [Problem Formulation](#1-problem-formulation)
2. [Lagrange Multipliers](#2-lagrange-multipliers)
3. [KKT Conditions](#3-kkt-conditions-complete-theory)
4. [Duality](#4-duality)
5. [SVM Derivation](#5-svm-derivation-via-lagrangian)
6. [Code Implementation](#6-code-implementation)
7. [Resources](#-resources)

---

## üé® Visual Overview

<img src="./images/lagrange-multipliers-visual.svg" width="100%">

```
+-----------------------------------------------------------------------------+

|                    CONSTRAINED OPTIMIZATION                                  |
+-----------------------------------------------------------------------------+
|                                                                              |
|   EQUALITY CONSTRAINED:            INEQUALITY CONSTRAINED:                  |
|   ---------------------            ------------------------                 |
|                                                                              |
|   min f(x)                         min f(x)                                 |
|   s.t. h(x) = 0                    s.t. g(x) ‚â§ 0                            |
|                                                                              |
|   ‚àáf parallel to ‚àáh               At optimum, either:                      |
|   at optimum                       ‚Ä¢ g(x*) < 0 (inactive, Œº=0)             |
|                                    ‚Ä¢ g(x*) = 0 (active, Œº‚â•0)               |
|                                                                              |
|        ‚Üó ‚àáf                        +-------------+                          |
|       /                            |  Feasible   |                          |
|      *---‚Üí ‚àáh                      |  Region     | ‚Üê constraint             |
|      |  optimum on                 |      *      |   g(x) ‚â§ 0              |
|      |  constraint                 |   optimum   |                          |
|      |                             +-------------+                          |
|                                                                              |
|   LAGRANGIAN: L(x,Œª,Œº) = f(x) + Œª·µÄh(x) + Œº·µÄg(x)                            |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. Problem Formulation

### üìå General Constrained Problem

```math
\begin{align}
\min_{\mathbf{x}} \quad & f(\mathbf{x}) \\
\text{s.t.} \quad & g_i(\mathbf{x}) \leq 0, \quad i = 1, \ldots, m \quad \text{(inequality)} \\
& h_j(\mathbf{x}) = 0, \quad j = 1, \ldots, p \quad \text{(equality)}
\end{align}

```

### üìê The Lagrangian

```math
\mathcal{L}(\mathbf{x}, \boldsymbol{\mu}, \boldsymbol{\lambda}) = f(\mathbf{x}) + \sum_{i=1}^{m} \mu_i g_i(\mathbf{x}) + \sum_{j=1}^{p} \lambda_j h_j(\mathbf{x})

```

where:

- $\mu\_i \geq 0$: multipliers for inequality constraints

- $\lambda\_j$: multipliers for equality constraints (can be any sign)

---

## 2. Lagrange Multipliers

### üìå Problem (Equality Only)

```math
\min f(\mathbf{x}) \quad \text{s.t.} \quad h(\mathbf{x}) = 0

```

### üìê Theorem

At a local minimum $\mathbf{x}^*$ with $\nabla h(\mathbf{x}^*) \neq 0$, there exists $\lambda^*$ such that:

```math
\nabla f(\mathbf{x}^*) + \lambda^* \nabla h(\mathbf{x}^*) = 0

```

### üîç Proof (Geometric Argument)

```
Step 1: At optimum x*, we're on the constraint surface h(x) = 0

Step 2: ‚àáh is perpendicular to the constraint surface
        (It points in the direction of steepest increase of h)

Step 3: If ‚àáf were not parallel to ‚àáh, we could move along the constraint
        in a direction that decreases f (contradiction!)

        More precisely: Let v be tangent to h(x) = 0, so ‚àáh¬∑v = 0
        
        If ‚àáf¬∑v ‚â† 0, we can move along v to decrease f while staying on constraint

Step 4: Therefore ‚àáf must be parallel to ‚àáh at optimum:
        ‚àáf = -Œª‚àáh for some scalar Œª
        
        Equivalently: ‚àáf + Œª‚àáh = 0  ‚àé

```

### üí° Examples

**Example 1**: Minimize Distance to Plane

```
Minimize f(x,y,z) = x¬≤ + y¬≤ + z¬≤
Subject to: x + y + z = 3

Lagrangian: L = x¬≤ + y¬≤ + z¬≤ + Œª(x + y + z - 3)

Conditions:
‚àÇL/‚àÇx = 2x + Œª = 0  ‚Üí  x = -Œª/2
‚àÇL/‚àÇy = 2y + Œª = 0  ‚Üí  y = -Œª/2
‚àÇL/‚àÇz = 2z + Œª = 0  ‚Üí  z = -Œª/2
‚àÇL/‚àÇŒª = x + y + z - 3 = 0

Substitute: -3Œª/2 = 3  ‚Üí  Œª = -2

Solution: x = y = z = 1, minimum distance = ‚àö3

```

**Example 2**: Max Entropy Distribution

```
Maximize H(p) = -Œ£·µ¢ p·µ¢ log(p·µ¢)
Subject to: Œ£·µ¢ p·µ¢ = 1

Lagrangian: L = -Œ£·µ¢ p·µ¢ log(p·µ¢) + Œª(Œ£·µ¢ p·µ¢ - 1)

‚àÇL/‚àÇp·µ¢ = -log(p·µ¢) - 1 + Œª = 0
log(p·µ¢) = Œª - 1
p·µ¢ = e^(Œª-1) = constant!

With constraint: n¬∑e^(Œª-1) = 1  ‚Üí  p·µ¢ = 1/n

The uniform distribution maximizes entropy!

```

---

## 3. KKT Conditions (Complete Theory)

### üìå The Four Conditions

For the constrained problem with both equality and inequality constraints, the **Karush-Kuhn-Tucker conditions** are:

**1. Stationarity**:

```math
\nabla f(\mathbf{x}^*) + \sum_i \mu_i^* \nabla g_i(\mathbf{x}^*) + \sum_j \lambda_j^* \nabla h_j(\mathbf{x}^*) = 0

```

**2. Primal Feasibility**:

```math
g_i(\mathbf{x}^*) \leq 0, \quad h_j(\mathbf{x}^*) = 0

```

**3. Dual Feasibility**:

```math
\mu_i^* \geq 0

```

**4. Complementary Slackness**:

```math
\mu_i^* g_i(\mathbf{x}^*) = 0 \quad \forall i

```

### üîç Understanding Complementary Slackness

```
Œº·µ¢ ¬∑ g·µ¢(x*) = 0 means ONE of two cases:

Case A: g·µ¢(x*) < 0 (constraint is SLACK, not binding)
        Then Œº·µ¢ = 0 (constraint has no influence)

Case B: g·µ¢(x*) = 0 (constraint is ACTIVE, binding)
        Then Œº·µ¢ ‚â• 0 (constraint is pushing on solution)

Intuition: 
  "You only pay for constraints that are actually restricting you"

```

### üìê When are KKT Sufficient?

KKT conditions are **necessary** for local optimality (under constraint qualification).

They are **sufficient** for global optimality when:

- $f$ is convex

- $g\_i$ are convex

- $h\_j$ are affine

### üí° Example: KKT Step by Step

```
Minimize f(x) = (x - 2)¬≤
Subject to: x ‚â• 1  (equivalently: g(x) = 1 - x ‚â§ 0)

Lagrangian: L = (x - 2)¬≤ + Œº(1 - x)

KKT conditions:
1. Stationarity: ‚àÇL/‚àÇx = 2(x-2) - Œº = 0
2. Primal feasibility: 1 - x ‚â§ 0  ‚Üí  x ‚â• 1
3. Dual feasibility: Œº ‚â• 0
4. Complementary slackness: Œº(1 - x) = 0

Case A: Œº = 0 (inactive constraint)
  From (1): 2(x-2) = 0  ‚Üí  x = 2
  Check (2): 2 ‚â• 1 ‚úì
  Solution: x* = 2, f* = 0

Case B: 1 - x = 0 (active constraint)
  x = 1
  From (1): Œº = 2(1-2) = -2 < 0  ‚úó violates (3)
  No solution in this case

Final answer: x* = 2, unconstrained optimum is feasible!

```

---

## 4. Duality

### üìê Lagrangian Dual Problem

The **dual function**:

```math
g(\boldsymbol{\mu}, \boldsymbol{\lambda}) = \inf_{\mathbf{x}} \mathcal{L}(\mathbf{x}, \boldsymbol{\mu}, \boldsymbol{\lambda})

```

The **dual problem**:

```math
\max_{\boldsymbol{\mu} \geq 0, \boldsymbol{\lambda}} g(\boldsymbol{\mu}, \boldsymbol{\lambda})

```

### üìê Weak Duality

For any feasible primal $\mathbf{x}$ and dual $(\boldsymbol{\mu}, \boldsymbol{\lambda})$:

```math
g(\boldsymbol{\mu}, \boldsymbol{\lambda}) \leq f(\mathbf{x})

```

**Proof**:

```
g(Œº, Œª) = inf_x L(x, Œº, Œª)
        ‚â§ L(x*, Œº, Œª)  for any feasible x*
        = f(x*) + Œ£·µ¢ Œº·µ¢g·µ¢(x*) + Œ£‚±º Œª‚±ºh‚±º(x*)
        ‚â§ f(x*)  (since Œº·µ¢ ‚â• 0, g·µ¢(x*) ‚â§ 0, h‚±º(x*) = 0)  ‚àé

```

### üìê Strong Duality

Under **Slater's condition** (exists strictly feasible point), the duality gap is zero:

```math
p^* = d^* \quad \text{(optimal values are equal)}

```

---

## 5. SVM Derivation via Lagrangian

### üìê Hard-Margin SVM

```
Primal Problem:
  min_{w,b} ¬Ω‚Äñw‚Äñ¬≤
  s.t. y·µ¢(w¬∑x·µ¢ + b) ‚â• 1  for all i

Lagrangian:
  L(w, b, Œ±) = ¬Ω‚Äñw‚Äñ¬≤ - Œ£·µ¢ Œ±·µ¢[y·µ¢(w¬∑x·µ¢ + b) - 1]

KKT conditions:
1. ‚àÇL/‚àÇw = w - Œ£·µ¢ Œ±·µ¢y·µ¢x·µ¢ = 0  ‚Üí  w = Œ£·µ¢ Œ±·µ¢y·µ¢x·µ¢
2. ‚àÇL/‚àÇb = -Œ£·µ¢ Œ±·µ¢y·µ¢ = 0
3. Œ±·µ¢ ‚â• 0
4. Œ±·µ¢[y·µ¢(w¬∑x·µ¢ + b) - 1] = 0

Dual Problem (substitute w):
  max_Œ± Œ£·µ¢ Œ±·µ¢ - ¬Ω Œ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±º(x·µ¢¬∑x‚±º)
  s.t. Œ±·µ¢ ‚â• 0, Œ£·µ¢ Œ±·µ¢y·µ¢ = 0

Key insight from complementary slackness:
  Œ±·µ¢ > 0 only when y·µ¢(w¬∑x·µ¢ + b) = 1 (support vectors!)

```

---

## 6. Code Implementation

```python
import numpy as np
from scipy.optimize import minimize

def constrained_optimization_example():
    """
    Solve: min f(x,y) = (x-1)¬≤ + (y-2)¬≤
           s.t. x + y ‚â§ 2
                x ‚â• 0
                y ‚â• 0
    """
    
    def objective(X):
        x, y = X
        return (x - 1)**2 + (y - 2)**2
    
    def gradient(X):
        x, y = X
        return np.array([2*(x-1), 2*(y-2)])
    
    # Inequality constraints: g(x) <= 0 form
    constraints = [
        {'type': 'ineq', 'fun': lambda X: 2 - X[0] - X[1]},  # x + y <= 2
        {'type': 'ineq', 'fun': lambda X: X[0]},              # x >= 0
        {'type': 'ineq', 'fun': lambda X: X[1]},              # y >= 0
    ]
    
    result = minimize(
        objective,
        x0=[0.5, 0.5],
        jac=gradient,
        constraints=constraints,
        method='SLSQP'
    )
    
    print(f"Optimal solution: x={result.x[0]:.4f}, y={result.x[1]:.4f}")
    print(f"Optimal value: {result.fun:.4f}")
    print(f"Active constraints: x+y={sum(result.x):.4f}")
    
    return result

def lagrangian_method_manual():
    """
    Manually solve using Lagrangian for equality constraint.
    
    min x¬≤ + y¬≤
    s.t. x + y = 1
    """
    # System: ‚àáL = 0
    # 2x + Œª = 0
    # 2y + Œª = 0
    # x + y - 1 = 0
    
    # From equations: x = y = -Œª/2
    # Substitute: -Œª = 1, so Œª = -1
    # Therefore: x = y = 0.5
    
    x_opt, y_opt = 0.5, 0.5
    lambda_opt = -1
    
    # Verify KKT
    grad_f = np.array([2*x_opt, 2*y_opt])
    grad_h = np.array([1, 1])
    stationarity = grad_f + lambda_opt * grad_h
    
    print(f"Solution: ({x_opt}, {y_opt})")
    print(f"Lagrange multiplier: Œª = {lambda_opt}")
    print(f"Stationarity check (should be 0): {stationarity}")

def kkt_verification(x, mu, f, g, grad_f, grad_g):
    """
    Verify KKT conditions for a solution.
    """
    print("=" * 40)
    print("KKT Conditions Check")
    print("=" * 40)
    
    # 1. Stationarity
    stationarity = grad_f(x)
    for i, (m, gg) in enumerate(zip(mu, grad_g)):
        stationarity = stationarity + m * gg(x)
    print(f"1. Stationarity: ‚àáL = {stationarity}")
    print(f"   (Should be ‚âà 0)")
    
    # 2. Primal feasibility
    print(f"2. Primal feasibility:")
    for i, gi in enumerate(g):
        print(f"   g_{i}(x) = {gi(x):.6f} <= 0? {gi(x) <= 1e-6}")
    
    # 3. Dual feasibility
    print(f"3. Dual feasibility:")
    for i, m in enumerate(mu):
        print(f"   Œº_{i} = {m:.6f} >= 0? {m >= -1e-6}")
    
    # 4. Complementary slackness
    print(f"4. Complementary slackness:")
    for i, (m, gi) in enumerate(zip(mu, g)):
        cs = m * gi(x)
        print(f"   Œº_{i} ¬∑ g_{i}(x) = {cs:.6f} = 0? {abs(cs) < 1e-6}")

# Run examples
constrained_optimization_example()
print("\n")
lagrangian_method_manual()

```

### SVM Implementation

```python
import numpy as np
from scipy.optimize import minimize

def svm_dual(X, y):
    """
    Solve SVM dual problem:
    max Œ£Œ±·µ¢ - ¬ΩŒ£Œ£ Œ±·µ¢Œ±‚±ºy·µ¢y‚±º(x·µ¢¬∑x‚±º)
    s.t. Œ±·µ¢ ‚â• 0, Œ£Œ±·µ¢y·µ¢ = 0
    """
    n = len(y)
    
    # Gram matrix
    K = X @ X.T
    
    def objective(alpha):
        return -np.sum(alpha) + 0.5 * np.sum(alpha.reshape(-1,1) * alpha * y.reshape(-1,1) * y * K)
    
    def gradient(alpha):
        return -np.ones(n) + alpha * y * (K @ (alpha * y))
    
    # Constraints
    constraints = [
        {'type': 'eq', 'fun': lambda a: np.dot(a, y)}  # Œ£Œ±·µ¢y·µ¢ = 0
    ]
    bounds = [(0, None) for _ in range(n)]  # Œ±·µ¢ ‚â• 0
    
    result = minimize(
        objective,
        x0=np.zeros(n),
        jac=gradient,
        method='SLSQP',
        bounds=bounds,
        constraints=constraints
    )
    
    alpha = result.x
    
    # Recover w from w = Œ£Œ±·µ¢y·µ¢x·µ¢
    w = np.sum(alpha.reshape(-1,1) * y.reshape(-1,1) * X, axis=0)
    
    # Find support vectors
    sv_idx = alpha > 1e-5
    
    # Recover b from any support vector
    b = y[sv_idx][0] - np.dot(w, X[sv_idx][0])
    
    return w, b, alpha

# Example usage
X = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])
y = np.array([1, 1, 1, -1, -1])

w, b, alpha = svm_dual(X, y)
print(f"w = {w}")
print(f"b = {b}")
print(f"Support vector alphas: {alpha[alpha > 1e-5]}")

```

---

## üìö Resources

| Type | Resource | Description |
|------|----------|-------------|
| üìñ | [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) | Boyd & Vandenberghe Ch 5 |
| üìñ | Numerical Optimization | Nocedal & Wright Ch 12 |
| üé• | [Stanford EE364A](https://www.youtube.com/playlist?list=PL3940DD956CDF0622) | Boyd's lectures |

---

## üó∫Ô∏è Navigation

| ‚¨ÖÔ∏è Previous | üè† Home | ‚û°Ô∏è Next |
|:-----------:|:-------:|:-------:|
| [Basics](../01_basics/README.md) | [Optimization](../README.md) | [Convex](../03_convex/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
