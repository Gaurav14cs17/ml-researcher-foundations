<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=00C853&height=120&section=header&text=Second-Order%20Methods&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-02-00C853?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/second-order.svg" width="100%">

*Caption: Newton: Î¸ â† Î¸ - Hâ»Â¹âˆ‡f uses Hessian curvature. Converges quadratically near optimum. Cost: O(nÂ³) for Hessian inverse. Quasi-Newton (BFGS, L-BFGS) approximates Hâ»Â¹ efficiently.*

---

## ğŸ“‚ Overview

Second-order methods use curvature information to take smarter steps. While expensive for deep learning, they're standard for smaller problems and inspire adaptive learning rate methods like Adam.

---

## ğŸ“ Mathematical Definitions

### Newton's Method
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Hâ»Â¹âˆ‡f(Î¸â‚œ)

Where H = âˆ‡Â²f(Î¸) is the Hessian matrix

Convergence: Quadratic near optimum
             ||Î¸â‚œâ‚Šâ‚ - Î¸*|| â‰¤ c||Î¸â‚œ - Î¸*||Â²

Cost: O(nÂ³) for matrix inverse (expensive!)
```

### Hessian Properties
```
H = âˆ‡Â²f(Î¸) = [âˆ‚Â²f/âˆ‚Î¸áµ¢âˆ‚Î¸â±¼]

At minimum Î¸*:
â€¢ H â‰» 0 (positive definite)
â€¢ Eigenvalues = curvatures along eigenvectors
â€¢ Large eigenvalue = steep direction
â€¢ Small eigenvalue = flat direction
```

### Quasi-Newton Methods
```
Instead of computing Hâ»Â¹, approximate it:
B_{t+1} â‰ˆ Hâ»Â¹

BFGS Update:
Bâ‚œâ‚Šâ‚ = (I - Ïâ‚œsâ‚œyâ‚œáµ€)Bâ‚œ(I - Ïâ‚œyâ‚œsâ‚œáµ€) + Ïâ‚œsâ‚œsâ‚œáµ€

Where:
â€¢ sâ‚œ = Î¸â‚œâ‚Šâ‚ - Î¸â‚œ
â€¢ yâ‚œ = âˆ‡f(Î¸â‚œâ‚Šâ‚) - âˆ‡f(Î¸â‚œ)
â€¢ Ïâ‚œ = 1/(yâ‚œáµ€sâ‚œ)
```

### L-BFGS (Limited Memory BFGS)
```
Store only last m (s, y) pairs
Memory: O(mn) instead of O(nÂ²)
Used for large-scale optimization
```

### Natural Gradient
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· Fâ»Â¹âˆ‡L(Î¸â‚œ)

F = Fisher Information Matrix
F = E[âˆ‡log p(x|Î¸) âˆ‡log p(x|Î¸)áµ€]

Natural gradient accounts for parameter space geometry
TRPO/PPO use approximations of natural gradient
```

---

## ğŸ’» Code Examples

```python
import numpy as np
from scipy.optimize import minimize

# L-BFGS-B for optimization
result = minimize(
    fun=loss_fn,
    x0=initial_params,
    method='L-BFGS-B',
    jac=grad_fn,  # Gradient function
    options={'maxiter': 100}
)
optimal_params = result.x

# Newton's method (small scale)
def newton_step(theta, grad, hessian):
    return theta - np.linalg.solve(hessian, grad)

# Hessian computation in PyTorch
import torch
from torch.autograd.functional import hessian

def loss_fn(params):
    return (params ** 2).sum()

H = hessian(loss_fn, torch.randn(3))

# Approximate second-order: Adam adapts per-parameter learning rate
# v_t â‰ˆ diagonal of Hessian estimate
```

---

## ğŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Nocedal: Numerical Opt | [Book](https://www.springer.com/gp/book/9780387303031) |
| ğŸ“„ | Natural Gradient | [Paper](https://www.jmlr.org/papers/v3/amari02a.html) |
| ğŸ“– | Boyd: Convex Opt Ch. 9 | [Book](https://web.stanford.edu/~boyd/cvxbook/) |
| ğŸ‡¨ğŸ‡³ | ç‰›é¡¿æ³•è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25383715) |
| ğŸ‡¨ğŸ‡³ | L-BFGSåŸç† | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88318597) |
| ğŸ‡¨ğŸ‡³ | ä¼˜åŒ–ç®—æ³•å¯¹æ¯” | [Bç«™](https://www.bilibili.com/video/BV1Vt411G7nq) |

---

## ğŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [First Order](../05_first_order/README.md) | [Optimization](../README.md) | [Stochastic](../07_stochastic/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=00C853&height=80&section=footer" width="100%"/>
</p>
