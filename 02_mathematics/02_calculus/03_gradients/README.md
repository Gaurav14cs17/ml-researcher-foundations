<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Gradients%20Â·%20Jacobians%20Â·%20Hessians&fontSize=34&fontColor=fff&animation=twinkling&fontAlignY=32&desc=Multivariable%20Derivatives%20for%20Machine%20Learning&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-02.03_Gradients-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-Gradient_Jacobian_Hessian-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## âš¡ TL;DR

> **These are the fundamental derivatives of multivariable calculus.** Gradients point to steepest ascent, Jacobians linearize vector functions, Hessians capture curvature.

- ğŸ“ **Gradient** ($\nabla f$): Direction of steepest increase for scalar functions
- ğŸ“Š **Jacobian** ($J$): Matrix of all partial derivatives for vector functions
- ğŸ”„ **Hessian** ($H$): Matrix of second derivatives, captures curvature

---

## ğŸ“‘ Table of Contents

1. [Gradient](#1-gradient)
2. [Jacobian Matrix](#2-jacobian-matrix)
3. [Hessian Matrix](#3-hessian-matrix)
4. [Relationships](#4-relationships-between-them)
5. [Matrix Calculus Identities](#5-matrix-calculus-identities)
6. [Code Implementation](#6-code-implementation)
7. [ML Applications](#7-ml-applications)
8. [Resources](#-resources)

---

## ğŸ¨ Visual Overview

<img src="./images/jacobian-hessian-complete.svg" width="100%">

```
+-----------------------------------------------------------------------------+
|                    GRADIENT / JACOBIAN / HESSIAN                             |
+-----------------------------------------------------------------------------+
|                                                                              |
|   FUNCTION TYPE           DERIVATIVE TYPE         DIMENSION                 |
|   -------------           ---------------         ---------                 |
|                                                                              |
|   f: â„ â†’ â„                f'(x)                   Scalar                    |
|   (scalar â†’ scalar)       (derivative)                                       |
|                                                                              |
|   f: â„â¿ â†’ â„               âˆ‡f(x)                   Vector (nÃ—1)              |
|   (vector â†’ scalar)       (GRADIENT)              Points to steepest ascent |
|                                                                              |
|   f: â„â¿ â†’ â„áµ              J(x)                    Matrix (mÃ—n)              |
|   (vector â†’ vector)       (JACOBIAN)              Row i = âˆ‡fáµ¢               |
|                                                                              |
|   f: â„â¿ â†’ â„               H(x) = âˆ‡Â²f(x)           Matrix (nÃ—n)              |
|   (second derivative)     (HESSIAN)               Symmetric, curvature      |
|                                                                              |
|   GRADIENT DESCENT: Î¸ â† Î¸ - Î±âˆ‡L(Î¸)                                          |
|   NEWTON'S METHOD: Î¸ â† Î¸ - Hâ»Â¹âˆ‡L(Î¸)                                         |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## 1. Gradient

### ğŸ“Œ Definition

For $f: \mathbb{R}^n \to \mathbb{R}$:

```math
\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}

```

### ğŸ“ Key Properties

| Property | Formula | Interpretation |
|----------|---------|----------------|
| Direction | $\nabla f / \|\nabla f\|$ | Steepest ascent direction |
| Magnitude | $\|\nabla f\|$ | Rate of maximum increase |
| Level sets | $\nabla f \perp$ level curves | Perpendicular to contours |
| Critical point | $\nabla f = 0$ | Local extremum or saddle |

### ğŸ” Proof: Gradient Points to Steepest Ascent

```
Goal: Show that âˆ‡f is the direction of maximum rate of change.

Step 1: Directional derivative in direction u (unit vector):
        Dáµ¤f = lim_{hâ†’0} [f(x + hu) - f(x)] / h
            = âˆ‡f Â· u  (by chain rule)
            = â€–âˆ‡fâ€– â€–uâ€– cos(Î¸)
            = â€–âˆ‡fâ€– cos(Î¸)  (since â€–uâ€– = 1)

Step 2: Maximum rate of change
        Dáµ¤f is maximized when cos(Î¸) = 1, i.e., Î¸ = 0
        This means u points in same direction as âˆ‡f

Step 3: Value of maximum rate
        max Dáµ¤f = â€–âˆ‡fâ€– Â· 1 = â€–âˆ‡fâ€–

Therefore: âˆ‡f points in direction of steepest ascent
           with rate of increase = â€–âˆ‡fâ€–  âˆ

```

### ğŸ’¡ Examples

**Example 1**: Quadratic Function

```
f(x, y) = xÂ² + 4yÂ²

âˆ‚f/âˆ‚x = 2x
âˆ‚f/âˆ‚y = 8y

âˆ‡f = [2x, 8y]áµ€

At (1, 2):
  âˆ‡f = [2, 16]áµ€
  Direction of steepest ascent: [2, 16]/â€–[2, 16]â€–
  Rate of increase: âˆš(4 + 256) = âˆš260 â‰ˆ 16.1

```

**Example 2**: Cross-Entropy Loss

```
L = -Î£áµ¢ yáµ¢ log(páµ¢)  where páµ¢ = softmax(záµ¢)

âˆ‚L/âˆ‚zâ±¼ = pâ±¼ - yâ±¼  (the famous softmax gradient!)

Proof:
  For softmax: pâ±¼ = e^zâ±¼ / Î£â‚– e^zâ‚–
  
  âˆ‚pâ±¼/âˆ‚zâ‚– = pâ±¼(Î´â±¼â‚– - pâ‚–)  where Î´â±¼â‚– is Kronecker delta
  
  âˆ‚L/âˆ‚zâ±¼ = -Î£áµ¢ yáµ¢ (1/páµ¢) âˆ‚páµ¢/âˆ‚zâ±¼
         = -Î£áµ¢ yáµ¢ (1/páµ¢) páµ¢(Î´áµ¢â±¼ - pâ±¼)
         = -Î£áµ¢ yáµ¢(Î´áµ¢â±¼ - pâ±¼)
         = -yâ±¼ + pâ±¼Î£áµ¢yáµ¢
         = -yâ±¼ + pâ±¼  (since Î£áµ¢yáµ¢ = 1 for one-hot)
         = pâ±¼ - yâ±¼  âˆ

```

---

## 2. Jacobian Matrix

### ğŸ“Œ Definition

For $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$:

```math
J = \begin{bmatrix} \nabla f_1^T \\ \nabla f_2^T \\ \vdots \\ \nabla f_m^T \end{bmatrix} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}

```

**Key**: Row $i$ = gradient of $f\_i$, Column $j$ = sensitivity to $x\_j$

### ğŸ“ Linear Approximation

```math
\mathbf{f}(\mathbf{x} + \delta) \approx \mathbf{f}(\mathbf{x}) + J(\mathbf{x}) \cdot \delta

```

The Jacobian is the **best linear approximation** to $\mathbf{f}$ near $\mathbf{x}$.

### ğŸ” Chain Rule with Jacobians

For $\mathbf{h} = \mathbf{f} \circ \mathbf{g}$:

```math
J_{\mathbf{h}} = J_{\mathbf{f}} \cdot J_{\mathbf{g}}

```

This is why backpropagation multiplies Jacobians!

### ğŸ’¡ Examples

**Example 1**: Basic Jacobian

```
f: â„Â² â†’ â„Â³
f(x, y) = [xÂ²y, xyÂ², x+y]

J = [âˆ‚fâ‚/âˆ‚x  âˆ‚fâ‚/âˆ‚y]   [2xy   xÂ² ]
    [âˆ‚fâ‚‚/âˆ‚x  âˆ‚fâ‚‚/âˆ‚y] = [yÂ²    2xy]
    [âˆ‚fâ‚ƒ/âˆ‚x  âˆ‚fâ‚ƒ/âˆ‚y]   [1     1  ]

At (1, 2):
J = [4   1]
    [4   4]
    [1   1]

```

**Example 2**: Neural Network Layer

```
Layer: h = Ïƒ(Wx + b)  where Ïƒ is element-wise activation

âˆ‚h/âˆ‚x = diag(Ïƒ'(Wx + b)) Â· W

Jacobian = D Â· W  where D = diagonal matrix of activation derivatives

For ReLU: D = diag(1[Wx+b > 0])
For sigmoid: D = diag(Ïƒ(z)(1-Ïƒ(z)))

```

### ğŸ“ Jacobian Determinant

For square Jacobian ($m = n$):

```math
\det(J) = \text{local volume scaling factor}

```

**Applications**:
- Normalizing flows: $p(x) = p(f^{-1}(x)) \cdot |\det(J\_{f^{-1}})|$
- Change of variables in integration

---

## 3. Hessian Matrix

### ğŸ“Œ Definition

For $f: \mathbb{R}^n \to \mathbb{R}$:

```math
H = \nabla^2 f = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}

```

**Key**: $H$ is symmetric for $C^2$ functions (Schwarz's theorem)

### ğŸ“ Taylor Expansion (Quadratic Approximation)

```math
f(\mathbf{x} + \delta) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^T \delta + \frac{1}{2} \delta^T H(\mathbf{x}) \delta

```

### ğŸ“ Critical Point Classification

At a critical point where $\nabla f = 0$:

| Hessian Property | Point Type |
|------------------|------------|
| $H \succ 0$ (positive definite) | **Local minimum** |
| $H \prec 0$ (negative definite) | **Local maximum** |
| $H$ indefinite | **Saddle point** |
| $H$ singular | Inconclusive (higher order needed) |

### ğŸ” Proof: Positive Definite Hessian âŸ¹ Local Minimum

```
Proof:

Step 1: Taylor expansion at critical point x* where âˆ‡f(x*) = 0
        f(x* + Î´) = f(x*) + âˆ‡f(x*)áµ€Î´ + Â½Î´áµ€H(x*)Î´ + O(â€–Î´â€–Â³)
                  = f(x*) + 0 + Â½Î´áµ€HÎ´ + O(â€–Î´â€–Â³)
                  = f(x*) + Â½Î´áµ€HÎ´ + O(â€–Î´â€–Â³)

Step 2: If H â‰» 0, then Î´áµ€HÎ´ > 0 for all Î´ â‰  0
        Let Î»_min > 0 be smallest eigenvalue of H
        Then: Î´áµ€HÎ´ â‰¥ Î»_min â€–Î´â€–Â²

Step 3: For small enough â€–Î´â€–:
        Â½Î´áµ€HÎ´ dominates O(â€–Î´â€–Â³)
        So: f(x* + Î´) > f(x*) for all small Î´ â‰  0

Step 4: This means x* is a local minimum.  âˆ

```

### ğŸ’¡ Examples

**Example 1**: Quadratic Function

```
f(x, y) = xÂ² + 3xy + 2yÂ²

âˆ‡f = [2x + 3y, 3x + 4y]áµ€

H = [âˆ‚Â²f/âˆ‚xÂ²     âˆ‚Â²f/âˆ‚xâˆ‚y]   [2  3]
    [âˆ‚Â²f/âˆ‚yâˆ‚x   âˆ‚Â²f/âˆ‚yÂ²  ] = [3  4]

Eigenvalues of H: 
  Î» = (6 Â± âˆš(36-4Â·(-1)))/2 = (6 Â± âˆš40)/2 â‰ˆ 6.16, -0.16

H is indefinite (one positive, one negative eigenvalue)
â†’ The origin is a SADDLE POINT

```

**Example 2**: Condition Number

```
f(x, y) = xÂ² + 100yÂ²  (ill-conditioned)

H = [2    0 ]
    [0   200]

Condition number: Îº = Î»_max/Î»_min = 200/2 = 100

This means gradient descent will be slow!
- Fast convergence along x (low curvature)
- Slow convergence along y (high curvature)

```

---

## 4. Relationships Between Them

```
+-----------------------------------------------------------------+
|                     DERIVATIVE HIERARCHY                         |
+-----------------------------------------------------------------+
|                                                                  |
|   f: â„â¿ â†’ â„          f: â„â¿ â†’ â„áµ                                 |
|                                                                  |
|   âˆ‡f âˆˆ â„â¿            J âˆˆ â„áµË£â¿                                   |
|   (gradient)         (Jacobian)                                  |
|       |                  |                                       |
|       |                  |                                       |
|       â–¼                  â–¼                                       |
|   H = âˆ‡(âˆ‡f) âˆˆ â„â¿Ë£â¿   âˆ‚J/âˆ‚x = tensor                            |
|   (Hessian)           (3rd order)                                |
|                                                                  |
|   KEY RELATIONSHIPS:                                             |
|   -----------------                                              |
|   â€¢ H = Jacobian of gradient: H = J(âˆ‡f)                          |
|   â€¢ Gradient = transpose of single-row Jacobian                  |
|   â€¢ For scalar loss L: âˆ‚L/âˆ‚x = (J^T) @ (âˆ‚L/âˆ‚f)                  |
|                                                                  |
+-----------------------------------------------------------------+

```

---

## 5. Matrix Calculus Identities

### ğŸ“Š Essential Identities

| Expression | Gradient / Derivative |
|------------|----------------------|
| $f = \mathbf{a}^T\mathbf{x}$ | $\nabla\_x f = \mathbf{a}$ |
| $f = \mathbf{x}^T\mathbf{x}$ | $\nabla\_x f = 2\mathbf{x}$ |
| $f = \mathbf{x}^T A \mathbf{x}$ | $\nabla\_x f = (A + A^T)\mathbf{x}$, $= 2A\mathbf{x}$ if symmetric |
| $f = \|A\mathbf{x} - \mathbf{b}\|^2$ | $\nabla\_x f = 2A^T(A\mathbf{x} - \mathbf{b})$ |
| $f = \text{tr}(AB)$ | $\nabla\_A f = B^T$ |
| $f = \log \det(A)$ | $\nabla\_A f = A^{-T}$ |

### ğŸ” Derivation: Gradient of $\|\mathbf{Ax} - \mathbf{b}\|^2$

```
f(x) = â€–Ax - bâ€–Â² = (Ax - b)áµ€(Ax - b)

Step 1: Expand
        f = xáµ€Aáµ€Ax - 2xáµ€Aáµ€b + báµ€b

Step 2: Take gradient
        âˆ‡f = 2Aáµ€Ax - 2Aáµ€b + 0
           = 2Aáµ€(Ax - b)

Step 3: Set to zero for least squares solution
        0 = Aáµ€(Ax - b)
        Aáµ€Ax = Aáµ€b
        x = (Aáµ€A)â»Â¹Aáµ€b  (normal equations)

```

---

## 6. Code Implementation

```python
import torch
import numpy as np
from torch.autograd.functional import jacobian, hessian

# ============================================================
# GRADIENT COMPUTATION
# ============================================================

def compute_gradient(f, x):
    """Compute gradient of scalar function f at point x."""
    x = x.clone().requires_grad_(True)
    y = f(x)
    y.backward()
    return x.grad

# Example: Gradient of loss
def mse_loss(params):
    # Simulated prediction and target
    pred = params @ torch.randn(3, 1)
    target = torch.ones(3, 1)
    return ((pred - target) ** 2).mean()

params = torch.randn(3, 3)
grad = compute_gradient(mse_loss, params)
print(f"Gradient shape: {grad.shape}")

# ============================================================
# JACOBIAN COMPUTATION
# ============================================================

def neural_layer(x):
    """Example: Linear layer followed by ReLU."""
    W = torch.randn(5, 3)
    return torch.relu(x @ W.T)

x = torch.randn(3)
J = jacobian(neural_layer, x)
print(f"Jacobian shape: {J.shape}")  # (5, 3)

# Verify Jacobian numerically
def numerical_jacobian(f, x, eps=1e-5):
    n = len(x)
    m = len(f(x))
    J = torch.zeros(m, n)
    for i in range(n):
        x_plus = x.clone()
        x_plus[i] += eps
        x_minus = x.clone()
        x_minus[i] -= eps
        J[:, i] = (f(x_plus) - f(x_minus)) / (2 * eps)
    return J

# ============================================================
# HESSIAN COMPUTATION
# ============================================================

def quadratic(x):
    """Example: Quadratic function x^T A x + b^T x."""
    A = torch.tensor([[2., 1.], [1., 3.]])
    b = torch.tensor([1., 2.])
    return x @ A @ x + b @ x

x = torch.tensor([1., 1.])
H = hessian(quadratic, x)
print(f"Hessian:\n{H}")

# Analyze critical point
eigenvalues = torch.linalg.eigvalsh(H)
print(f"Eigenvalues: {eigenvalues}")
if torch.all(eigenvalues > 0):
    print("Positive definite â†’ Local minimum exists")
elif torch.all(eigenvalues < 0):
    print("Negative definite â†’ Local maximum exists")
else:
    print("Indefinite â†’ Saddle point")

# ============================================================
# OPTIMIZATION USING GRADIENTS
# ============================================================

def gradient_descent(f, x0, lr=0.01, n_steps=100):
    """Basic gradient descent."""
    x = x0.clone().requires_grad_(True)
    history = [x.detach().clone()]
    
    for _ in range(n_steps):
        loss = f(x)
        loss.backward()
        
        with torch.no_grad():
            x -= lr * x.grad
            x.grad.zero_()
        
        history.append(x.detach().clone())
    
    return x.detach(), history

def newtons_method(f, x0, n_steps=10):
    """Newton's method using Hessian."""
    x = x0.clone()
    
    for _ in range(n_steps):
        # Compute gradient and Hessian
        x_req = x.clone().requires_grad_(True)
        grad = torch.autograd.grad(f(x_req), x_req, create_graph=True)[0]
        H = hessian(f, x)
        
        # Newton update: x = x - Hâ»Â¹âˆ‡f
        with torch.no_grad():
            x = x - torch.linalg.solve(H, grad)
    
    return x

# Example: Minimize Rosenbrock function
def rosenbrock(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

x0 = torch.tensor([-1., 1.])
x_opt, history = gradient_descent(rosenbrock, x0, lr=0.001, n_steps=10000)
print(f"GD solution: {x_opt}")  # Should approach [1, 1]

```

---

## 7. ML Applications

### ğŸ¤– Gradient Descent

```python
# The fundamental training loop
for batch in dataloader:
    # Forward
    loss = model(batch)
    
    # Compute gradients (via chain rule)
    loss.backward()  # âˆ‡L(Î¸) computed
    
    # Update parameters
    optimizer.step()  # Î¸ â† Î¸ - Î±âˆ‡L(Î¸)
    optimizer.zero_grad()

```

### ğŸ¤– Newton's Method (Second-Order)

```python
# Uses Hessian for faster convergence
# Update: Î¸ â† Î¸ - Hâ»Â¹âˆ‡L

# Impractical for deep learning (H is nÃ—n where n = millions)
# Approximations: L-BFGS, K-FAC, Adam (diagonal Hessian)

```

### ğŸ¤– Normalizing Flows

```python
# Change of variables requires Jacobian determinant
# p(x) = p(fâ»Â¹(x)) Â· |det(J_{fâ»Â¹}(x))|

def log_prob_flow(z, flow_layers):
    log_prob = standard_normal_log_prob(z)
    x = z
    for layer in flow_layers:
        x, log_det_J = layer.forward_with_log_det(x)
        log_prob -= log_det_J  # Subtract because inverse
    return x, log_prob

```

---

## ğŸ“š Resources

| Type | Resource | Description |
|------|----------|-------------|
| ğŸ¥ | [3Blue1Brown: Gradient](https://www.youtube.com/watch?v=tIpKfDc295M) | Visual intuition |
| ğŸ“– | [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) | Identity reference |
| ğŸ¥ | [Khan Academy Multivariable](https://www.khanacademy.org/math/multivariable-calculus) | Complete course |

---

## ğŸ—ºï¸ Navigation

| â¬…ï¸ Previous | ğŸ  Home | â¡ï¸ Next |
|:-----------:|:-------:|:-------:|
| [Derivatives](../02_derivatives/README.md) | [Calculus](../README.md) | [Integration](../04_integration/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
