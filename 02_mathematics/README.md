<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=180&section=header&text=Mathematics%20for%20ML&fontSize=36&fontColor=fff&animation=twinkling&fontAlignY=32&desc=Linear%20Algebra%20Â·%20Calculus%20Â·%20Optimization&descAlignY=52&descSize=16" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“š_Section-02_Mathematics-00C853?style=for-the-badge" alt="Section"/>
  <img src="https://img.shields.io/badge/ğŸ“Š_Topics-28_Modules-blue?style=for-the-badge" alt="Topics"/>
  <img src="https://img.shields.io/badge/âœï¸_Author-Gaurav_Goswami-purple?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/ğŸ“…_Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“Š Learning Path

```
+-----------------------------------------------------------------------------+
|                           MATHEMATICS LEARNING PATH                          |
+-----------------------------------------------------------------------------+
|                                                                              |
|  ğŸš€ Start --â†’ ğŸ“ Linear Algebra --â†’ ğŸ“ˆ Calculus --â†’ ğŸ¯ Optimization --â†’ âœ…  |
|                    |                     |                |                  |
|                    â–¼                     â–¼                â–¼                  |
|              Vectors/Matrices      Gradients/Chain    GD/SGD/Adam           |
|              Eigen/SVD/PCA         Rule/Hessian       Convergence           |
|                                                                              |
+-----------------------------------------------------------------------------+

```

## ğŸ¯ What You'll Learn

> ğŸ’¡ Every ML algorithm can be understood through **linear algebra, calculus, and optimization**.

<table>
<tr>
<td align="center">

### ğŸ“ Linear Algebra
<img src="https://img.shields.io/badge/12_hours-blue?style=flat-square"/>

Vectors, Matrices, SVD, PCA

</td>
<td align="center">

### ğŸ“ˆ Calculus
<img src="https://img.shields.io/badge/10_hours-green?style=flat-square"/>

Gradients, Chain Rule, Hessian

</td>
<td align="center">

### ğŸ¯ Optimization
<img src="https://img.shields.io/badge/10_hours-orange?style=flat-square"/>

GD, SGD, Adam, Convergence

</td>
</tr>
</table>

---

## ğŸ“š Main Topics

### 1ï¸âƒ£ Linear Algebra

<img src="https://img.shields.io/badge/Time-12_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/Priority-â­â­â­-gold?style=flat-square"/>

```
Vectors --â†’ Matrices --â†’ Multiplication --â†’ Eigen --â†’ SVD --â†’ PCA

```

<details>
<summary><b>ğŸ” Core Concepts</b></summary>

- Vectors & Vector Spaces
- Matrix Operations & Properties
- Linear Transformations
- Eigenvalues & Eigenvectors
- Singular Value Decomposition (SVD)
- Principal Component Analysis (PCA)

</details>

<details>
<summary><b>ğŸ¯ Why It Matters</b></summary>

- Neural networks are matrix multiplications
- PCA for dimensionality reduction
- SVD for recommender systems
- Eigenvalues for stability analysis

</details>

<a href="./01_linear_algebra/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-4285F4?style=for-the-badge" alt="Learn"/></a>

---

### 2ï¸âƒ£ Calculus

<img src="https://img.shields.io/badge/Time-10_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/Priority-â­â­â­-gold?style=flat-square"/>

```
Limits --â†’ Derivatives --â†’ Partial --â†’ Gradient --â†’ Chain Rule --â†’ Jacobian --â†’ Hessian

```

<details>
<summary><b>ğŸ” Core Concepts</b></summary>

- Derivatives & Partial Derivatives
- Gradient Vectors
- Chain Rule (backbone of backpropagation)
- Jacobian Matrices
- Hessian Matrices
- Taylor Series Approximation

</details>

<details>
<summary><b>ğŸ¯ Why It Matters</b></summary>

- Gradients are how neural networks learn
- Chain rule enables backpropagation
- Hessian for second-order optimization
- Taylor series for function approximation

</details>

<a href="./02_calculus/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-4285F4?style=for-the-badge" alt="Learn"/></a>

---

### 3ï¸âƒ£ Optimization Theory

<img src="https://img.shields.io/badge/Time-10_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/Priority-â­â­â­-gold?style=flat-square"/>

```
Convex --â†’ GD --â†’ SGD --â†’ Momentum --â†’ Adam --â†’ Convergence

```

<details>
<summary><b>ğŸ” Core Concepts</b></summary>

- Convex vs Non-Convex Functions
- Gradient Descent Variants
- First-Order Methods
- Second-Order Methods (Newton)
- Convergence Guarantees

</details>

<details>
<summary><b>ğŸ¯ Why It Matters</b></summary>

- Training = Optimization
- Understand SGD, Adam, AdamW
- Know when optimization will work
- Debug training issues

</details>

<a href="./03_optimization/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-4285F4?style=for-the-badge" alt="Learn"/></a>

---

### 4ï¸âƒ£ Distance Metrics

<img src="https://img.shields.io/badge/Time-4_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/Priority-â­â­-gold?style=flat-square"/>

```
L1 (Manhattan) --â†’ L2 (Euclidean) --â†’ Cosine --â†’ Mahalanobis

```

<a href="./04_distance_metrics/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-4285F4?style=for-the-badge" alt="Learn"/></a>

---

## ğŸ”„ How These Connect

```
+-----------------------------------------------------------------------------+
|                          CONNECTIONS                                         |
+-----------------------------------------------------------------------------+
|                                                                              |
|   ğŸ“ Linear Algebra -----+                                                   |
|        |                 |                                                   |
|        +-â†’ Dim Reduction +-----------+                                       |
|        |                 |           |                                       |
|        â–¼                 â–¼           â–¼                                       |
|   ğŸ§  Neural Nets â†-- ğŸ“ˆ Calculus   ğŸš€ Deep Learning                          |
|        â–²                 |           â–²                                       |
|        |                 |           |                                       |
|        +-â†’ Backprop -----+           |                                       |
|        |                 |           |                                       |
|   ğŸ¯ Optimization -------+-â†’ Training +                                      |
|                                                                              |
+-----------------------------------------------------------------------------+

```

---

## ğŸ’¡ Key Formulas

<table>
<tr>
<td>

### ğŸ“ Linear Algebra

```
Matrix:    (AB)áµ€ = Báµ€Aáµ€
Eigen:     A = QÎ›Qáµ€
SVD:       A = UÎ£Váµ€

```

</td>
<td>

### ğŸ“ˆ Calculus

```
Gradient:  âˆ‡f = [âˆ‚f/âˆ‚xáµ¢]áµ€
Chain:     âˆ‚z/âˆ‚x = âˆ‚z/âˆ‚y Â· âˆ‚y/âˆ‚x
Jacobian:  J = [âˆ‚fáµ¢/âˆ‚xâ±¼]

```

</td>
<td>

### ğŸ¯ Optimization

```
GD:   Î¸ â† Î¸ - Î·âˆ‡L(Î¸)
Adam: Î¸ â† Î¸ - Î·Â·mÌ‚/(âˆšvÌ‚+Îµ)

```

</td>
</tr>
</table>

---

## ğŸ“‚ Folder Structure

```
02_mathematics/
+-- 01_linear_algebra/          # 10 topics
|   +-- 01_decompositions/
|   +-- 02_dimensionality_reduction/
|   +-- 03_eigen/
|   +-- 04_eigenvalues/
|   +-- 05_matrix_factorization/
|   +-- 06_matrix_properties/
|   +-- 07_svd/
|   +-- 08_transformations/
|   +-- 09_vectors_matrices/
|   +-- 10_vector_spaces/
+-- 02_calculus/                # 6 topics
|   +-- 01_chain_rule/
|   +-- 02_derivatives/
|   +-- 03_gradients/
|   +-- 04_integration/
|   +-- 05_limits_continuity/
|   +-- 06_taylor/
+-- 03_optimization/            # 7 topics
|   +-- 01_basics/
|   +-- 02_constrained/
|   +-- 03_convex/
|   +-- 04_duality/
|   +-- 05_first_order/
|   +-- 06_second_order/
|   +-- 07_stochastic/
+-- 04_distance_metrics/        # 1 topic

```

---

## ğŸ”— Prerequisites & Next Steps

```
âœ… Foundations --â†’ ğŸ“Š Mathematics --â†’ ğŸ“ˆ Prob/Stats --â†’ ğŸ¯ ML Theory --â†’ ğŸš€ Deep Learning
     (You are here)

```

<p align="center">
  <a href="../01_foundations/README.md"><img src="https://img.shields.io/badge/â†_Prerequisites:_Foundations-gray?style=for-the-badge" alt="Prev"/></a>
  <a href="../03_probability_statistics/README.md"><img src="https://img.shields.io/badge/Next:_Probability_â†’-00C853?style=for-the-badge" alt="Next"/></a>
</p>

---

## ğŸ“š Recommended Resources

| Type | Resource | Focus |
|:----:|----------|-------|
| ğŸ“˜ | [Mathematics for ML](https://mml-book.github.io/) | Complete reference |
| ğŸ¬ | [3Blue1Brown - Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) | Visual intuition |
| ğŸ¬ | [3Blue1Brown - Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) | Fundamentals |
| ğŸ“ | MIT 18.06 | Linear Algebra |

---

## ğŸ—ºï¸ Quick Navigation

| Previous | Current | Next |
|:--------:|:-------:|:----:|
| [ğŸ”¢ Foundations](../01_foundations/README.md) | **ğŸ“Š Mathematics** | [ğŸ“ˆ Probability â†’](../03_probability_statistics/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&animation=twinkling" width="100%"/>
</p>
