<?xml version="1.0" ?>
<ns0:svg xmlns:ns0="http://www.w3.org/2000/svg" viewBox="0.0 0.0 2700.0 1800.0">
  <ns0:rect width="2700.0" height="1800.0" fill="#f8f9fa" rx="8"/>
  <ns0:text x="600" y="40" font-family="Arial, sans-serif" font-size="28" font-weight="bold" text-anchor="middle" fill="#1a237e">
    Mathematics for Machine Learning - Complete Overview
  </ns0:text>
  <ns0:g id="linear-algebra">
    <ns0:rect x="50" y="100" width="350" height="650" fill="#1a237e" stroke="#007bff" stroke-width="4" rx="10"/>
    <ns0:text x="225" y="140" font-family="Arial, sans-serif" font-size="22" font-weight="bold" text-anchor="middle" fill="#1565c0">
      ğŸ“Š Linear Algebra
    </ns0:text>
    <ns0:text x="70" y="180" font-family="Arial, sans-serif" font-size="14" fill="#1a237e" font-weight="bold">Topics:</ns0:text>
    <ns0:text x="80" y="210" font-family="Arial, sans-serif" font-size="13" fill="#1a237e">
      <ns0:tspan x="80" dy="0">â€¢ Vectors &amp; Matrices</ns0:tspan>
      <ns0:tspan x="80" dy="25">â€¢ Vector Spaces</ns0:tspan>
      <ns0:tspan x="80" dy="25">â€¢ Linear Transformations</ns0:tspan>
      <ns0:tspan x="80" dy="25">â€¢ Eigenvalues &amp; Eigenvectors</ns0:tspan>
      <ns0:tspan x="80" dy="25">â€¢ SVD Decomposition</ns0:tspan>
      <ns0:tspan x="80" dy="25">â€¢ QR Decomposition</ns0:tspan>
    </ns0:text>
    <ns0:rect x="70" y="410" width="310" height="120" fill="#e8f4f8" rx="5"/>
    <ns0:text x="225" y="435" font-family="Arial, sans-serif" font-size="13" font-weight="bold" text-anchor="middle" fill="#1565c0">
      ğŸ”¥ Why It Matters
    </ns0:text>
    <ns0:text x="80" y="460" font-family="Arial, sans-serif" font-size="12" fill="#1a237e">
      <ns0:tspan x="80" dy="0">â€¢ Neural networks = matrix ops</ns0:tspan>
      <ns0:tspan x="80" dy="20">â€¢ PCA = eigenvalue decomposition</ns0:tspan>
      <ns0:tspan x="80" dy="20">â€¢ Recommender systems = SVD</ns0:tspan>
      <ns0:tspan x="80" dy="20">â€¢ All of deep learning!</ns0:tspan>
    </ns0:text>
    <ns0:rect x="70" y="550" width="310" height="180" fill="#d5f4e6" rx="5"/>
    <ns0:text x="225" y="575" font-family="Arial, sans-serif" font-size="13" font-weight="bold" text-anchor="middle" fill="#2e7d32">
      ğŸ“ˆ Key Formulas
    </ns0:text>
    <ns0:text x="80" y="600" font-family="Arial, sans-serif" font-size="11" fill="#1a237e">
      <ns0:tspan x="80" dy="0">â€¢ Matrix multiply: C = AB</ns0:tspan>
      <ns0:tspan x="80" dy="22">â€¢ Eigenvalue: Av = Î»v</ns0:tspan>
      <ns0:tspan x="80" dy="22">â€¢ SVD: A = UÎ£Váµ€</ns0:tspan>
      <ns0:tspan x="80" dy="22">â€¢ Dot product: aÂ·b = Î£aáµ¢báµ¢</ns0:tspan>
      <ns0:tspan x="80" dy="22">â€¢ L2 norm: â€–xâ€–â‚‚ = âˆš(Î£xáµ¢Â²)</ns0:tspan>
    </ns0:text>
  </ns0:g>
  <ns0:g id="calculus">
    <ns0:rect x="425" y="100" width="350" height="650" fill="#1a237e" stroke="#dc3545" stroke-width="4" rx="10"/>
    <ns0:text x="600" y="625.0" font-family="Arial, sans-serif" font-size="22" font-weight="bold" text-anchor="middle" fill="#c62828">
      ğŸ“ˆ Calculus
    </ns0:text>
    <ns0:text x="445" y="650.0" font-family="Arial, sans-serif" font-size="14" fill="#1a237e" font-weight="bold">Topics:</ns0:text>
    <ns0:text x="455" y="675.0" font-family="Arial, sans-serif" font-size="13" fill="#1a237e">
      <ns0:tspan x="455" dy="0">â€¢ Limits &amp; Continuity</ns0:tspan>
      <ns0:tspan x="455" dy="25">â€¢ Partial Derivatives</ns0:tspan>
      <ns0:tspan x="455" dy="25">â€¢ Gradients (âˆ‡f)</ns0:tspan>
      <ns0:tspan x="455" dy="25">â€¢ Jacobian Matrix</ns0:tspan>
      <ns0:tspan x="455" dy="25">â€¢ Hessian Matrix</ns0:tspan>
      <ns0:tspan x="455" dy="25">â€¢ Chain Rule</ns0:tspan>
      <ns0:tspan x="455" dy="25">â€¢ Taylor Series</ns0:tspan>
    </ns0:text>
    <ns0:rect x="445" y="435" width="310" height="95" fill="#1a237e" rx="5"/>
    <ns0:text x="600" y="700.0" font-family="Arial, sans-serif" font-size="13" font-weight="bold" text-anchor="middle" fill="#c62828">
      ğŸ”¥ Why It Matters
    </ns0:text>
    <ns0:text x="455" y="725.0" font-family="Arial, sans-serif" font-size="12" fill="#1a237e">
      <ns0:tspan x="455" dy="0">â€¢ Gradients = how to optimize</ns0:tspan>
      <ns0:tspan x="455" dy="20">â€¢ Chain rule = backpropagation</ns0:tspan>
      <ns0:tspan x="455" dy="20">â€¢ THE foundation of training</ns0:tspan>
    </ns0:text>
    <ns0:rect x="445" y="550" width="310" height="180" fill="#fff5e6" rx="5"/>
    <ns0:text x="600" y="750.0" font-family="Arial, sans-serif" font-size="13" font-weight="bold" text-anchor="middle" fill="#e65100">
      ğŸ“ˆ Key Formulas
    </ns0:text>
    <ns0:text x="455" y="775.0" font-family="Arial, sans-serif" font-size="11" fill="#1a237e">
      <ns0:tspan x="455" dy="0">â€¢ Gradient: âˆ‡f = [âˆ‚f/âˆ‚xâ‚...âˆ‚f/âˆ‚xâ‚™]áµ€</ns0:tspan>
      <ns0:tspan x="455" dy="22">â€¢ Chain rule: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚yÂ·âˆ‚y/âˆ‚x</ns0:tspan>
      <ns0:tspan x="455" dy="22">â€¢ Jacobian: Jáµ¢â±¼ = âˆ‚fáµ¢/âˆ‚xâ±¼</ns0:tspan>
      <ns0:tspan x="455" dy="22">â€¢ Hessian: Háµ¢â±¼ = âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼</ns0:tspan>
      <ns0:tspan x="455" dy="22">â€¢ Taylor: f(x+h) â‰ˆ f(x)+âˆ‡fÂ·h</ns0:tspan>
    </ns0:text>
  </ns0:g>
  <ns0:g id="optimization">
    <ns0:rect x="800" y="100" width="350" height="650" fill="#1a237e" stroke="#6f42c1" stroke-width="4" rx="10"/>
    <ns0:text x="975" y="800.0" font-family="Arial, sans-serif" font-size="22" font-weight="bold" text-anchor="middle" fill="#6a1b9a">
      âš¡ Optimization
    </ns0:text>
    <ns0:text x="820" y="825.0" font-family="Arial, sans-serif" font-size="14" fill="#1a237e" font-weight="bold">Topics:</ns0:text>
    <ns0:text x="830" y="850.0" font-family="Arial, sans-serif" font-size="13" fill="#1a237e">
      <ns0:tspan x="830" dy="0">â€¢ Convex Functions</ns0:tspan>
      <ns0:tspan x="830" dy="25">â€¢ Gradient Descent</ns0:tspan>
      <ns0:tspan x="830" dy="25">â€¢ SGD &amp; Variants</ns0:tspan>
      <ns0:tspan x="830" dy="25">â€¢ Adam Optimizer</ns0:tspan>
      <ns0:tspan x="830" dy="25">â€¢ Newton's Method</ns0:tspan>
      <ns0:tspan x="830" dy="25">â€¢ Lagrange Multipliers</ns0:tspan>
      <ns0:tspan x="830" dy="25">â€¢ KKT Conditions</ns0:tspan>
    </ns0:text>
    <ns0:rect x="820" y="435" width="310" height="95" fill="#f4ecf7" rx="5"/>
    <ns0:text x="975" y="875.0" font-family="Arial, sans-serif" font-size="13" font-weight="bold" text-anchor="middle" fill="#6a1b9a">
      ğŸ”¥ Why It Matters
    </ns0:text>
    <ns0:text x="830" y="900.0" font-family="Arial, sans-serif" font-size="12" fill="#1a237e">
      <ns0:tspan x="830" dy="0">â€¢ Training = optimization</ns0:tspan>
      <ns0:tspan x="830" dy="20">â€¢ Adam = default optimizer</ns0:tspan>
      <ns0:tspan x="830" dy="20">â€¢ Powers ALL ML training</ns0:tspan>
    </ns0:text>
    <ns0:rect x="820" y="550" width="310" height="180" fill="#e8f8f5" rx="5"/>
    <ns0:text x="975" y="925.0" font-family="Arial, sans-serif" font-size="13" font-weight="bold" text-anchor="middle" fill="#2e7d32">
      ğŸ“ˆ Key Formulas
    </ns0:text>
    <ns0:text x="830" y="950.0" font-family="Arial, sans-serif" font-size="11" fill="#1a237e">
      <ns0:tspan x="830" dy="0">â€¢ GD: Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±âˆ‡L(Î¸â‚œ)</ns0:tspan>
      <ns0:tspan x="830" dy="22">â€¢ Momentum: vâ‚œ = Î²vâ‚œâ‚‹â‚ + âˆ‡L</ns0:tspan>
      <ns0:tspan x="830" dy="22">â€¢ Adam: adaptive learning rate</ns0:tspan>
      <ns0:tspan x="830" dy="22">â€¢ Lagrangian: â„’ = f + Î»g</ns0:tspan>
      <ns0:tspan x="830" dy="22">â€¢ KKT: âˆ‡â„’=0, Î»áµ¢gáµ¢=0</ns0:tspan>
    </ns0:text>
  </ns0:g>
  <ns0:text x="600" y="975.0" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#1a237e" font-style="italic">
    Together these form the complete mathematical foundation for machine learning
  </ns0:text>
</ns0:svg>