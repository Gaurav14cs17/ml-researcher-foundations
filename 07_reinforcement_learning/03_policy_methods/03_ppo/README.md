<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Proximal%20Policy%20Optimization%20P&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Policy Gradient](../02_policy_gradient/) | â¡ï¸ [Next: TRPO](../04_trpo/)

---

## ğŸ¯ Visual Overview

<img src="../images/ppo-clipping.svg" width="100%">

*Caption: PPO's clipped objective prevents large policy updates that could destabilize training. This is why PPO is the algorithm of choice for training ChatGPT, Claude, and most RLHF systems.*

---

## ğŸ“ Mathematical Formulation

### Policy Gradient Foundation

```
Policy Gradient Theorem:
âˆ‡J(Î¸) = E_Ï€[âˆ‡log Ï€Î¸(a|s) Â· QÏ€(s,a)]
      = E_Ï€[âˆ‡log Ï€Î¸(a|s) Â· AÏ€(s,a)]  (with baseline)

Where:
â€¢ J(Î¸) = E[Î£â‚œ Î³áµ— râ‚œ] (expected return)
â€¢ AÏ€(s,a) = QÏ€(s,a) - VÏ€(s) (advantage)
```

### TRPO Objective (Precursor)

```
maximize L(Î¸) = E[Ï€Î¸(a|s)/Ï€Î¸_old(a|s) Â· A(s,a)]
  subject to: KL(Ï€Î¸_old || Ï€Î¸) â‰¤ Î´

Problem: Constraint is expensive to compute
Solution: PPO approximates the constraint via clipping
```

### PPO Clipped Objective

```
L^CLIP(Î¸) = E[min(râ‚œ(Î¸)Aâ‚œ, clip(râ‚œ(Î¸), 1-Îµ, 1+Îµ)Aâ‚œ)]

Where:
â€¢ râ‚œ(Î¸) = Ï€Î¸(aâ‚œ|sâ‚œ) / Ï€Î¸_old(aâ‚œ|sâ‚œ)  (probability ratio)
â€¢ Îµ âˆˆ [0.1, 0.2]  (clipping parameter)
â€¢ Aâ‚œ = advantage estimate (typically GAE)

How clipping works:
If Aâ‚œ > 0 (good action): clip ratio at 1+Îµ (limit improvement)
If Aâ‚œ < 0 (bad action):  clip ratio at 1-Îµ (limit damage)

This keeps Ï€Î¸ close to Ï€Î¸_old without explicit KL constraint!
```

### Generalized Advantage Estimation (GAE)

```
Ã‚â‚œ^GAE(Î») = Î£â‚—â‚Œâ‚€^âˆ (Î³Î»)Ë¡ Î´â‚œâ‚Šâ‚—

Where:
â€¢ Î´â‚œ = râ‚œ + Î³V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)  (TD error)
â€¢ Î» âˆˆ [0.9, 0.99]  (bias-variance tradeoff)

Î» = 0: Low variance, high bias (1-step TD)
Î» = 1: High variance, low bias (Monte Carlo)
Î» = 0.95: Common practical choice
```

### Full PPO Objective

```
L(Î¸) = E[L^CLIP(Î¸) - câ‚ L^VF(Î¸) + câ‚‚ S[Ï€Î¸](s)]

Where:
â€¢ L^VF = (VÎ¸(s) - Vâ‚œarget)Â²  (value function loss)
â€¢ S[Ï€] = -Î£â‚ Ï€(a|s) log Ï€(a|s)  (entropy bonus)
â€¢ câ‚ â‰ˆ 0.5, câ‚‚ â‰ˆ 0.01  (coefficients)

Entropy bonus encourages exploration!
```

---

## ğŸ“ PPO for RLHF

```
RLHF Objective:
maximize E[R(x, y)] - Î² Â· KL(Ï€Î¸ || Ï€ref)
         ---------   -----------------
         reward      stay close to SFT policy

Where:
â€¢ R(x, y) = learned reward model
â€¢ Ï€ref = SFT policy (frozen reference)
â€¢ Î² âˆˆ [0.01, 0.1] (KL penalty coefficient)

PPO update with KL penalty:
L(Î¸) = E[min(râ‚œAâ‚œ, clip(râ‚œ)Aâ‚œ) - Î² log(Ï€Î¸/Ï€ref)]
```

---

## ğŸŒ Applications

| Application | Model | Details |
|-------------|-------|---------|
| **ChatGPT** | GPT-3.5/4 | RLHF with PPO |
| **Claude** | Anthropic | Constitutional AI + PPO |
| **LLaMA-2 Chat** | Meta | PPO fine-tuning |
| **OpenAI Five** | Dota 2 | Distributed PPO |
| **Robotics** | Various | Sim-to-real with PPO |

---

## ğŸ’» Code Examples

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class PPO:
    def __init__(self, policy, value_fn, lr=3e-4, clip_eps=0.2, 
                 gamma=0.99, gae_lambda=0.95, epochs=10):
        self.policy = policy
        self.value_fn = value_fn
        self.optimizer = torch.optim.Adam(
            list(policy.parameters()) + list(value_fn.parameters()), lr=lr
        )
        self.clip_eps = clip_eps
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.epochs = epochs
    
    def compute_gae(self, rewards, values, dones):
        """Generalized Advantage Estimation"""
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages[t] = gae
        
        returns = advantages + values
        return advantages, returns
    
    def update(self, states, actions, old_log_probs, rewards, dones):
        """PPO update step"""
        # Compute values and advantages
        values = self.value_fn(states).squeeze()
        advantages, returns = self.compute_gae(rewards, values.detach(), dones)
        
        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        for _ in range(self.epochs):
            # Current policy log probs
            log_probs = self.policy.log_prob(states, actions)
            
            # Probability ratio
            ratio = torch.exp(log_probs - old_log_probs.detach())
            
            # Clipped surrogate objective
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # Value function loss
            values = self.value_fn(states).squeeze()
            value_loss = F.mse_loss(values, returns)
            
            # Entropy bonus
            entropy = self.policy.entropy(states).mean()
            
            # Total loss
            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()

# RLHF-specific PPO
def rlhf_ppo_loss(policy, ref_policy, states, actions, rewards, beta=0.1):
    """PPO with KL penalty for RLHF"""
    log_probs = policy.log_prob(states, actions)
    ref_log_probs = ref_policy.log_prob(states, actions).detach()
    
    # KL penalty
    kl = log_probs - ref_log_probs
    
    # Reward with KL penalty
    modified_rewards = rewards - beta * kl
    
    # Standard PPO loss with modified rewards
    # ... (compute advantages with modified_rewards)
    
    return policy_loss
```

---

## ğŸ“Š Hyperparameters

| Parameter | Typical Value | Effect |
|-----------|---------------|--------|
| clip_eps (Îµ) | 0.1-0.2 | Trust region size |
| gamma (Î³) | 0.99 | Discount factor |
| gae_lambda (Î») | 0.95 | Bias-variance tradeoff |
| epochs | 3-10 | Updates per batch |
| batch_size | 64-256 | Samples per update |
| lr | 3e-4 | Learning rate |
| entropy_coef | 0.01 | Exploration bonus |
| value_coef | 0.5 | Value loss weight |
| kl_coef (Î²) | 0.01-0.1 | RLHF KL penalty |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | Proximal Policy Optimization | [arXiv](https://arxiv.org/abs/1707.06347) |
| ğŸ“„ | GAE | [arXiv](https://arxiv.org/abs/1506.02438) |
| ğŸ“„ | InstructGPT (RLHF) | [arXiv](https://arxiv.org/abs/2203.02155) |
| ğŸ“– | OpenAI Spinning Up | [Docs](https://spinningup.openai.com/en/latest/algorithms/ppo.html) |
| ğŸ“– | Hugging Face RLHF | [Blog](https://huggingface.co/blog/rlhf) |
| ğŸ’» | CleanRL PPO | [GitHub](https://github.com/vwxyzjn/cleanrl) |
| ğŸ‡¨ğŸ‡³ | PPOç®—æ³•è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/512327050) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ PPOæ•™ç¨‹ | [Bç«™](https://www.bilibili.com/video/BV1cP4y1Y7DN) |
| ğŸ‡¨ğŸ‡³ | RLHFå®æˆ˜ | [CSDN](https://blog.csdn.net/qq_37006625/article/details/129405866) |
| ğŸ‡¨ğŸ‡³ | PPOåŸç†ä¸å®ç° | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2018-06-19-8)

---

## ğŸ”— Where PPO Is Used

| Application | How PPO Is Applied |
|-------------|-------------------|
| **RLHF/ChatGPT** | Fine-tune LLMs to follow human preferences |
| **InstructGPT** | Original RLHF paper using PPO |
| **Claude/Anthropic** | Constitutional AI uses PPO variants |
| **Robotics** | Sim-to-real transfer for motor control |
| **Game Playing** | OpenAI Five (Dota 2), AlphaStar |
| **Autonomous Driving** | Waymo uses RL for planning |

---

â¬…ï¸ [Back: Policy Gradient](../02_policy_gradient/) | â¡ï¸ [Next: TRPO](../04_trpo/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
