<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Policy%20Gradient%20Methods&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Actor-Critic](../01_actor_critic/) | â¡ï¸ [Next: PPO](../03_ppo/)

---

## ğŸ¯ Visual Overview

<img src="./images/policy-gradient.svg" width="100%">

*Caption: Policy gradient methods parameterize the policy Ï€_Î¸(a|s) with a neural network and optimize it directly using gradient ascent on expected return. The REINFORCE algorithm increases probabilities of actions that led to high returns.*

---

## ğŸ“‚ Overview

Policy gradient methods directly optimize the policy parameters Î¸ to maximize expected cumulative reward, without explicitly learning value functions.

---

## ğŸ“ Mathematical Foundation

### Objective Function

```
J(Î¸) = E_Ï„~Ï€_Î¸ [R(Ï„)] = E_Ï„~Ï€_Î¸ [Î£â‚œ Î³áµ— râ‚œ]

Where:
    Ï„ = (sâ‚€, aâ‚€, râ‚€, sâ‚, aâ‚, râ‚, ...) is a trajectory
    Ï€_Î¸(a|s) is the parameterized policy
    R(Ï„) is the total return of trajectory Ï„

```

### Policy Gradient Theorem

```
âˆ‡_Î¸ J(Î¸) = E_Ï„~Ï€_Î¸ [Î£â‚œ âˆ‡_Î¸ log Ï€_Î¸(aâ‚œ|sâ‚œ) Â· Gâ‚œ]

Where:
    Gâ‚œ = Î£â‚–â‚Œâ‚œ^T Î³áµâ»áµ— râ‚–  (return from time t)

Key insight: We can estimate gradients without knowing P(s'|s,a)!

Proof sketch:
    âˆ‡_Î¸ J = âˆ‡_Î¸ E_Ï„[R(Ï„)]
          = âˆ‡_Î¸ âˆ« P(Ï„|Î¸) R(Ï„) dÏ„
          = âˆ« P(Ï„|Î¸) âˆ‡_Î¸ log P(Ï„|Î¸) R(Ï„) dÏ„  (log-derivative trick)
          = E_Ï„ [âˆ‡_Î¸ log P(Ï„|Î¸) Â· R(Ï„)]
          
    And: log P(Ï„|Î¸) = Î£â‚œ log Ï€_Î¸(aâ‚œ|sâ‚œ) + terms not depending on Î¸

```

### REINFORCE Algorithm

```
1. Sample trajectory Ï„ = (sâ‚€, aâ‚€, râ‚€, ..., s_T) using Ï€_Î¸

2. Compute returns Gâ‚œ = Î£â‚–â‚Œâ‚œ^T Î³áµâ»áµ— râ‚–

3. Update: Î¸ â† Î¸ + Î± Î£â‚œ âˆ‡_Î¸ log Ï€_Î¸(aâ‚œ|sâ‚œ) Â· Gâ‚œ

4. Repeat

```

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **REINFORCE** | Vanilla policy gradient using Monte Carlo returns |
| **Baseline** | Subtract b(s) to reduce variance: âˆ‡J âˆ (G - b) |
| **Actor-Critic** | Use learned value function V(s) as baseline |
| **Advantage** | A(s,a) = Q(s,a) - V(s), even lower variance |
| **GAE** | Generalized Advantage Estimation (Î»-weighted) |

### Variance Reduction

```
High Variance Problem:
    âˆ‡_Î¸ J = E[âˆ‡_Î¸ log Ï€(a|s) Â· G]
    
    G varies a lot â†’ noisy gradients â†’ slow learning

Solutions:

1. Baseline: Use V(s) as baseline (doesn't change expectation)
    âˆ‡_Î¸ J = E[âˆ‡_Î¸ log Ï€(a|s) Â· (G - V(s))]
    

2. Advantage: A(s,a) = Q(s,a) - V(s)
    âˆ‡_Î¸ J = E[âˆ‡_Î¸ log Ï€(a|s) Â· A(s,a)]
    

3. GAE: Î»-weighted combination of n-step advantages
    Ã‚_t^GAE = Î£â‚—â‚Œâ‚€^âˆ (Î³Î»)Ë¡ Î´â‚œâ‚Šâ‚—
    where Î´â‚œ = râ‚œ + Î³V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)

```

---

## ğŸ’» Code Examples

### REINFORCE

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_dim)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return F.softmax(self.fc3(x), dim=-1)
    
    def get_action(self, state):
        probs = self.forward(state)
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

def reinforce(env, policy, optimizer, num_episodes, gamma=0.99):
    """REINFORCE algorithm"""
    
    for episode in range(num_episodes):
        log_probs = []
        rewards = []
        
        state = env.reset()
        done = False
        
        # Collect trajectory
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action, log_prob = policy.get_action(state_tensor)
            next_state, reward, done, _ = env.step(action)
            
            log_probs.append(log_prob)
            rewards.append(reward)
            state = next_state
        
        # Compute returns
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        returns = torch.FloatTensor(returns)
        
        # Normalize returns (optional, helps training)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # Policy gradient update
        loss = 0
        for log_prob, G in zip(log_probs, returns):
            loss += -log_prob * G  # Negative for gradient ascent
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    return policy

```

### REINFORCE with Baseline

```python
class PolicyWithBaseline(nn.Module):
    """Actor-Critic style with shared layers"""
    
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )
        self.policy_head = nn.Linear(64, action_dim)
        self.value_head = nn.Linear(64, 1)
    
    def forward(self, x):
        features = self.shared(x)
        probs = F.softmax(self.policy_head(features), dim=-1)
        value = self.value_head(features)
        return probs, value

def reinforce_with_baseline(env, model, optimizer, num_episodes, gamma=0.99):
    """REINFORCE with learned baseline (value function)"""
    
    for episode in range(num_episodes):
        log_probs = []
        values = []
        rewards = []
        
        state = env.reset()
        done = False
        
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            probs, value = model(state_tensor)
            
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()
            
            log_probs.append(dist.log_prob(action))
            values.append(value.squeeze())
            
            state, reward, done, _ = env.step(action.item())
            rewards.append(reward)
        
        # Compute returns
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        returns = torch.FloatTensor(returns)
        values = torch.stack(values)
        
        # Advantages
        advantages = returns - values.detach()
        
        # Policy loss
        policy_loss = -(torch.stack(log_probs) * advantages).mean()
        
        # Value loss
        value_loss = F.mse_loss(values, returns)
        
        # Total loss
        loss = policy_loss + 0.5 * value_loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

```

### Generalized Advantage Estimation (GAE)

```python
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    """
    Compute Generalized Advantage Estimation
    
    GAE(Î³, Î») = Î£â‚—â‚Œâ‚€^âˆ (Î³Î»)Ë¡ Î´â‚œâ‚Šâ‚—
    where Î´â‚œ = râ‚œ + Î³V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)
    """
    advantages = []
    gae = 0
    
    # values should have one extra element (V(s_T+1) = 0 if terminal)
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0  # Terminal state
        else:
            next_value = values[t + 1]
        
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
        gae = delta + gamma * lam * (1 - dones[t]) * gae
        advantages.insert(0, gae)
    
    return torch.FloatTensor(advantages)

```

---

## ğŸ“Š Comparison of Policy Gradient Methods

| Method | Baseline | On/Off-Policy | Variance | Sample Efficiency |
|--------|----------|---------------|----------|-------------------|
| **REINFORCE** | None | On | High | Low |
| **REINFORCE + Baseline** | V(s) | On | Medium | Low |
| **A2C** | V(s) | On | Medium | Low |
| **PPO** | V(s), clipped | On | Low | Medium |
| **TRPO** | V(s), KL constraint | On | Low | Medium |

---

## ğŸ”— Connection to Other Methods

```
Policy Gradient
    |
    +-- REINFORCE (Monte Carlo)
    |       +-- + Baseline â†’ Actor-Critic (A2C)
    |                           +-- + Trust Region â†’ TRPO
    |                                                   +-- + Clipping â†’ PPO
    |
    +-- Connection to Value Methods
            Q(s,a) â‰ˆ advantage estimation
            V(s) as baseline reduces variance

```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Actor-Critic | [../01_actor_critic/](../01_actor_critic/) |
| ğŸ“– | PPO | [../03_ppo/](../03_ppo/) |
| ğŸ“– | TRPO | [../04_trpo/](../04_trpo/) |
| ğŸ“„ | Policy Gradient Theorem | [Paper](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf) |
| ğŸ“„ | GAE Paper | [arXiv](https://arxiv.org/abs/1506.02438) |
| ğŸ‡¨ğŸ‡³ | ç­–ç•¥æ¢¯åº¦ç®—æ³•è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/26174099) |
| ğŸ‡¨ğŸ‡³ | Policy Gradientæ¨å¯¼ | [CSDN](https://blog.csdn.net/qq_30615903/article/details/81275638) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ -ç­–ç•¥æ¢¯åº¦ | [Bç«™](https://www.bilibili.com/video/BV1sd4y167NS) |
| ğŸ‡¨ğŸ‡³ | ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç»¼è¿° | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2018-02-13-4) |
| ğŸ‡¨ğŸ‡³ | GAEè¯¦è§£ | [PaperWeekly](https://www.paperweekly.site/papers/notes/1468)

## ğŸ”— Where This Topic Is Used

| Application | Policy Gradient |
|-------------|----------------|
| **LLM Training** | REINFORCE for RLHF |
| **Robotics** | Direct policy learning |
| **NAS** | Architecture search |
| **Continuous Control** | Gaussian policies |

---

â¬…ï¸ [Back: Actor-Critic](../01_actor_critic/) | â¡ï¸ [Next: PPO](../03_ppo/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
