<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Trust%20Region%20Policy%20Optimization&fontSize=28&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: PPO](../03_ppo/) | â¡ï¸ [Next: Exploration](../../04_exploration/)

---

## ğŸ¯ Visual Overview

<img src="./images/trpo.svg" width="100%">

*Caption: TRPO prevents policy collapse by constraining updates to a trust region defined by KL divergence. This ensures monotonic improvement.*

---

## ğŸ“‚ Overview

TRPO is a policy gradient method that guarantees monotonic improvement by restricting how much the policy can change at each update. It provides the theoretical foundation for PPO.

---

## ğŸ“ Mathematical Foundation

### The Policy Optimization Problem

The goal is to find a policy that maximizes expected return:

```
max_Ï€ J(Ï€) = E_{Ï„~Ï€}[Î£_t Î³^t r_t]

```

The challenge: How do we update the policy without making it worse?

### Conservative Policy Iteration Bound

**Theorem (Kakade & Langford 2002):**

```
For any two policies Ï€ and Ï€':

J(Ï€') â‰¥ J(Ï€) + Î£_s Ï_Ï€(s) Î£_a Ï€'(a|s) A_Ï€(s,a) - C Â· max_s KL(Ï€'(Â·|s) || Ï€(Â·|s))

Where:
â€¢ Ï_Ï€(s) = Î£_t Î³^t P(s_t = s | Ï€) is the discounted state visitation
â€¢ A_Ï€(s,a) = Q_Ï€(s,a) - V_Ï€(s) is the advantage
â€¢ C = 2Î³Îµ_max / (1-Î³)Â² where Îµ_max = max_s KL(Ï€' || Ï€)

```

This bound guarantees improvement if we control the KL divergence!

---

## ğŸ“ TRPO Objective

### Surrogate Objective

Instead of maximizing J(Ï€) directly, TRPO maximizes a surrogate:

```
L(Î¸) = E_{s~Ï_{Î¸_old}, a~Ï€_{Î¸_old}} [Ï€_Î¸(a|s) / Ï€_{Î¸_old}(a|s) Â· A_{Î¸_old}(s,a)]
     = E [Ï(Î¸) Â· A]

Where Ï(Î¸) = Ï€_Î¸(a|s) / Ï€_{Î¸_old}(a|s) is the probability ratio.

```

### Trust Region Constraint

```
TRPO Optimization Problem:

max_Î¸  L(Î¸) = E[Ï(Î¸) Â· A]

subject to:  E_s[KL(Ï€_{Î¸_old}(Â·|s) || Ï€_Î¸(Â·|s))] â‰¤ Î´

Where Î´ is the trust region size (typically 0.01).

```

---

## ğŸ“ Derivation of TRPO Update

### Step 1: Linear Approximation

Near Î¸_old, the objective is approximately linear:

```
L(Î¸) â‰ˆ L(Î¸_old) + g^T(Î¸ - Î¸_old)

Where g = âˆ‡_Î¸ L(Î¸)|_{Î¸=Î¸_old}

```

### Step 2: Quadratic Approximation of Constraint

The KL divergence is approximately quadratic:

```
KL(Ï€_{Î¸_old} || Ï€_Î¸) â‰ˆ Â½(Î¸ - Î¸_old)^T F (Î¸ - Î¸_old)

Where F = E[âˆ‡_Î¸ log Ï€ Â· (âˆ‡_Î¸ log Ï€)^T] is the Fisher Information Matrix.

```

### Step 3: Constrained Optimization

The problem becomes:

```
max_{Î¸}  g^T(Î¸ - Î¸_old)
s.t.     Â½(Î¸ - Î¸_old)^T F (Î¸ - Î¸_old) â‰¤ Î´

Using Lagrangian:
L = g^T d - Î»(Â½ d^T F d - Î´)

Taking derivative: g - Î»Fd = 0  â†’  d = Î»^{-1} F^{-1} g

```

### Step 4: Natural Gradient Direction

```
The optimal step direction is:

d* = F^{-1} g  (Natural gradient!)

The step size Î± is chosen to satisfy the constraint:
Â½ Î±Â² d^T F d = Î´  â†’  Î± = âˆš(2Î´ / d^T F d)

Final update:
Î¸ â† Î¸_old + Î± Â· F^{-1} g
Î¸ â† Î¸_old + âˆš(2Î´ / g^T F^{-1} g) Â· F^{-1} g

```

---

## ğŸ“ Fisher Information Matrix

### Definition

```
F = E_{s~Ï, a~Ï€}[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· (âˆ‡_Î¸ log Ï€_Î¸(a|s))^T]

This is the expected outer product of the score function.

```

### Properties

```
1. F is positive semi-definite

2. F measures the curvature of the KL divergence

3. F^{-1} transforms gradients to natural gradient space

4. Natural gradients are invariant to parameterization

```

### Fisher-Vector Product (Efficient Computation)

Computing F^{-1}g directly is expensive. Instead, use conjugate gradient:

```
Solve: Fx = g  for x using conjugate gradient

The Fisher-vector product Fv can be computed efficiently:

Fv = âˆ‡_Î¸ [âˆ‡_Î¸ L(Î¸)^T v]  (Hessian-vector product on KL)

```

---

## ğŸ“ Conjugate Gradient Algorithm

```
To solve Fx = g without forming F explicitly:

Initialize: x_0 = 0, r_0 = g, p_0 = r_0

For k = 0, 1, 2, ..., until convergence:
    Î±_k = r_k^T r_k / (p_k^T F p_k)
    x_{k+1} = x_k + Î±_k p_k
    r_{k+1} = r_k - Î±_k F p_k
    Î²_k = r_{k+1}^T r_{k+1} / (r_k^T r_k)
    p_{k+1} = r_{k+1} + Î²_k p_k

The key is that F p_k (Fisher-vector product) can be computed efficiently!

```

---

## ğŸ“ Line Search for Constraint Satisfaction

After finding the search direction, perform line search to ensure:

1. The constraint is satisfied

2. The objective actually improves

```
For step_size in [1, 0.5, 0.25, 0.125, ...]:
    Î¸_new = Î¸_old + step_size Â· d
    
    if KL(Ï€_{Î¸_old} || Ï€_{Î¸_new}) â‰¤ Î´  and  L(Î¸_new) > L(Î¸_old):
        Accept Î¸_new
        break

```

---

## ğŸ’» Complete Implementation

```python
import torch
import torch.nn as nn
import numpy as np

class TRPOAgent:
    """Trust Region Policy Optimization"""
    
    def __init__(self, policy, value_fn, delta=0.01, damping=0.1,
                 cg_iters=10, backtrack_iters=10, backtrack_coef=0.5):
        self.policy = policy
        self.value_fn = value_fn
        self.delta = delta
        self.damping = damping
        self.cg_iters = cg_iters
        self.backtrack_iters = backtrack_iters
        self.backtrack_coef = backtrack_coef
        
        self.value_optimizer = torch.optim.Adam(value_fn.parameters(), lr=1e-3)
    
    def compute_advantages(self, states, rewards, dones, gamma=0.99, lam=0.95):
        """Compute GAE advantages"""
        values = self.value_fn(states).detach().squeeze()
        T = len(rewards)
        advantages = torch.zeros(T)
        gae = 0
        
        for t in reversed(range(T)):
            if t == T - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + gamma * lam * (1 - dones[t]) * gae
            advantages[t] = gae
        
        returns = advantages + values
        return advantages, returns
    
    def surrogate_loss(self, states, actions, advantages, old_log_probs):
        """Compute surrogate objective L(Î¸)"""
        log_probs = self.policy.log_prob(states, actions)
        ratio = torch.exp(log_probs - old_log_probs)
        return (ratio * advantages).mean()
    
    def kl_divergence(self, states, old_policy_params):
        """Compute mean KL divergence"""
        # Save current params
        current_params = self.get_flat_params()
        
        # Get old distribution
        self.set_flat_params(old_policy_params)
        old_dist = self.policy.get_distribution(states)
        
        # Restore current params
        self.set_flat_params(current_params)
        new_dist = self.policy.get_distribution(states)
        
        kl = torch.distributions.kl_divergence(old_dist, new_dist).mean()
        return kl
    
    def hessian_vector_product(self, states, vector, old_params):
        """Compute F @ vector efficiently"""
        self.set_flat_params(old_params)
        kl = self.kl_divergence(states, old_params)
        
        grads = torch.autograd.grad(kl, self.policy.parameters(), create_graph=True)
        flat_grads = torch.cat([g.view(-1) for g in grads])
        
        grad_vector_product = (flat_grads * vector).sum()
        hvp = torch.autograd.grad(grad_vector_product, self.policy.parameters())
        flat_hvp = torch.cat([g.view(-1) for g in hvp])
        
        return flat_hvp + self.damping * vector
    
    def conjugate_gradient(self, states, b, old_params):
        """Solve Fx = b using conjugate gradient"""
        x = torch.zeros_like(b)
        r = b.clone()
        p = r.clone()
        r_dot_r = torch.dot(r, r)
        
        for _ in range(self.cg_iters):
            Ap = self.hessian_vector_product(states, p, old_params)
            alpha = r_dot_r / (torch.dot(p, Ap) + 1e-8)
            x = x + alpha * p
            r = r - alpha * Ap
            new_r_dot_r = torch.dot(r, r)
            beta = new_r_dot_r / (r_dot_r + 1e-8)
            p = r + beta * p
            r_dot_r = new_r_dot_r
            
            if r_dot_r < 1e-10:
                break
        
        return x
    
    def get_flat_params(self):
        """Flatten all policy parameters"""
        return torch.cat([p.view(-1) for p in self.policy.parameters()])
    
    def set_flat_params(self, flat_params):
        """Set policy parameters from flat vector"""
        idx = 0
        for p in self.policy.parameters():
            p.data.copy_(flat_params[idx:idx + p.numel()].view(p.shape))
            idx += p.numel()
    
    def update(self, states, actions, rewards, dones):
        """TRPO update step"""
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        dones = torch.FloatTensor(dones)
        
        # Compute advantages
        advantages, returns = self.compute_advantages(states, rewards, dones)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # Get old log probs
        with torch.no_grad():
            old_log_probs = self.policy.log_prob(states, actions)
        
        old_params = self.get_flat_params().detach()
        
        # Compute policy gradient
        loss = self.surrogate_loss(states, actions, advantages, old_log_probs)
        grads = torch.autograd.grad(loss, self.policy.parameters())
        flat_grads = torch.cat([g.view(-1) for g in grads])
        
        # Compute natural gradient using conjugate gradient
        step_dir = self.conjugate_gradient(states, flat_grads, old_params)
        
        # Compute step size
        sHs = torch.dot(step_dir, self.hessian_vector_product(states, step_dir, old_params))
        max_step_size = torch.sqrt(2 * self.delta / (sHs + 1e-8))
        
        # Line search
        for i in range(self.backtrack_iters):
            step_size = max_step_size * (self.backtrack_coef ** i)
            new_params = old_params + step_size * step_dir
            self.set_flat_params(new_params)
            
            new_loss = self.surrogate_loss(states, actions, advantages, old_log_probs)
            kl = self.kl_divergence(states, old_params)
            
            if kl <= self.delta and new_loss > loss:
                break
        else:
            # Revert to old params if line search failed
            self.set_flat_params(old_params)
        
        # Update value function
        for _ in range(5):
            values = self.value_fn(states).squeeze()
            value_loss = ((values - returns) ** 2).mean()
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
        
        return {
            'policy_loss': loss.item(),
            'value_loss': value_loss.item(),
            'kl': kl.item() if 'kl' in dir() else 0
        }

```

---

## ğŸ“Š TRPO vs PPO Comparison

| Aspect | TRPO | PPO |
|--------|------|-----|
| **Constraint** | Hard KL constraint | Soft clipping |
| **Optimization** | Conjugate gradient | Standard SGD |
| **Computation** | Expensive (Hessian) | Cheap |
| **Stability** | Very stable | Stable |
| **Implementation** | Complex | Simple |
| **Performance** | Similar | Similar |

### Why PPO Replaced TRPO

```
PPO approximates TRPO's trust region with clipping:

L_PPO(Î¸) = E[min(Ï(Î¸)A, clip(Ï(Î¸), 1-Îµ, 1+Îµ)A)]

Benefits:

1. No conjugate gradient needed

2. No Fisher information matrix

3. Works with standard SGD

4. Easier to implement and tune

5. Comparable or better performance

```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | TRPO Paper | [arXiv](https://arxiv.org/abs/1502.05477) |
| ğŸ“„ | Natural Policy Gradient | [Paper](https://papers.nips.cc/paper/2002/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html) |
| ğŸ“– | OpenAI Spinning Up | [Docs](https://spinningup.openai.com/en/latest/algorithms/trpo.html) |
| ğŸ‡¨ğŸ‡³ | TRPOè¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/26308073) |
| ğŸ‡¨ğŸ‡³ | ä¿¡èµ–åŸŸæ–¹æ³•åŸç† | [CSDN](https://blog.csdn.net/qq_30615903/article/details/81275638) |
| ğŸ‡¨ğŸ‡³ | TRPOä¸PPOå¯¹æ¯” | [Bç«™](https://www.bilibili.com/video/BV1cP4y1Y7DN) |

## ğŸ”— Where This Topic Is Used

| Application | TRPO |
|-------------|-----|
| **Robotics** | Safe policy updates |
| **PPO Predecessor** | Led to simpler PPO |
| **Continuous Control** | MuJoCo benchmarks |
| **Safe RL** | Trust region constraints |

---

â¬…ï¸ [Back: PPO](../03_ppo/) | â¡ï¸ [Next: Exploration](../../04_exploration/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
