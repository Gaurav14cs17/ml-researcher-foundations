<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Policy-Based%20Methods&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Value Methods](../02_value_methods/) | â¡ï¸ [Next: Exploration](../04_exploration/)

---

## ğŸ¯ Visual Overview

<img src="./images/ppo-clipping.svg" width="100%">

*Caption: PPO's clipped objective prevents destructive policy updates. For good actions (A>0), it limits how much the policy can increase; for bad actions (A<0), it limits how much it can decrease. This stabilization is why PPO is the default choice for RLHF in ChatGPT and Claude.*

---

## ğŸ“‚ Topics

| Folder | Topic | Key Concepts |
|--------|-------|--------------|
| [01_actor_critic/](./01_actor_critic/) | A2C, A3C | Value + Policy |
| [02_policy_gradient/](./02_policy_gradient/) | REINFORCE | âˆ‡J(Î¸), baseline |
| [03_ppo/](./03_ppo/) | Proximal Policy Optimization | ğŸ”¥ Modern standard |
| [04_trpo/](./04_trpo/) | Trust Region | Natural gradient |

---

## ğŸ“ Mathematical Foundations

### Policy Parameterization

```
Discrete: Ï€_Î¸(a|s) = softmax(fÎ¸(s))_a
Continuous: Ï€_Î¸(a|s) = N(Î¼Î¸(s), ÏƒÎ¸(s)Â²)

```

### Policy Gradient Derivation

```
J(Î¸) = E_Ï€ [Î£â‚œ Î³áµ— râ‚œ]

âˆ‡J(Î¸) = E_Ï€ [Î£â‚œ âˆ‡log Ï€_Î¸(aâ‚œ|sâ‚œ) G_t]

Where G_t = Î£_{k=t}^T Î³^{k-t} r_k (return from t)

```

### Advantage Estimation (GAE)

```
Aâ‚œ^GAE = Î£_{l=0}^âˆ (Î³Î»)^l Î´â‚œâ‚Šâ‚—

Where Î´â‚œ = râ‚œ + Î³V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)

Î» = 0: A = Î´ (high bias, low variance)
Î» = 1: A = G_t - V(s) (low bias, high variance)

```

### PPO Clipped Objective

```
L^CLIP(Î¸) = E [min(râ‚œ(Î¸)Aâ‚œ, clip(râ‚œ(Î¸), 1-Îµ, 1+Îµ)Aâ‚œ)]

Where râ‚œ(Î¸) = Ï€_Î¸(aâ‚œ|sâ‚œ) / Ï€_{Î¸_old}(aâ‚œ|sâ‚œ)

```

---

## ğŸ”‘ Policy Gradient Theorem

```
âˆ‡J(Î¸) = E_Ï€[âˆ‡log Ï€Î¸(a|s) Â· QÏ€(s,a)]
      = E_Ï€[âˆ‡log Ï€Î¸(a|s) Â· AÏ€(s,a)]  # With advantage

Where:
â€¢ J(Î¸) = Expected return
â€¢ Ï€_Î¸(a|s) = Policy parameterized by Î¸
â€¢ A(s,a) = Q(s,a) - V(s) = Advantage

```

---

## ğŸ“Š Comparison

| Method | Variance | Bias | Stability |
|--------|----------|------|-----------|
| REINFORCE | High | None | Low |
| Actor-Critic | Medium | Some | Medium |
| PPO | Low | Some | ğŸ”¥ High |

---

## ğŸ”— Where This Topic Is Used

| Topic | How Policy Methods Are Used |
|-------|----------------------------|
| **RLHF** | PPO optimizes LLM policy |
| **InstructGPT** | PPO + KL penalty |
| **ChatGPT** | PPO for alignment |
| **Claude** | Constitutional AI (policy-based) |
| **LLaMA-2 Chat** | PPO for instruction following |
| **Robotics** | PPO/TRPO for continuous control |
| **Game AI** | A3C for Atari, PPO for games |
| **AlphaGo** | Policy network + MCTS |
| **Autonomous Driving** | Policy gradient for decisions |

### Prerequisite For

```
Policy Methods --> RLHF (PPO is standard)
              --> DPO (derived from policy gradient)
              --> Actor-Critic variants
              --> Multi-agent RL

```

### Methods Used In

| Method | Used By |
|--------|---------|
| **PPO** | ChatGPT, Claude, LLaMA-2, OpenAI Five |
| **TRPO** | Safety-critical robotics |
| **A3C/A2C** | DeepMind Impala, distributed RL |
| **SAC** | Robotics (continuous control) |

### PPO's Role in LLM Training

```
Pretrain (unsupervised) â†’ SFT (supervised) â†’ RLHF (PPO!)
                                               â†‘
                                        Policy gradient
                                        optimizes for
                                        human preference

```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | PPO Paper | [arXiv](https://arxiv.org/abs/1707.06347) |
| ğŸ“„ | TRPO Paper | [arXiv](https://arxiv.org/abs/1502.05477) |
| ğŸ“– | OpenAI Spinning Up | [Docs](https://spinningup.openai.com/) |
| ğŸ‡¨ğŸ‡³ | ç­–ç•¥æ¢¯åº¦æ–¹æ³•è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/26174099) |
| ğŸ‡¨ğŸ‡³ | PPOç®—æ³•è¯¦è§£ | [CSDN](https://blog.csdn.net/qq_30615903/article/details/81275638) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ–¹æ³• | [Bç«™](https://www.bilibili.com/video/BV1sd4y167NS) |

---

â¬…ï¸ [Back: Value Methods](../02_value_methods/) | â¡ï¸ [Next: Exploration](../04_exploration/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
