<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=World%20Models&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Planning](../03_planning/) | â¡ï¸ [Next: Applications](../../06_applications/)

---

## ğŸ¯ Visual Overview

<img src="../images/world-models.svg" width="100%">

*Caption: World models learn to predict future states and rewards given actions. The agent can then "dream" - simulate trajectories in imagination to plan without real environment interaction.*

---

## ğŸ“‚ Overview

World models are learned representations of environment dynamics. They enable sample-efficient RL by allowing agents to learn from imagined experience.

---

## ğŸ”‘ Key Components

| Component | Function |
|-----------|----------|
| **Encoder** | State â†’ latent representation z |
| **Dynamics Model** | Predict next latent: z' = f(z, a) |
| **Reward Model** | Predict reward: rÌ‚ = g(z, a) |
| **Decoder** | Latent â†’ reconstructed state (optional) |

---

## ğŸ“ Mathematical Framework

### World Model Definition

```
A world model consists of:

1. Dynamics Model: p_Î¸(s_{t+1} | s_t, a_t)
   or deterministic: s_{t+1} = f_Î¸(s_t, a_t)

2. Reward Model: r_t = R_Ï†(s_t, a_t)

3. (Optional) Encoder: z_t = E_Ïˆ(s_t)
   Maps observations to latent states

4. (Optional) Decoder: Å_t = D_Ï‰(z_t)
   Reconstructs observations
```

### Latent Space Dynamics

```
For high-dimensional observations (images):

Encode: z_t = E_Ïˆ(s_t)
Dynamics: z_{t+1} = f_Î¸(z_t, a_t)  (in latent space!)
Reward: r_t = R_Ï†(z_t, a_t)
Decode: Å_t = D_Ï‰(z_t)  (for visualization)

Benefits:
  - Lower-dimensional (faster planning)
  - Captures relevant features
  - Ignores irrelevant details
```

---

## ğŸ“ Variational World Model (VAE-based)

### ELBO Objective

```
Maximize Evidence Lower Bound:

L(Î¸,Ïˆ,Ï†) = E_{z~q_Ïˆ(z|s)} [log p_Ï‰(s|z)]     (reconstruction)
         - Î² D_KL(q_Ïˆ(z|s) || p(z))           (regularization)
         + E_{z~q_Ïˆ} [log p_Ï†(r|z,a)]         (reward prediction)
         + E_{z~q_Ïˆ} [log p_Î¸(z'|z,a)]        (dynamics)

Where:
  q_Ïˆ(z|s) = encoder (posterior)
  p(z) = prior (typically N(0,I))
  p_Î¸(z'|z,a) = dynamics model
```

### Reparameterization Trick

```
For Gaussian latents:
  q_Ïˆ(z|s) = N(Î¼_Ïˆ(s), Ïƒ_Ïˆ(s)Â²)

Sample via:
  z = Î¼_Ïˆ(s) + Ïƒ_Ïˆ(s) âŠ™ Îµ,  where Îµ ~ N(0, I)

This allows gradient flow: âˆ‚L/âˆ‚Ïˆ well-defined
```

---

## ğŸ“ Recurrent World Models

### RNN/GRU Dynamics

```
Hidden state captures history:
  h_t = RNN(h_{t-1}, z_{t-1}, a_{t-1})

Stochastic latent:
  z_t ~ p_Î¸(z|h_t)

Combined state: (h_t, z_t)
  - h_t: Deterministic, long-term memory
  - z_t: Stochastic, captures uncertainty
```

### RSSM Loss (Dreamer-style)

```
L = E_t [ -log p(x_t|z_t,h_t)         (reconstruction)
        - log p(r_t|z_t,h_t)          (reward)
        + Î² D_KL(q(z_t|h_t,x_t) || p(z_t|h_t))  (KL)
       ]

Where:
  p(z_t|h_t) = prior (prediction before observation)
  q(z_t|h_t,x_t) = posterior (after seeing observation)
```

---

## ğŸ“ Model Uncertainty

### Epistemic vs Aleatoric Uncertainty

```
Total uncertainty = Epistemic + Aleatoric

Epistemic (model uncertainty):
  - Due to limited data
  - Reducible with more data
  - Model with ensemble: Var[f_1(s,a), ..., f_M(s,a)]

Aleatoric (environment stochasticity):
  - Inherent randomness
  - Irreducible
  - Model with distribution: p(s'|s,a)
```

### Ensemble World Models

```
Train M models: {f_Î¸â‚, ..., f_Î¸_M}

Prediction:
  Mean: Î¼(s,a) = (1/M) Î£áµ¢ f_Î¸áµ¢(s,a)
  Variance: ÏƒÂ²(s,a) = (1/M) Î£áµ¢ (f_Î¸áµ¢(s,a) - Î¼)Â²

Use uncertainty for:
  - Exploration bonus: r_i âˆ Ïƒ(s,a)
  - Conservative planning: penalize high variance
```

---

## ğŸ“ Model-Based Policy Optimization

### Dyna-style Learning

```
Algorithm:
  1. Act in real environment, collect (s,a,r,s')
  2. Add to replay buffer D
  3. Train world model on D
  4. Generate synthetic experience with model
  5. Train policy on real + synthetic data
  6. Repeat

Ratio of real:synthetic typically 1:10 or higher
```

### Analytic Policy Gradient (Dreamer)

```
For differentiable world model:

âˆ‡J(Ï€) = E_Ï„~model [Î£_t âˆ‡_Ï€ log Ï€(a_t|s_t) Â· âˆ‚V/âˆ‚a_t]

Can backprop through imagined trajectory:
  s_{t+1} = f_Î¸(s_t, a_t)  â† differentiable
  âˆ‚s_{t+1}/âˆ‚a_t = âˆ‚f_Î¸/âˆ‚a_t

This is more efficient than REINFORCE!
```

---

## ğŸŒ Applications

| Model | Key Innovation |
|-------|----------------|
| **World Models (Ha 2018)** | VAE + RNN for racing |
| **SimPLe** | Model-based Atari |
| **Dreamer** | RSSM, backprop through model |
| **MuZero** | Value prediction, no reconstruction |

---

## ğŸ’» Code

```python
class WorldModel(nn.Module):
    def __init__(self, state_dim, action_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(state_dim, 256), nn.ReLU(),
            nn.Linear(256, latent_dim * 2)  # mean + logvar
        )
        self.dynamics = nn.GRU(latent_dim + action_dim, 256)
        self.reward_head = nn.Linear(256, 1)
        
    def imagine(self, z, actions, horizon):
        """Rollout in imagination"""
        imagined = []
        h = self.dynamics.init_hidden(z.size(0))
        for t in range(horizon):
            z_a = torch.cat([z, actions[:, t]], dim=-1)
            h = self.dynamics(z_a, h)
            z = self.transition(h)
            r = self.reward_head(h)
            imagined.append((z, r))
        return imagined
```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | World Models Paper | [arXiv](https://arxiv.org/abs/1803.10122) |
| ğŸ“„ | Dreamer Paper | [arXiv](https://arxiv.org/abs/1912.01603) |
| ğŸ“„ | MuZero Paper | [Nature](https://www.nature.com/articles/s41586-020-03051-4) |
| ğŸ‡¨ğŸ‡³ | ä¸–ç•Œæ¨¡å‹è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/563656219) |
| ğŸ‡¨ğŸ‡³ | Dreamerç³»åˆ— | [CSDN](https://blog.csdn.net/qq_37006625/article/details/123629543) |
| ğŸ‡¨ğŸ‡³ | æ¨¡å‹åŸºRL | [Bç«™](https://www.bilibili.com/video/BV1C34y1H7Eq) |

## ğŸ”— Where This Topic Is Used

| Application | World Models |
|-------------|-------------|
| **Dreamer** | Learning in imagination |
| **MuZero** | Learned dynamics model |
| **Planning** | Model predictive control |
| **Sim-to-Real** | Domain randomization |

---

â¬…ï¸ [Back: Planning](../03_planning/) | â¡ï¸ [Next: Applications](../../06_applications/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
