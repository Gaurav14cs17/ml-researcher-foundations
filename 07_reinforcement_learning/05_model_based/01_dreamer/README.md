<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Dreamer&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Model-Based](../) | â¡ï¸ [Next: MCTS](../02_mcts/)

---

## ğŸ¯ Visual Overview

<img src="./images/dreamer.svg" width="100%">

*Caption: Dreamer learns a world model from real experience, then trains an actor-critic policy purely from imagined trajectories. The RSSM provides a compact latent state representation.*

---

## ğŸ“‚ Overview

Dreamer is a state-of-the-art model-based RL algorithm that achieves near model-free performance with ~100x less data by learning from imagined experience.

---

## ğŸ“ Mathematical Framework

### World Model Components

```
World Model = (Encoder, RSSM, Reward, Decoder)

Encoder:   q(z_t | h_t, x_t)      Image â†’ latent
RSSM:      p(z_t | h_t)           Dynamics prediction
           h_t = f(h_{t-1}, z_{t-1}, a_{t-1})
Reward:    p(r_t | h_t, z_t)      Latent â†’ reward
Decoder:   p(x_t | h_t, z_t)      Latent â†’ image
```

### Training Objective (ELBO)

```
max E[Î£_t (log p(x_t | z_t, h_t) + log p(r_t | z_t, h_t) 
        - Î² KL(q(z_t | h_t, x_t) || p(z_t | h_t)))]
        
Terms:
    log p(x_t | ...) : Reconstruction (image prediction)
    log p(r_t | ...) : Reward prediction
    KL(...) : Regularization (match prior)
```

---

## ğŸ“ Key Components

| Component | Function |
|-----------|----------|
| **RSSM** | Recurrent State-Space Model for dynamics |
| **Encoder** | Image â†’ latent state |
| **Decoder** | Latent â†’ reconstructed image |
| **Actor** | Ï€(a\|z) policy in latent space |
| **Critic** | V(z) value function |

---

## ğŸ”‘ RSSM (Recurrent State-Space Model)

```
State = (h, z) where:
- h âˆˆ â„^d: Deterministic recurrent state (GRU hidden)
- z âˆˆ â„^k: Stochastic latent state (sampled)

Transitions:
h_t = GRU(h_{t-1}, z_{t-1}, a_{t-1})    # Deterministic
z_t ~ p(z_t | h_t)                       # Stochastic prior
z_t ~ q(z_t | h_t, x_t)                  # Posterior (training)

Why both?
- Deterministic: Captures long-term dependencies
- Stochastic: Models environment uncertainty
```

### RSSM Architecture

```
                    x_t (observation)
                      |
                      v
    +--------------------------------+
    |           Encoder              |
    |    q(z_t | h_t, x_t)          |
    +--------------------------------+
                      |
                      v
    +---------------------------------------------+
    |                RSSM Core                     |
    |  +---------+      +-----------------+       |
    |  |   GRU   |------|  Stochastic z   |       |
    |  |  (h_t)  |      |  p(z_t | h_t)   |       |
    |  +---------+      +-----------------+       |
    |       ^                   |                 |
    |       |      â–¼      |
    |  [h_{t-1}, z_{t-1}, a_{t-1}]   State (h_t, z_t)  |
    +---------------------------------------------+
```

---

## ğŸš€ Training Loop

```
1. Collect experience in real environment (small amount)
2. Train world model on real data
3. Imagine trajectories starting from real states
4. Train actor-critic on imagined trajectories
5. Repeat

Key insight: Backprop through differentiable world model!
```

### Imagination Process

```
Given initial state (h_0, z_0) from real data:

For t = 1 to H (imagination horizon):
    a_t = Ï€(h_t, z_t)           # Actor samples action
    h_{t+1} = GRU(h_t, z_t, a_t) # Deterministic transition
    z_{t+1} ~ p(z | h_{t+1})     # Stochastic transition
    r_t = reward_model(h_t, z_t) # Predicted reward

Gradients flow through the entire imagined trajectory!
```

---

## ğŸ“ Policy Learning (Actor-Critic)

### Value Function (Î»-returns)

```
V_Î»(z_t) = (1-Î») Î£_{n=1}^{H-t} Î»^{n-1} V_n(z_t) + Î»^{H-t} V_H(z_t)

Where V_n is n-step return:
V_n(z_t) = E[r_t + Î³r_{t+1} + ... + Î³^{n-1}r_{t+n-1} + Î³^n V(z_{t+n})]
```

### Actor Objective

```
max E[Î£_t (V_Î»(z_t) - Î· H[Ï€(Â·|z_t)])]

With entropy regularization for exploration
```

---

## ğŸ’» Code Implementation

### World Model

```python
import torch
import torch.nn as nn
import torch.distributions as D

class RSSM(nn.Module):
    """Recurrent State-Space Model"""
    
    def __init__(self, obs_dim, action_dim, hidden_dim=200, stoch_dim=30):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.stoch_dim = stoch_dim
        
        # Encoder: observation â†’ posterior
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, 200),
            nn.ELU(),
            nn.Linear(200, 2 * stoch_dim)  # mean, std
        )
        
        # Recurrent model
        self.gru = nn.GRUCell(stoch_dim + action_dim, hidden_dim)
        
        # Prior: h â†’ z distribution
        self.prior = nn.Sequential(
            nn.Linear(hidden_dim, 200),
            nn.ELU(),
            nn.Linear(200, 2 * stoch_dim)
        )
        
        # Decoder, reward predictor
        self.decoder = nn.Linear(hidden_dim + stoch_dim, obs_dim)
        self.reward_head = nn.Linear(hidden_dim + stoch_dim, 1)
    
    def get_dist(self, stats):
        mean, std = stats.chunk(2, dim=-1)
        std = nn.functional.softplus(std) + 0.1
        return D.Normal(mean, std)
    
    def observe(self, obs, action, h_prev, z_prev):
        """One step with observation (training)"""

        # Deterministic transition
        h = self.gru(torch.cat([z_prev, action], -1), h_prev)
        
        # Posterior (uses observation)
        posterior_stats = self.encoder(obs) + self.prior(h)
        posterior = self.get_dist(posterior_stats)
        z = posterior.rsample()
        
        # Prior (for KL loss)
        prior_stats = self.prior(h)
        prior = self.get_dist(prior_stats)
        
        return h, z, prior, posterior
    
    def imagine(self, action, h_prev, z_prev):
        """One step without observation (imagination)"""
        h = self.gru(torch.cat([z_prev, action], -1), h_prev)
        prior = self.get_dist(self.prior(h))
        z = prior.rsample()
        return h, z
    
    def reward(self, h, z):
        return self.reward_head(torch.cat([h, z], -1))

class DreamerAgent:
    def __init__(self, obs_dim, action_dim):
        self.world_model = RSSM(obs_dim, action_dim)
        self.actor = Actor(obs_dim=230, action_dim=action_dim)
        self.critic = Critic(obs_dim=230)
        
        self.world_opt = torch.optim.Adam(self.world_model.parameters(), lr=6e-4)
        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=8e-5)
        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=8e-5)
    
    def imagine_trajectories(self, initial_h, initial_z, horizon=15):
        """Dream about the future"""
        h, z = initial_h, initial_z
        imagined = []
        
        for t in range(horizon):
            state = torch.cat([h, z], -1)
            action = self.actor(state).sample()
            h, z = self.world_model.imagine(action, h, z)
            reward = self.world_model.reward(h, z)
            imagined.append((h, z, action, reward))
        
        return imagined
    
    def update_world_model(self, batch):
        """Train world model on real experience"""
        obs, actions, rewards = batch
        T, B = obs.shape[:2]
        
        h = torch.zeros(B, self.world_model.hidden_dim)
        z = torch.zeros(B, self.world_model.stoch_dim)
        
        total_loss = 0
        for t in range(T-1):
            h, z, prior, posterior = self.world_model.observe(
                obs[t], actions[t], h, z
            )
            
            # Reconstruction loss
            recon = self.world_model.decoder(torch.cat([h, z], -1))
            recon_loss = ((recon - obs[t+1]) ** 2).mean()
            
            # Reward loss
            pred_reward = self.world_model.reward(h, z)
            reward_loss = ((pred_reward - rewards[t]) ** 2).mean()
            
            # KL loss
            kl_loss = D.kl_divergence(posterior, prior).mean()
            
            total_loss += recon_loss + reward_loss + 0.1 * kl_loss
        
        self.world_opt.zero_grad()
        total_loss.backward()
        self.world_opt.step()
        
        return total_loss.item()
```

---

## ğŸ“Š Dreamer Versions

| Version | Key Improvement | Paper |
|---------|----------------|-------|
| **Dreamer v1** | Basic RSSM + Î»-returns | Hafner 2019 |
| **Dreamer v2** | Discrete latents, actor-critic fixes | Hafner 2020 |
| **Dreamer v3** | Symlog predictions, world model scaling | Hafner 2023 |

---

## ğŸ”— Connection to Other Methods

```
Model-Based RL
    |
    +-- Dyna (simple model + planning)
    +-- World Models (VAE + RNN)
    +-- MuZero (learned model + MCTS)
    +-- Dreamer (RSSM + imagination)
            |
            +-- DreamerV2 (discrete latents)
            +-- DreamerV3 (scaling + symlog)
```

---

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | DreamerV1 | [arXiv](https://arxiv.org/abs/1912.01603) |
| ğŸ“„ | DreamerV2 | [arXiv](https://arxiv.org/abs/2010.02193) |
| ğŸ“„ | DreamerV3 | [arXiv](https://arxiv.org/abs/2301.04104) |
| ğŸ’» | Official Code | [GitHub](https://github.com/danijar/dreamer) |
| ğŸ‡¨ğŸ‡³ | Dreamerç³»åˆ—è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/563656219) |
| ğŸ‡¨ğŸ‡³ | ä¸–ç•Œæ¨¡å‹ä¸Dreamer | [CSDN](https://blog.csdn.net/qq_37006625/article/details/123629543) |
| ğŸ‡¨ğŸ‡³ | æ¨¡å‹åŸºRL-Dreamer | [Bç«™](https://www.bilibili.com/video/BV1C34y1H7Eq) |
| ğŸ‡¨ğŸ‡³ | DreamerV3è§£è¯» | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2023-01-13-5) |
| ğŸ‡¨ğŸ‡³ | åŸºäºæƒ³è±¡çš„å¼ºåŒ–å­¦ä¹  | [PaperWeekly](https://www.paperweekly.site/papers/notes/2568)

## ğŸ”— Where This Topic Is Used

| Application | Dreamer |
|-------------|--------|
| **Sample Efficiency** | Learn from imagination |
| **Visual RL** | Latent dynamics |
| **Continuous Control** | DMC benchmarks |
| **World Models** | State-of-the-art |

---

â¬…ï¸ [Back: Model-Based](../) | â¡ï¸ [Next: MCTS](../02_mcts/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
