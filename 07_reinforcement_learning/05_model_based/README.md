<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Model-Based%20RL&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Exploration](../04_exploration/) | â¡ï¸ [Next: Applications](../06_applications/)

---

## ğŸ¯ Visual Overview

<img src="./images/world-models.svg" width="100%">

*Caption: Model-based RL learns a world model (dynamics p(s'|s,a) and reward r(s,a)) from real experience, then plans or trains in imagination. This is how AlphaGo, MuZero, and Dreamer achieve sample efficiency.*

---

## ğŸ“ Mathematical Foundations

### World Model

```
Learn: pÌ‚(s'|s,a) and rÌ‚(s,a)

From data: D = {(sâ‚œ, aâ‚œ, râ‚œ, sâ‚œâ‚Šâ‚)}

Minimize: L = E[||Å' - s'||Â² + (rÌ‚ - r)Â²]

```

### Model Predictive Control (MPC)

```
At each step:

1. Plan: Ï€â‚œ = argmax E[Î£áµ¢â‚Œâ‚€á´´ Î³â± rÌ‚(sâ‚œâ‚Šáµ¢, aâ‚œâ‚Šáµ¢)]
   using model pÌ‚(s'|s,a)

2. Execute: Take action aâ‚œ

3. Re-plan with new observation

```

### MCTS (UCT formula)

```
UCT score = Q(s,a)/N(s,a) + câˆš(ln N(s)/N(s,a))

Selection â†’ Expansion â†’ Simulation â†’ Backpropagation

```

### Dreamer (RSSM)

```
Recurrent State-Space Model:
hâ‚œ = f(hâ‚œâ‚‹â‚, zâ‚œâ‚‹â‚, aâ‚œâ‚‹â‚)  (deterministic)
zâ‚œ ~ q(zâ‚œ|hâ‚œ, oâ‚œ)          (stochastic)

Learn in imagination:
Roll out latent trajectories, train actor-critic

```

---

## ğŸ“‚ Topics

| Folder | Topic | Key Idea |
|--------|-------|----------|
| [01_dreamer/](./01_dreamer/) | Imagination | Dream to learn |
| [02_mcts/](./02_mcts/) | Monte Carlo Tree Search | AlphaGo |
| [03_planning/](./03_planning/) | Use model to plan | MPC |
| [04_world_models/](./04_world_models/) | Learn dynamics | p(s'\|s,a) |

---

## ğŸ¯ Key Idea

```
Model-Free: Learn policy directly from experience
Model-Based: Learn model, then plan/imagine

Advantages:
â€¢ Sample efficient (fewer environment interactions)
â€¢ Can plan ahead
â€¢ Transfer to new tasks

Disadvantages:
â€¢ Model errors compound
â€¢ More computation

```

---

## ğŸ”¥ Notable Systems

| System | Method | Achievement |
|--------|--------|-------------|
| **AlphaGo** | MCTS + NN | Beat world champion |
| **MuZero** | Learned model + MCTS | Master multiple games |
| **Dreamer** | World model + imagination | Efficient learning |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | Dreamer Paper | [arXiv](https://arxiv.org/abs/1912.01603) |
| ğŸ“„ | MuZero Paper | [Nature](https://www.nature.com/articles/s41586-020-03051-4) |
| ğŸ“„ | World Models | [arXiv](https://arxiv.org/abs/1803.10122) |
| ğŸ‡¨ğŸ‡³ | æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹  | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/563656219) |
| ğŸ‡¨ğŸ‡³ | Dreamerè¯¦è§£ | [CSDN](https://blog.csdn.net/qq_37006625/article/details/123629543) |
| ğŸ‡¨ğŸ‡³ | AlphaGoåŸç† | [Bç«™](https://www.bilibili.com/video/BV1C34y1H7Eq) |

## ğŸ”— Where This Topic Is Used

| Application | Model-Based RL |
|-------------|---------------|
| **Sample Efficiency** | Fewer real interactions |
| **Robotics** | Safe exploration |
| **Games** | AlphaZero, MuZero |
| **Planning** | MPC, trajectory opt |

---

â¬…ï¸ [Back: Exploration](../04_exploration/) | â¡ï¸ [Next: Applications](../06_applications/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
