<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Reinforcement%20Learning%20Foundations&fontSize=28&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## üîó Navigation

‚¨ÖÔ∏è [Back: Reinforcement Learning](../) | ‚û°Ô∏è [Next: MDP](./01_mdp/)

---

## üìê Core Concepts

### The RL Framework

Reinforcement Learning studies how agents learn to make sequential decisions through interaction with an environment to maximize cumulative reward.

```
Agent-Environment Interaction Loop:

    +--------+    action a_t    +-----------+
    |        | ---------------> |           |
    |  Agent |                  | Environment |
    |        | <--------------- |           |
    +--------+   state s_t      +-----------+
                 reward r_t

```

### Formal Definition

```
At each timestep t:

1. Agent observes state s_t ‚àà S

2. Agent selects action a_t ‚àà A according to policy œÄ(a|s)

3. Environment transitions: s_{t+1} ~ P(¬∑|s_t, a_t)

4. Agent receives reward: r_t = R(s_t, a_t, s_{t+1})

5. Repeat until termination

Goal: Find policy œÄ* that maximizes expected cumulative reward
      œÄ* = argmax_œÄ E_œÄ[Œ£_{t=0}^‚àû Œ≥^t r_t]

```

---

## üìê Mathematical Framework

### Key Elements

| Symbol | Name | Definition |
|--------|------|------------|
| S | State space | Set of all possible states |
| A | Action space | Set of all possible actions |
| P(s'\|s,a) | Transition dynamics | Probability of next state |
| R(s,a,s') | Reward function | Immediate feedback signal |
| Œ≥ ‚àà [0,1] | Discount factor | Present value of future rewards |
| œÄ(a\|s) | Policy | Probability of action given state |

### The Markov Property

```
The Markov property states that the future is independent of the past 
given the present:

P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)

This is the key assumption that makes RL tractable!

```

### Value Functions

```
State-Value Function:
V^œÄ(s) = E_œÄ[Œ£_{t=0}^‚àû Œ≥^t r_t | s_0 = s]
       = E_œÄ[r_0 + Œ≥r_1 + Œ≥¬≤r_2 + ... | s_0 = s]

"Expected total discounted reward starting from state s, following policy œÄ"

Action-Value Function:
Q^œÄ(s,a) = E_œÄ[Œ£_{t=0}^‚àû Œ≥^t r_t | s_0 = s, a_0 = a]

"Expected total discounted reward starting from (s,a), then following œÄ"

Relationship:
V^œÄ(s) = Œ£_a œÄ(a|s) Q^œÄ(s,a)
Q^œÄ(s,a) = R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V^œÄ(s')

```

### Bellman Equations

```
Bellman Expectation Equations:

V^œÄ(s) = Œ£_a œÄ(a|s) [R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V^œÄ(s')]

Q^œÄ(s,a) = R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) Œ£_{a'} œÄ(a'|s') Q^œÄ(s',a')

Bellman Optimality Equations:

V*(s) = max_a [R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V*(s')]

Q*(s,a) = R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) max_{a'} Q*(s',a')

Optimal Policy:
œÄ*(s) = argmax_a Q*(s,a)

```

---

## üìê Key Theorems

### Theorem 1: Policy Improvement

```
For any policy œÄ, if we define œÄ' as:

œÄ'(s) = argmax_a Q^œÄ(s,a)

Then: V^{œÄ'}(s) ‚â• V^œÄ(s) for all s

Proof sketch:
V^œÄ(s) ‚â§ max_a Q^œÄ(s,a)           (by definition of œÄ')
       = Q^œÄ(s, œÄ'(s))            (œÄ' picks the argmax)
       = R(s,œÄ'(s)) + Œ≥ Œ£_{s'} P(s'|s,œÄ'(s)) V^œÄ(s')
       ‚â§ R(s,œÄ'(s)) + Œ≥ Œ£_{s'} P(s'|s,œÄ'(s)) max_{a'} Q^œÄ(s',a')
       = ... (repeat recursively)
       = V^{œÄ'}(s)

```

### Theorem 2: Contraction Property

```
The Bellman operator T is a Œ≥-contraction:

||T Q_1 - T Q_2||_‚àû ‚â§ Œ≥ ||Q_1 - Q_2||_‚àû

This guarantees:

1. Value iteration converges

2. Q* is the unique fixed point

3. Convergence rate is O(Œ≥^n)

```

### Theorem 3: Policy Gradient

```
‚àá_Œ∏ J(Œ∏) = E_{œÑ~œÄ_Œ∏} [Œ£_t ‚àá_Œ∏ log œÄ_Œ∏(a_t|s_t) ¬∑ Q^{œÄ_Œ∏}(s_t, a_t)]

This allows gradient-based optimization of policies!

```

---

## üìä RL Taxonomy

```
RL Methods

|
+-- Value-Based
|   +-- Dynamic Programming (known model)
|   |   +-- Policy Iteration

|   |   +-- Value Iteration
|   |
|   +-- Model-Free

|       +-- Monte Carlo
|       +-- TD Learning (SARSA)
|       +-- Q-Learning ‚Üí DQN ‚Üí Rainbow

|
+-- Policy-Based
|   +-- REINFORCE
|   +-- Actor-Critic (A2C, A3C)

|   +-- PPO, TRPO ‚Üí RLHF for LLMs
|
+-- Model-Based
    +-- Dyna
    +-- MCTS (AlphaGo)
    +-- World Models (Dreamer)

```

---

## üíª Code: Basic RL Framework

```python
import numpy as np
from abc import ABC, abstractmethod

class Environment(ABC):
    """Abstract RL Environment"""
    
    @abstractmethod
    def reset(self) -> np.ndarray:
        """Reset and return initial state"""
        pass
    
    @abstractmethod
    def step(self, action: int) -> tuple:
        """Execute action, return (next_state, reward, done, info)"""
        pass

class Agent(ABC):
    """Abstract RL Agent"""
    
    @abstractmethod
    def select_action(self, state: np.ndarray) -> int:
        """Select action given current state"""
        pass
    
    @abstractmethod
    def update(self, state, action, reward, next_state, done):
        """Update agent from experience"""
        pass

def train(env: Environment, agent: Agent, n_episodes: int) -> list:
    """Standard RL training loop"""
    rewards_history = []
    
    for episode in range(n_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            # Agent selects action
            action = agent.select_action(state)
            
            # Environment responds
            next_state, reward, done, info = env.step(action)
            
            # Agent learns
            agent.update(state, action, reward, next_state, done)
            
            total_reward += reward
            state = next_state
        
        rewards_history.append(total_reward)
    
    return rewards_history

# Example: Value Function Computation
def compute_value_function(env, policy, gamma=0.99, theta=1e-6):
    """
    Policy Evaluation: Compute V^œÄ using iterative Bellman updates
    
    V(s) ‚Üê Œ£_a œÄ(a|s) [R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V(s')]
    """
    V = np.zeros(env.n_states)
    
    while True:
        delta = 0
        for s in range(env.n_states):
            v = V[s]
            V[s] = sum(
                policy[s, a] * sum(
                    env.P[s, a, s_next] * (env.R[s, a, s_next] + gamma * V[s_next])
                    for s_next in range(env.n_states)
                )
                for a in range(env.n_actions)
            )
            delta = max(delta, abs(v - V[s]))
        
        if delta < theta:
            break
    
    return V

```

---

## üìö References

| Type | Title | Link |
|------|-------|------|
| üìñ | Sutton & Barto (2018) | [RL: An Introduction](http://incompleteideas.net/book/) |
| üé• | David Silver Lectures | [YouTube](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) |
| üéì | Berkeley CS285 | [Deep RL Course](http://rail.eecs.berkeley.edu/deeprlcourse/) |
| üá®üá≥ | Âº∫ÂåñÂ≠¶‰π†ÂÖ•Èó® | [Áü•‰πé](https://zhuanlan.zhihu.com/p/25498081) |
| üá®üá≥ | RLÂü∫Á°ÄÊïôÁ®ã | [BÁ´ô](https://www.bilibili.com/video/BV1sd4y167NS) |

---

## üîó Navigation

‚¨ÖÔ∏è [Back: Reinforcement Learning](../) | ‚û°Ô∏è [Next: MDP](../01_mdp/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
