<?xml version="1.0" ?>
<ns0:svg xmlns:ns0="http://www.w3.org/2000/svg" viewBox="0.0 0.0 2250.0 1912.5">
  <ns0:defs>
    <ns0:marker id="arrowBlue2" markerWidth="8" markerHeight="8" refX="7" refY="3" orient="auto">
      <ns0:path d="M0,0 L0,6 L7,3 z" fill="#1565c0"/>
    </ns0:marker>
    <ns0:marker id="arrowPurple" markerWidth="8" markerHeight="8" refX="7" refY="3" orient="auto">
      <ns0:path d="M0,0 L0,6 L7,3 z" fill="#6a1b9a"/>
    </ns0:marker>
  </ns0:defs>
  <ns0:text x="500" y="30" font-size="22" font-weight="bold" text-anchor="middle" fill="#1a237e">
    Bellman Equations: Complete Framework
  </ns0:text>
  <ns0:g transform="translate(50, 70)">
    <ns0:rect x="0" y="0" width="2025.0" height="225.0" fill="#f8f9fa" stroke="#28a745" stroke-width="2" rx="8"/>
    <ns0:text x="450" y="55.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#145a32">
      Markov Decision Process (MDP): (S, A, P, R, Œ≥)
    </ns0:text>
    <ns0:g transform="translate(20, 45)">
      <ns0:text x="0" y="80.0" font-size="13" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#2e7d32">S:</ns0:tspan>
         State space
      </ns0:text>
      <ns0:text x="170" y="105.0" font-size="13" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#1565c0">A:</ns0:tspan>
         Action space
      </ns0:text>
      <ns0:text x="340" y="130.0" font-size="13" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#6a1b9a">P:</ns0:tspan>
         Transition P(s'|s,a)
      </ns0:text>
      <ns0:text x="0" y="155.0" font-size="13" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#c62828">R:</ns0:tspan>
         Reward R(s,a,s')
      </ns0:text>
      <ns0:text x="170" y="180.0" font-size="13" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#e65100">Œ≥:</ns0:tspan>
         Discount ‚àà [0,1)
      </ns0:text>
      <ns0:text x="340" y="205.0" font-size="13" fill="#1a237e">Goal: Find optimal policy œÄ*</ns0:text>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(50, 200)">
    <ns0:rect x="0" y="0" width="435" height="220" fill="#1a237e" stroke="#007bff" stroke-width="3" rx="8"/>
    <ns0:text x="217" y="230.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#1565c0">
      Bellman Expectation Equations
    </ns0:text>
    <ns0:text x="217" y="255.0" font-size="12" text-anchor="middle" fill="#1a237e">
      (For policy evaluation)
    </ns0:text>
    <ns0:g transform="translate(15, 70)">
      <ns0:text x="0" y="280.0" font-size="14" font-weight="bold" fill="#1565c0">State Value Function V^œÄ(s):</ns0:text>
      <ns0:rect x="0" y="10" width="405" height="50" fill="#1a237e" stroke="#007bff" stroke-width="1" rx="3"/>
      <ns0:text x="202" y="305.0" font-size="13" font-family="Arial, sans-serif" text-anchor="middle" fill="#1a5276">
        V^œÄ(s) = Œ£_a œÄ(a|s) [R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V^œÄ(s')]
      </ns0:text>
      <ns0:text x="202" y="330.0" font-size="11" text-anchor="middle" fill="#1565c0">
        Expected return starting from state s
      </ns0:text>
    </ns0:g>
    <ns0:g transform="translate(15, 135)">
      <ns0:text x="0" y="355.0" font-size="14" font-weight="bold" fill="#1565c0">Action Value Function Q^œÄ(s,a):</ns0:text>
      <ns0:rect x="0" y="10" width="405" height="50" fill="#1a237e" stroke="#007bff" stroke-width="1" rx="3"/>
      <ns0:text x="202" y="380.0" font-size="13" font-family="Arial, sans-serif" text-anchor="middle" fill="#1a5276">
        Q^œÄ(s,a) = R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V^œÄ(s')
      </ns0:text>
      <ns0:text x="202" y="405.0" font-size="11" text-anchor="middle" fill="#1565c0">
        Expected return from (s,a) then following œÄ
      </ns0:text>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(515, 200)">
    <ns0:rect x="0" y="0" width="435" height="220" fill="#1a237e" stroke="#fd7e14" stroke-width="3" rx="8"/>
    <ns0:text x="217" y="430.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#d35400">
      Bellman Optimality Equations
    </ns0:text>
    <ns0:text x="217" y="455.0" font-size="12" text-anchor="middle" fill="#1a237e">
      (For finding optimal policy)
    </ns0:text>
    <ns0:g transform="translate(15, 70)">
      <ns0:text x="0" y="480.0" font-size="14" font-weight="bold" fill="#e67e22">Optimal State Value V*(s):</ns0:text>
      <ns0:rect x="0" y="10" width="405" height="50" fill="#fdebd0" stroke="#e67e22" stroke-width="1" rx="3"/>
      <ns0:text x="202" y="505.0" font-size="13" font-family="Arial, sans-serif" text-anchor="middle" fill="#d35400">
        V*(s) = max_a [R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V*(s')]
      </ns0:text>
      <ns0:text x="202" y="530.0" font-size="11" text-anchor="middle" fill="#e67e22">
        Maximum return achievable from state s
      </ns0:text>
    </ns0:g>
    <ns0:g transform="translate(15, 135)">
      <ns0:text x="0" y="555.0" font-size="14" font-weight="bold" fill="#e67e22">Optimal Action Value Q*(s,a):</ns0:text>
      <ns0:rect x="0" y="10" width="405" height="50" fill="#fdebd0" stroke="#e67e22" stroke-width="1" rx="3"/>
      <ns0:text x="202" y="580.0" font-size="13" font-family="Arial, sans-serif" text-anchor="middle" fill="#d35400">
        Q*(s,a) = R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) max_{a'} Q*(s',a')
      </ns0:text>
      <ns0:text x="202" y="605.0" font-size="11" text-anchor="middle" fill="#e67e22">
        Maximum return from (s,a) then acting optimally
      </ns0:text>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(50, 450)">
    <ns0:text x="450" y="630.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#1a237e">
      Backup Diagram: How Bellman Works
    </ns0:text>
    <ns0:rect x="0" y="20" width="2025.0" height="180" fill="#1a237e" stroke="#e0e0e0" stroke-width="2" rx="8"/>
    <ns0:circle cx="450" cy="70" r="30" fill="#2e7d32" stroke="#2e7d32" stroke-width="3"/>
    <ns0:text x="450" y="655.0" font-size="14" font-weight="bold" text-anchor="middle" fill="#1a237e">s</ns0:text>
    <ns0:circle cx="300" cy="150" r="20" fill="#1565c0" stroke="#007bff" stroke-width="2"/>
    <ns0:text x="300" y="680.0" font-size="12" font-weight="bold" text-anchor="middle" fill="#1a237e">a‚ÇÅ</ns0:text>
    <ns0:circle cx="450" cy="150" r="20" fill="#1565c0" stroke="#007bff" stroke-width="2"/>
    <ns0:text x="450" y="705.0" font-size="12" font-weight="bold" text-anchor="middle" fill="#1a237e">a‚ÇÇ</ns0:text>
    <ns0:circle cx="600" cy="150" r="20" fill="#1565c0" stroke="#007bff" stroke-width="2"/>
    <ns0:text x="600" y="730.0" font-size="12" font-weight="bold" text-anchor="middle" fill="#1a237e">a‚ÇÉ</ns0:text>
    <ns0:circle cx="250" cy="180" r="15" fill="#6a1b9a" stroke="#6c3483" stroke-width="2"/>
    <ns0:text x="250" y="755.0" font-size="10" text-anchor="middle" fill="#1a237e">s'‚ÇÅ</ns0:text>
    <ns0:circle cx="350" cy="180" r="15" fill="#6a1b9a" stroke="#6c3483" stroke-width="2"/>
    <ns0:text x="350" y="780.0" font-size="10" text-anchor="middle" fill="#1a237e">s'‚ÇÇ</ns0:text>
    <ns0:circle cx="550" cy="180" r="15" fill="#6a1b9a" stroke="#6c3483" stroke-width="2"/>
    <ns0:text x="550" y="805.0" font-size="10" text-anchor="middle" fill="#1a237e">s'‚ÇÉ</ns0:text>
    <ns0:circle cx="650" cy="180" r="15" fill="#6a1b9a" stroke="#6c3483" stroke-width="2"/>
    <ns0:text x="650" y="830.0" font-size="10" text-anchor="middle" fill="#1a237e">s'‚ÇÑ</ns0:text>
    <ns0:path d="M 430,90 L 310,135" stroke="#007bff" stroke-width="2" marker-end="url(#arrowBlue2)"/>
    <ns0:path d="M 450,100 L 450,130" stroke="#007bff" stroke-width="2" marker-end="url(#arrowBlue2)"/>
    <ns0:path d="M 470,90 L 590,135" stroke="#007bff" stroke-width="2" marker-end="url(#arrowBlue2)"/>
    <ns0:path d="M 290,160 L 260,170" stroke="#6f42c1" stroke-width="2" marker-end="url(#arrowPurple)"/>
    <ns0:path d="M 310,160 L 355,170" stroke="#6f42c1" stroke-width="2" marker-end="url(#arrowPurple)"/>
    <ns0:path d="M 460,160 L 545,175" stroke="#6f42c1" stroke-width="2" marker-end="url(#arrowPurple)"/>
    <ns0:path d="M 590,160 L 645,175" stroke="#6f42c1" stroke-width="2" marker-end="url(#arrowPurple)"/>
    <ns0:text x="360" y="855.0" font-size="11" fill="#1565c0">œÄ(a|s)</ns0:text>
    <ns0:text x="280" y="880.0" font-size="10" fill="#1a237e">P(s'|s,a)</ns0:text>
    <ns0:text x="180" y="905.0" font-size="10" fill="#c62828">R + Œ≥V(s')</ns0:text>
  </ns0:g>
  <ns0:rect x="50" y="660" width="2025.0" height="85" fill="#1a237e" stroke="#28a745" stroke-width="2" rx="8"/>
  <ns0:text x="500" y="930.0" font-size="15" font-weight="bold" text-anchor="middle" fill="#145a32">
    üîë Key Properties
  </ns0:text>
  <ns0:g transform="translate(70, 700)">
    <ns0:text x="0" y="955.0" font-size="13" fill="#1a237e">
      <ns0:tspan font-weight="bold" fill="#2e7d32">1. Fixed Point:</ns0:tspan>
       Optimal value satisfies V* = ùì£V* (contraction mapping)
    </ns0:text>
    <ns0:text x="0" y="980.0" font-size="13" fill="#1a237e">
      <ns0:tspan font-weight="bold" fill="#1565c0">2. Policy Extraction:</ns0:tspan>
       œÄ*(s) = argmax_a Q*(s,a) ‚Üí Greedy w.r.t. Q* is optimal
    </ns0:text>
    <ns0:text x="0" y="1005.0" font-size="13" fill="#1a237e">
      <ns0:tspan font-weight="bold" fill="#6a1b9a">3. Bootstrapping:</ns0:tspan>
       V(s) depends on V(s') ‚Üí Update estimates using estimates!
    </ns0:text>
  </ns0:g>
  <ns0:rect x="50" y="765" width="2025.0" height="70" fill="#1a237e" stroke="#fd7e14" stroke-width="2" rx="8"/>
  <ns0:text x="500" y="1030.0" font-size="15" font-weight="bold" text-anchor="middle" fill="#d35400">
    üí° Algorithms Based on Bellman
  </ns0:text>
  <ns0:text x="500" y="1055.0" font-size="12" text-anchor="middle" fill="#1a237e">
    <ns0:tspan font-weight="bold" fill="#e67e22">Value Iteration:</ns0:tspan>
     V_{k+1}(s) = max_a [R + Œ≥Œ£P(s'|s,a)V_k(s')] ‚Ä¢ 
    <ns0:tspan font-weight="bold" fill="#1565c0">Policy Iteration:</ns0:tspan>
     Evaluate then Improve ‚Ä¢ 
    <ns0:tspan font-weight="bold" fill="#2e7d32">Q-Learning:</ns0:tspan>
     Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥max_a' Q(s',a') - Q(s,a)]
  </ns0:text>
</ns0:svg>
