<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Intrinsic%20Motivation&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Epsilon-Greedy](../02_epsilon_greedy/) | â¡ï¸ [Next: UCB](../04_ucb/)

---

## ğŸ¯ Visual Overview

<img src="./images/intrinsic-motivation.svg" width="100%">

*Caption: Intrinsic rewards are generated by the agent itself based on prediction error, information gain, or state novelty. Combined with extrinsic rewards, they encourage exploration in sparse reward environments.*

---

## ğŸ“‚ Overview

Intrinsic motivation provides internal rewards when external rewards are sparse or absent. This enables learning in hard exploration problems like Montezuma's Revenge.

---

## ğŸ“ Mathematical Framework

### General Formulation

```
Total reward at time t:
  r_t = r_t^e + Î² Â· r_t^i

Where:
  r_t^e = extrinsic (environment) reward
  r_t^i = intrinsic (agent-generated) reward
  Î² = intrinsic reward coefficient

Goal: Maximize E[Î£_t Î³^t (r_t^e + Î² Â· r_t^i)]

```

### Types of Intrinsic Rewards

| Type | Formula | Method |
|------|---------|--------|
| **Prediction Error** | \|\|s' - Å'\|\| | ICM, RND |
| **Information Gain** | I(Î¸; s'\|s,a) | VIME |
| **State Novelty** | 1/âˆšN(s) | Count-based |
| **Ensemble Disagreement** | Var(Qâ‚...Qâ‚™) | Bootstrapped DQN |

---

## ğŸ“ Count-Based Exploration

### Pseudocount Definition

```
Classical count-based bonus:
  r_i(s) = Î² / âˆšN(s)

Where N(s) = number of visits to state s

For continuous states, use density model:
  NÌ‚(s) = Ï(s) Â· n / (1 - Ï(s))

Where:
  Ï(s) = density estimate
  n = total samples seen
  NÌ‚(s) = "pseudocount"

```

### Theoretical Foundation

```
Theorem (UCB Regret Bound):
  For UCB exploration with bonus câˆš(log t / N(s,a)):
  
  Cumulative regret â‰¤ O(âˆš(|S||A|T log T))
  
  After T steps, regret grows sublinearly â†’ optimal!

Proof idea:
  1. Bonus ensures under-explored actions are tried
  2. As N(s,a) â†’ âˆ, bonus â†’ 0
  3. Eventually exploits optimal action

```

---

## ğŸ“ Information-Theoretic Approaches

### VIME (Variational Information Maximizing Exploration)

```
Intrinsic reward = information gain about dynamics:

  r_i(s,a,s') = D_KL[p(Î¸|D âˆª {s,a,s'}) || p(Î¸|D)]

Where:
  Î¸ = dynamics model parameters
  D = replay buffer
  
Intuition: Reward for transitions that change our 
understanding of the world.

```

### Empowerment

```
Empowerment = mutual information between actions and future states:

  Îµ(s) = max_Ï‰ I(A; S' | s)
       = max_Ï‰ H(S' | s) - H(S' | A, s)

Maximize control: Choose states where actions 
have maximum influence on outcomes.

```

---

## ğŸ“ Ensemble-Based Exploration

### Bootstrapped DQN

```
Train ensemble of Q-networks {Q_1, ..., Q_K}
Each Q_k trained on different bootstrap sample

Exploration bonus:
  r_i(s,a) = Ïƒ(Q_1(s,a), ..., Q_K(s,a))
           = standard deviation across ensemble

High disagreement â†’ high uncertainty â†’ explore!

```

### Epistemic Uncertainty

```
Decomposition of uncertainty:
  Var[Q(s,a)] = E[Var[Q|Î¸]] + Var[E[Q|Î¸]]
                -----------   -------------
                 aleatoric     epistemic
                (irreducible)  (reducible)

Aleatoric: Inherent randomness (noisy rewards)
Epistemic: Model uncertainty (lack of data)

Exploration should target epistemic uncertainty!

```

---

## ğŸ“ Reward Normalization

### Running Statistics

```
Problem: Intrinsic reward scale varies across training

Solution: Normalize by running statistics:
  r_i^norm = (r_i - Î¼_i) / Ïƒ_i

Where Î¼_i, Ïƒ_i are running mean/std of intrinsic rewards.

Or use reward clipping:
  r_i^clip = clip(r_i, -c, c)

```

### Adaptive Scaling

```
Adaptive Î² (intrinsic coefficient):
  Î²_t = Î²_0 Â· decay^t

Start with high exploration (large Î²)
Gradually shift to exploitation (small Î²)

```

---

## ğŸ”‘ Key Methods

| Method | Idea | Paper |
|--------|------|-------|
| **ICM** | Curiosity from prediction error | Pathak et al. 2017 |
| **RND** | Random network distillation | Burda et al. 2018 |
| **NGU** | Never Give Up | Badia et al. 2020 |
| **Go-Explore** | Archive + goal-conditioned | Ecoffet et al. 2019 |

---

## ğŸ’» Code

```python
class IntrinsicReward:
    def __init__(self, state_dim, hidden_dim=64):
        # Target network (random, fixed)
        self.target = RandomNetwork(state_dim, hidden_dim)
        # Predictor network (learned)
        self.predictor = RandomNetwork(state_dim, hidden_dim)
        self.optimizer = torch.optim.Adam(self.predictor.parameters())
    
    def compute_reward(self, state):
        """RND-style intrinsic reward"""
        with torch.no_grad():
            target_feat = self.target(state)
        pred_feat = self.predictor(state)
        
        # Prediction error = intrinsic reward
        intrinsic_reward = ((target_feat - pred_feat) ** 2).mean()
        
        # Train predictor
        loss = intrinsic_reward
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return intrinsic_reward.item()

```

## ğŸ”— Where This Topic Is Used

| Application | Intrinsic Motivation |
|-------------|---------------------|
| **Sparse Rewards** | Exploration bonus |
| **Curiosity-Driven** | ICM, RND methods |
| **Montezuma's Revenge** | Hard exploration |
| **Open-Ended Learning** | Novelty search |

## ğŸ“š References

| Type | Resource | Link |
|------|----------|------|
| ğŸ“– | Textbook | See parent folder |
| ğŸ¥ | Video Lectures | YouTube/Coursera |

---

â¬…ï¸ [Back: Epsilon-Greedy](../02_epsilon_greedy/) | â¡ï¸ [Next: UCB](../04_ucb/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
