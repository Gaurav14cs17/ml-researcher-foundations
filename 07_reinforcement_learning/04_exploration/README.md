<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Exploration&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Policy Methods](../03_policy_methods/) | â¡ï¸ [Next: Model-Based](../05_model_based/)

---

## ğŸ¯ Visual Overview

<img src="./images/exploration-vs-exploitation.svg" width="100%">

*Caption: The exploration-exploitation trade-off is fundamental to RL. Explore too much and you waste time; exploit too much and you miss better options. Different methods (Îµ-greedy, UCB, curiosity) balance this differently.*

---

## ğŸ“ Mathematical Foundations

### Îµ-Greedy
```
Ï€(a|s) = {
  1 - Îµ + Îµ/|A|  if a = argmax Q(s,a)
  Îµ/|A|          otherwise
}

Îµ typically decays: Îµâ‚œ = Îµâ‚€ Ã— decay^t
```

### Upper Confidence Bound (UCB)
```
UCB1: a = argmax[Q(s,a) + câˆš(ln(t)/N(s,a))]

Where:
â€¢ Q(s,a) = estimated value (exploitation)
â€¢ âˆš(ln(t)/N(s,a)) = uncertainty bonus (exploration)
â€¢ c = exploration coefficient
```

### Entropy Regularization
```
Objective: J(Ï€) = E[Î£â‚œ râ‚œ + Î± H(Ï€(Â·|sâ‚œ))]

Where H(Ï€(Â·|s)) = -Î£â‚ Ï€(a|s) log Ï€(a|s)

Maximum entropy RL encourages exploration
```

### Intrinsic Curiosity
```
ICM bonus: ráµ¢ = ||Å' - s'||Â²  (prediction error)

RND bonus: ráµ¢ = ||f(s') - fÌ‚(s')||Â²
Where f is random fixed, fÌ‚ is learned
```

---

## ğŸ“‚ Topics

| Folder | Topic | Key Idea |
|--------|-------|----------|
| [01_curiosity/](./01_curiosity/) | ICM, RND | Prediction error |
| [02_epsilon_greedy/](./02_epsilon_greedy/) | Îµ-greedy | Random with prob Îµ |
| [03_intrinsic/](./03_intrinsic/) | Intrinsic motivation | Curiosity |
| [04_ucb/](./04_ucb/) | Upper Confidence Bound | Optimism |

---

## ğŸ¯ The Dilemma

```
Exploration: Try new things to learn
Exploitation: Use what you know to get reward

Too much exploration â†’ Never get good rewards
Too little â†’ Miss better options
```

---

## ğŸ“Š Methods Comparison

| Method | Pros | Cons |
|--------|------|------|
| **Îµ-greedy** | Simple | Not directed |
| **UCB** | Principled | Requires counts |
| **Curiosity** | Works on hard exploration | May get stuck |
| **Entropy bonus** | Smooth exploration | Hyperparameter |

---

## ğŸ”— Where This Topic Is Used

| Topic | How Exploration Is Used |
|-------|------------------------|
| **DQN** | Îµ-greedy action selection |
| **PPO** | Entropy bonus in objective |
| **A/B Testing** | UCB, Thompson sampling |
| **Recommender Systems** | Exploration-exploitation tradeoff |
| **AlphaGo** | MCTS exploration (UCB) |
| **Curiosity-driven RL** | ICM, RND for sparse rewards |
| **Safe RL** | Constrained exploration |
| **Multi-armed Bandits** | UCB, Thompson sampling |

### Exploration In Famous Systems

| System | Exploration Method |
|--------|-------------------|
| **DQN (Atari)** | Îµ-greedy |
| **AlphaGo** | UCB in MCTS |
| **OpenAI Five** | Entropy bonus |
| **Montezuma** | Curiosity (ICM/RND) |
| **Ad systems** | Thompson sampling |

### Prerequisite For

```
Exploration --> Any RL algorithm
           --> Bandit algorithms
           --> MCTS (Monte Carlo Tree Search)
           --> Safe reinforcement learning
```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Sutton & Barto Ch. 2 | [RL Book](http://incompleteideas.net/book/) |
| ğŸ“„ | ICM Paper | [arXiv](https://arxiv.org/abs/1705.05363) |
| ğŸ“„ | RND Paper | [arXiv](https://arxiv.org/abs/1810.12894) |
| ğŸ‡¨ğŸ‡³ | æ¢ç´¢ä¸åˆ©ç”¨è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/32356077) |
| ğŸ‡¨ğŸ‡³ | å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢ | [CSDN](https://blog.csdn.net/qq_30615903/article/details/80952771) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ æ¢ç´¢ç­–ç•¥ | [Bç«™](https://www.bilibili.com/video/BV1yp4y1s7Qw) |

---

â¬…ï¸ [Back: Policy Methods](../03_policy_methods/) | â¡ï¸ [Next: Model-Based](../05_model_based/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
