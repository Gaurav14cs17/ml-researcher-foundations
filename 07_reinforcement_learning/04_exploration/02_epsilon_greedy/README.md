<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Îµ-Greedy%20Exploration&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Curiosity](../01_curiosity/) | â¡ï¸ [Next: Intrinsic](../03_intrinsic/)

---

## ğŸ¯ Visual Overview

<img src="./images/epsilon-greedy.svg" width="100%">

*Caption: Îµ-greedy explores by taking random actions with probability Îµ, and exploits by taking the best known action with probability 1-Îµ. Typically Îµ decays over time: explore early, exploit later.*

---

## ğŸ“‚ Overview

Îµ-greedy is the most common exploration strategy in RL due to its simplicity. It provides a tunable trade-off between exploration and exploitation.

---

## ğŸ“ Mathematical Definition

### Action Selection

```
       â§ random action from A    with probability Îµ
a_t = â¨
       â© argmax_a Q(s_t, a)      with probability 1-Îµ

Where:
    Îµ âˆˆ [0, 1]: Exploration rate
    A: Action space
    Q(s, a): Estimated action-value function
```

### Expected Behavior

```
For |A| actions:

P(select optimal action) = (1 - Îµ) + Îµ/|A|
P(select random action) = Îµ Ã— (|A|-1)/|A|

Example: |A| = 4, Îµ = 0.1
    P(optimal) = 0.9 + 0.1/4 = 0.925
    P(each non-optimal) = 0.1 Ã— 3/4 Ã— 1/3 = 0.025
```

---

## ğŸ“ Decay Schedules

### Linear Decay

```
Îµ_t = max(Îµ_min, Îµ_0 - t Ã— decay_rate)

Example: Îµ_0 = 1.0, Îµ_min = 0.01, decay_rate = 0.001
    t=0:    Îµ = 1.0
    t=500:  Îµ = 0.5
    t=990:  Îµ = 0.01 (minimum)
```

### Exponential Decay

```
Îµ_t = max(Îµ_min, Îµ_0 Ã— decay^t)

Example: Îµ_0 = 1.0, decay = 0.995
    t=0:    Îµ = 1.0
    t=100:  Îµ â‰ˆ 0.606
    t=500:  Îµ â‰ˆ 0.082
```

### Inverse Decay

```
Îµ_t = 1 / (1 + t Ã— k)

More exploration-heavy at start, slower decay
```

---

## ğŸ“ Theoretical Analysis

### GLIE Condition

```
Theorem (Greedy in the Limit with Infinite Exploration):

Q-learning with Îµ-greedy converges to Q* if:

1. All state-action pairs are visited infinitely often:
   lim_{tâ†’âˆ} N_t(s,a) = âˆ  âˆ€s,a

2. Policy becomes greedy in the limit:
   lim_{tâ†’âˆ} Ï€_t(a|s) = 1  for a = argmax Q(s,a)

Sufficient condition: Î£_t Îµ_t = âˆ and Î£_t Îµ_tÂ² < âˆ

Example satisfying GLIE: Îµ_t = 1/t
```

### Regret Analysis

```
Definition: Regret R_T = Î£_{t=1}^T (r* - r_t)
  Where r* = optimal expected reward

For Îµ-greedy with Îµ_t = c/t on K-armed bandit:

  E[R_T] = O(K log T / Î”)
  
  Where Î” = min gap between optimal and suboptimal

Comparison:
  - Îµ-greedy: O(K log T / Î”)
  - UCB: O(K log T / Î”)  (similar, but directed exploration)
  - Thompson: O(K log T / Î”)  (Bayesian optimal)
```

### Probability of Optimal Selection

```
After t steps with Îµ_t = 1/t:

P(select optimal action) = (1 - 1/t) + 1/(tÂ·|A|)
                        â‰ˆ 1 - (|A|-1)/(tÂ·|A|)  for large t

Convergence: P â†’ 1 as t â†’ âˆ

Time to 99% exploitation (Îµ â‰¤ 0.01):
  For Îµ_t = 1/t: t = 100 steps
  For Îµ_t = Îµâ‚€Â·0.99^t: t â‰ˆ 459 steps (if Îµâ‚€=1)
```

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **Îµ = 1.0** | Pure exploration (all random) |
| **Îµ = 0.0** | Pure exploitation (all greedy) |
| **Îµ-decay** | Reduce Îµ over time (exploreâ†’exploit) |
| **Îµ-first** | Explore Îµ% of time, then exploit forever |
| **Annealing** | Smooth transition from explore to exploit |

---

## ğŸ“Š Comparison with Other Strategies

| Strategy | Exploration | Optimal? | Complexity |
|----------|-------------|----------|------------|
| **Îµ-greedy** | Random | Suboptimal | O(1) |
| **Softmax** | Probability âˆ Q | Better | O(\|A\|) |
| **UCB** | Uncertainty-based | Near-optimal | O(\|A\|) |
| **Thompson** | Bayesian sampling | Optimal | O(\|A\|) |

```
Îµ-greedy Limitation:

    Q(aâ‚) = 10 Â± 0.1   â† Very confident
    Q(aâ‚‚) = 9  Â± 5.0   â† Very uncertain
    
Îµ-greedy: Explores both equally (random)
UCB/Thompson: Explores aâ‚‚ more (might be better!)
```

---

## ğŸ’» Code Examples

### Basic Implementation

```python
import numpy as np

def epsilon_greedy(q_values, epsilon):
    """
    Select action using epsilon-greedy policy
    
    Args:
        q_values: Q(s, a) for all actions
        epsilon: Exploration probability
    
    Returns:
        Selected action index
    """
    if np.random.random() < epsilon:
        return np.random.randint(len(q_values))  # Explore
    else:
        return np.argmax(q_values)  # Exploit
```

### With Decay Schedules

```python
class EpsilonScheduler:
    """Different epsilon decay strategies"""
    
    def __init__(self, start=1.0, end=0.01, decay_type='exponential'):
        self.epsilon = start
        self.start = start
        self.end = end
        self.decay_type = decay_type
        self.step = 0
    
    def get_epsilon(self):
        return self.epsilon
    
    def update(self, decay_param=0.995):
        self.step += 1
        
        if self.decay_type == 'exponential':
            self.epsilon = max(self.end, self.epsilon * decay_param)
        
        elif self.decay_type == 'linear':
            self.epsilon = max(self.end, self.start - self.step * decay_param)
        
        elif self.decay_type == 'inverse':
            self.epsilon = max(self.end, 1.0 / (1 + self.step * decay_param))
        
        return self.epsilon

# Usage in training loop
scheduler = EpsilonScheduler(start=1.0, end=0.01, decay_type='exponential')

for episode in range(1000):
    state = env.reset()
    epsilon = scheduler.get_epsilon()
    
    while not done:
        q_values = model(state)
        action = epsilon_greedy(q_values, epsilon)
        # ... take action, train model ...
    
    scheduler.update(decay_param=0.995)
```

### With Q-Learning

```python
def q_learning_with_epsilon_greedy(env, num_episodes, 
                                    alpha=0.1, gamma=0.99,
                                    epsilon_start=1.0, epsilon_end=0.01):
    """
    Q-Learning with decaying epsilon-greedy exploration
    """
    Q = np.zeros((env.observation_space.n, env.action_space.n))
    epsilon = epsilon_start
    decay = (epsilon_start - epsilon_end) / num_episodes
    
    rewards_history = []
    
    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            # Îµ-greedy action selection
            if np.random.random() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(Q[state])
            
            next_state, reward, done, _ = env.step(action)
            
            # Q-learning update
            td_target = reward + gamma * np.max(Q[next_state]) * (1 - done)
            Q[state, action] += alpha * (td_target - Q[state, action])
            
            state = next_state
            total_reward += reward
        
        # Decay epsilon
        epsilon = max(epsilon_end, epsilon - decay)
        rewards_history.append(total_reward)
    
    return Q, rewards_history
```

---

## ğŸ“Š Hyperparameter Guidelines

| Scenario | Îµ_start | Îµ_end | Decay |
|----------|---------|-------|-------|
| **Small env** | 1.0 | 0.01 | Fast (0.99) |
| **Large env** | 1.0 | 0.05 | Slow (0.999) |
| **DQN (Atari)** | 1.0 | 0.1 | Linear over 1M |
| **Bandits** | 0.1 | 0.01 | Very slow |

---

## ğŸ”— Connection to Other Strategies

```
Îµ-Greedy (simplest)
    |
    +-- Softmax/Boltzmann (smooth probability)
    |       a ~ exp(Q(s,a)/Ï„) / Î£ exp(Q(s,a')/Ï„)
    |
    +-- UCB (uncertainty bonus)
    |       a = argmax Q(s,a) + câˆš(log t / N(a))
    |
    +-- Thompson Sampling (Bayesian)
            Sample Q ~ posterior, act greedily on sample
```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | UCB Exploration | [../04_ucb/](../04_ucb/) |
| ğŸ“– | Curiosity-Driven | [../01_curiosity/](../01_curiosity/) |
| ğŸ“– | Sutton & Barto Ch. 2 | [RL Book](http://incompleteideas.net/book/) |
| ğŸ¥ | David Silver Lecture 9 | [YouTube](https://www.youtube.com/watch?v=sGuiWX07sKw) |
| ğŸ‡¨ğŸ‡³ | æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/32356077) |
| ğŸ‡¨ğŸ‡³ | DQNä¸­çš„Îµ-greedy | [CSDN](https://blog.csdn.net/qq_30615903/article/details/80952771) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ æ¢ç´¢ç­–ç•¥ | [Bç«™](https://www.bilibili.com/video/BV1yp4y1s7Qw) |
| ğŸ‡¨ğŸ‡³ | å¤šè‡‚è€è™æœºé—®é¢˜ | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2017-08-14-2)

## ğŸ”— Where This Topic Is Used

| Application | Îµ-greedy |
|-------------|---------|
| **DQN** | Standard exploration |
| **Bandits** | Simple exploration |
| **A/B Testing** | Exploration-exploitation |
| **Recommendation** | Random recommendations |

---

â¬…ï¸ [Back: Curiosity](../01_curiosity/) | â¡ï¸ [Next: Intrinsic](../03_intrinsic/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
