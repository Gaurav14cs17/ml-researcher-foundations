<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Reward%20Functions&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Dynamics](../02_dynamics/) | â¡ï¸ [Next: States & Actions](../04_states_actions/)

---

## ğŸ¯ Visual Overview

<img src="./images/reward-function.svg" width="100%">

*Caption: Rewards R(s,a,s') provide scalar feedback after each transition. The agent's goal is to maximize cumulative discounted rewards. Sparse rewards are hard to learn from; dense rewards are easier but may cause reward hacking.*

---

## ğŸ“‚ Overview

The reward function defines what the agent should optimize. Good reward design is crucial - poorly designed rewards lead to unintended behaviors.

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **Reward r** | Scalar feedback signal |
| **Return G** | Cumulative discounted reward |
| **Sparse Reward** | Signal only at goal |
| **Dense Reward** | Signal every step |
| **Reward Shaping** | Hand-designed hints |

---

## ğŸ“ Mathematical Formulation

### Reward Function Definition

```
R: S Ã— A Ã— S â†’ â„

R(s, a, s') = immediate reward for transitioning from s to s' via action a

Alternative formulations:
  R(s, a)    - Reward depends only on state-action
  R(s)       - Reward depends only on state
  R(s, a, s') - Full specification (most general)
```

### Expected Reward

```
r(s, a) = E[R(s, a, S')] = Î£_{s'} P(s'|s, a) R(s, a, s')

This is the expected immediate reward for taking action a in state s.
```

---

## ğŸ“ Return and Value Functions

### Discounted Return

```
G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...
    = Î£_{k=0}^âˆ Î³^k R_{t+k+1}

Properties:
  1. Finite if Î³ < 1 and rewards bounded: |G_t| â‰¤ R_max/(1-Î³)
  2. Recursive: G_t = R_{t+1} + Î³G_{t+1}
```

### Value Function Derivation

```
V^Ï€(s) = E_Ï€[G_t | S_t = s]
       = E_Ï€[R_{t+1} + Î³G_{t+1} | S_t = s]
       = E_Ï€[R_{t+1} | S_t = s] + Î³ E_Ï€[G_{t+1} | S_t = s]

By tower property and Markov:
       = Î£_a Ï€(a|s) r(s,a) + Î³ Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a) V^Ï€(s')
       = Î£_a Ï€(a|s) [r(s,a) + Î³ Î£_{s'} P(s'|s,a) V^Ï€(s')]
```

---

## ğŸ“ Reward Shaping Theory

### Potential-Based Shaping

```
Theorem (Ng et al., 1999): Potential-based reward shaping preserves 
optimal policies.

Shaped reward:
  R'(s, a, s') = R(s, a, s') + Î³Î¦(s') - Î¦(s)

Where Î¦: S â†’ â„ is any potential function.

Proof sketch:
  Let G'_t be return under R'. Then:
  G'_t = Î£_{k=0}^âˆ Î³^k [R_{t+k+1} + Î³Î¦(S_{t+k+2}) - Î¦(S_{t+k+1})]
       = Î£_{k=0}^âˆ Î³^k R_{t+k+1} + Î£_{k=0}^âˆ Î³^{k+1}Î¦(S_{t+k+2}) - Î£_{k=0}^âˆ Î³^k Î¦(S_{t+k+1})
       = G_t + Î³Î¦(S_âˆ) - Î¦(S_t)
       = G_t - Î¦(s)  (assuming terminal Î¦ = 0)
       
  So V'(s) = V(s) - Î¦(s)
  Same ordering of policies: Ï€* unchanged! âˆ
```

### Non-Potential Shaping

```
Warning: Non-potential shaping can change optimal policy!

Example: 
  Original: R(s,a) = 1 at goal, 0 elsewhere
  Bad shaping: R'(s,a) = R(s,a) + 0.1 for action "left"
  
  Result: Agent prefers "left" even when suboptimal!
```

---

## ğŸ“ Reward Sparsity Analysis

### Sparse Reward Problem

```
For goal-reaching task:
  R(s) = { 1  if s = s_goal
         { 0  otherwise

Expected reward per episode:
  E[Î£_t R_t] = P(reach goal)

If P(reach goal) â‰ˆ 0 via random exploration:
  - Gradient â‰ˆ 0 (no learning signal)
  - Credit assignment over long horizons
  - Exploration becomes critical
```

### Information-Theoretic View

```
Reward signal entropy:
  H(R) = -Î£_r P(R=r) log P(R=r)

Sparse reward: H(R) â‰ˆ 0 (almost always 0)
Dense reward: H(R) > 0 (varied feedback)

More informative rewards â†’ faster learning
But risk of reward hacking with dense rewards!
```

---

## ğŸ“ Intrinsic Motivation

### Curiosity-Based Rewards

```
Intrinsic reward = prediction error

ICM (Intrinsic Curiosity Module):
  r_i(s, a, s') = ||Å' - s'||Â²
  
  Where Å' = f(s, a) is predicted next state.
  Novel states â†’ high error â†’ high reward.
```

### Count-Based Exploration

```
r_i(s) = Î² / âˆšN(s)

Where N(s) = visit count for state s.
Less-visited states get higher bonus.

Theoretical basis: Upper Confidence Bound
  UCB(s,a) = Q(s,a) + câˆš(log t / N(s,a))
```

---

## âš ï¸ Reward Hacking

```
Problem: Agent finds unintended ways to maximize reward

Example: Racing game with reward for speed
- Agent learns to go in circles (high speed, no progress)
- Agent finds walls that give infinite speed glitch

Solution: Careful reward design, human oversight, RLHF
```

---

## ğŸ’» Code

```python
def sparse_reward(state, goal):
    """Reward only at goal - hard to learn"""
    return 1.0 if state == goal else 0.0

def dense_reward(state, goal):
    """Reward based on progress - easier"""
    distance = np.linalg.norm(state - goal)
    return -distance  # Closer = higher reward

def shaped_reward(state, prev_state, goal):
    """Potential-based shaping - preserves optimal policy"""
    phi = lambda s: -np.linalg.norm(s - goal)
    return phi(state) - phi(prev_state)
```

## ğŸ”— Where This Topic Is Used

| Application | Reward Design |
|-------------|--------------|
| **RLHF** | Human preference scores |
| **Game AI** | Win/lose signals |
| **Robotics** | Task completion bonus |
| **Recommendation** | Click/engagement |

## ğŸ“š References

| Type | Resource | Link |
|------|----------|------|
| ğŸ“– | Textbook | See parent folder |
| ğŸ¥ | Video Lectures | YouTube/Coursera |

---

â¬…ï¸ [Back: Dynamics](../02_dynamics/) | â¡ï¸ [Next: States & Actions](../04_states_actions/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
