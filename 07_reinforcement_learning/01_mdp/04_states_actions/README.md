<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=States%20and%20Actions&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Rewards](../03_rewards/) | â¡ï¸ [Next: Value Methods](../../02_value_methods/)

---

## ğŸ¯ Visual Overview

<img src="./images/states-actions.svg" width="100%">

*Caption: State space S contains all possible situations the agent can observe. Action space A contains all possible decisions the agent can make. These can be discrete (finite set) or continuous (real-valued).*

---

## ğŸ“‚ Overview

States represent what the agent observes about the environment. Actions represent what the agent can do to change the environment.

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **State s** | Complete description of environment at time t |
| **Action a** | Decision/control taken by agent |
| **State Space S** | Set of all possible states |
| **Action Space A** | Set of all possible actions |

---

## ğŸ“ Mathematical Formalization

### State Space

```
S = {sâ‚, sâ‚‚, ..., sâ‚™}  (discrete, finite)
  or
S âŠ† â„â¿               (continuous)

Properties:
  1. Complete: Contains all distinguishable situations
  2. Markov: s_t encodes all relevant history
  3. Observable: Agent can perceive s_t at each step
```

### Action Space

```
A = {aâ‚, aâ‚‚, ..., aâ‚˜}  (discrete, finite)
  or
A âŠ† â„áµ               (continuous)

Can be state-dependent: A(s) âŠ† A
  Example: Legal moves depend on board position
```

### Policy as Mapping

```
Deterministic policy: Ï€: S â†’ A
  a = Ï€(s)

Stochastic policy: Ï€: S â†’ Î”(A)
  Ï€(a|s) = P(A_t = a | S_t = s)
  
  Properties:
    Ï€(a|s) â‰¥ 0  âˆ€a, s
    Î£_a Ï€(a|s) = 1  âˆ€s
```

---

## ğŸ“ State Representation Theory

### Sufficient Statistics

```
Theorem: A state representation Ï†(h_t) is sufficient if:

  P(R_{t+1}, S_{t+1} | Ï†(h_t), A_t) = P(R_{t+1}, S_{t+1} | h_t, A_t)

Where h_t = (S_0, A_0, R_1, S_1, ..., S_t) is history.

Sufficient statistics preserve Markov property.
```

### State Aggregation

```
Partition S into groups {G_1, G_2, ..., G_k}

Aggregated MDP is valid if for all s, s' âˆˆ G_i:
  P(s'' âˆˆ G_j | s, a) = P(s'' âˆˆ G_j | s', a)  âˆ€a, j
  R(s, a) = R(s', a)  âˆ€a

This preserves optimal value function on aggregate states.
```

---

## ğŸ“ Continuous Spaces

### Discretization

```
For continuous S âŠ† â„â¿, discretize into grid:

S_discrete = {s_i}  where s_i = centers of grid cells

Trade-off:
  Fine grid: |S_discrete| large, curse of dimensionality
  Coarse grid: Loss of precision, suboptimal policies
```

### Function Approximation

```
Instead of discretizing, approximate value function:

V_Î¸(s) â‰ˆ V^Ï€(s)  for all s âˆˆ S

Common choices:
  - Linear: V_Î¸(s) = Î¸áµ€Ï†(s)
  - Neural network: V_Î¸(s) = NN_Î¸(s)
  
Advantage: Generalizes to unseen states
```

### Continuous Actions

```
For A âŠ† â„áµ, policy outputs parameters of distribution:

Gaussian policy:
  Ï€_Î¸(a|s) = N(a; Î¼_Î¸(s), Ïƒ_Î¸(s)Â²)
  
  Î¼_Î¸(s) = neural network output
  Ïƒ_Î¸(s) = learned or fixed variance

Sampling: a ~ Ï€_Î¸(Â·|s)
```

---

## ğŸ“ Complexity Analysis

### Tabular Methods

```
Space complexity: O(|S| Ã— |A|) for Q-table

Time per update: O(1)

Total for value iteration:
  O(|S|Â² Ã— |A| Ã— 1/(1-Î³) Ã— log(1/Îµ))
  
Curse of dimensionality: 
  If S âŠ† â„â¿ discretized with k bins per dimension:
  |S| = kâ¿ (exponential in dimension!)
```

### Deep RL

```
Parameter count: O(hidden_dimsÂ²)  (for MLP)
                 O(filters Ã— kernelÂ²)  (for CNN)

Time per update: O(batch_size Ã— parameter_count)

Generalization to unseen states enables tractable learning
in high-dimensional spaces.
```

---

## ğŸ“ Types of Spaces

| Type | State Examples | Action Examples |
|------|----------------|-----------------|
| **Discrete** | Grid positions, game boards | Left/Right/Up/Down |
| **Continuous** | Robot joint angles, velocity | Force, torque values |
| **High-Dim** | Images (84Ã—84Ã—4) | Multi-joint control |
| **Hybrid** | Mixed discrete + continuous | Discrete choice + continuous param |

---

## ğŸ’» Code

```python
import gymnasium as gym

# Discrete: CartPole
env = gym.make("CartPole-v1")
print(f"State: {env.observation_space}")  # Box(4,) - continuous
print(f"Actions: {env.action_space}")     # Discrete(2) - left/right

# Continuous: MuJoCo
env = gym.make("HalfCheetah-v4")
print(f"State: {env.observation_space}")  # Box(17,) - joint positions/velocities
print(f"Actions: {env.action_space}")     # Box(6,) - continuous torques
```

## ğŸ”— Where This Topic Is Used

| Application | How States/Actions Are Used |
|-------------|---------------------------|
| **Game Playing** | Board state â†’ legal moves |
| **Robotics** | Joint positions â†’ torques |
| **Trading** | Market state â†’ buy/sell |
| **Dialogue** | Conversation history â†’ responses |

## ğŸ“š References

| Type | Resource | Link |
|------|----------|------|
| ğŸ“– | Textbook | See parent folder |
| ğŸ¥ | Video Lectures | YouTube/Coursera |

---

â¬…ï¸ [Back: Rewards](../03_rewards/) | â¡ï¸ [Next: Value Methods](../../02_value_methods/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
