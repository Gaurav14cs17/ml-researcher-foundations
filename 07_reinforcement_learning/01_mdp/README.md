<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Markov%20Decision%20Processes%20MDP&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Reinforcement Learning](../) | â¡ï¸ [Next: Value Methods](../02_value_methods/)

---

## ğŸ¯ Visual Overview

<img src="./images/mdp-diagram.svg" width="100%">

*Caption: An MDP models sequential decision-making as a tuple (S, A, P, R, Î³). At each step, the agent observes state s, takes action a, receives reward r, and transitions to next state s'. The Markov property means the future depends only on the current state.*

---

## ğŸ“‚ Topics

| Folder | Topic | Key Concepts |
|--------|-------|--------------|
| [01_discounting/](./01_discounting/) | Discount factor | Î³, infinite horizon |
| [02_dynamics/](./02_dynamics/) | Transition dynamics | P(s'\|s,a) |
| [03_rewards/](./03_rewards/) | Reward function | r(s,a,s'), shaping |
| [04_states_actions/](./04_states_actions/) | State & Action spaces | S, A definitions |

---

## ğŸ“ MDP Definition

```
MDP = (S, A, P, R, Î³)

S: State space     â€” What the agent observes
A: Action space    â€” What the agent can do
P: P(s'|s,a)       â€” Transition probability (dynamics)
R: r(s,a,s')       â€” Reward function
Î³: Discount factor â€” Î³ âˆˆ [0,1], importance of future rewards
```

### The Markov Property

```
P(sâ‚œâ‚Šâ‚ | sâ‚œ, aâ‚œ, sâ‚œâ‚‹â‚, aâ‚œâ‚‹â‚, ..., sâ‚€, aâ‚€) = P(sâ‚œâ‚Šâ‚ | sâ‚œ, aâ‚œ)

"The future is independent of the past given the present"

This simplifies planning: we only need to know the current state!
```

---

## ğŸ¯ Goal: Find Optimal Policy

```
Find policy Ï€(a|s) that maximizes:

    V^Ï€(s) = E_Ï€[Î£â‚œâ‚Œâ‚€^âˆ Î³áµ— râ‚œ | sâ‚€ = s]
    
    Expected discounted cumulative reward from state s

Optimal Value:
    V*(s) = max_Ï€ V^Ï€(s)
    
Optimal Policy:
    Ï€*(s) = argmax_a Q*(s, a)
```

---

## ğŸ“ Key Equations

### Value Function (State Value)

```
V^Ï€(s) = E_Ï€[râ‚œ + Î³V^Ï€(sâ‚œâ‚Šâ‚) | sâ‚œ = s]

       = Î£_a Ï€(a|s) Î£_s' P(s'|s,a) [R(s,a,s') + Î³V^Ï€(s')]
```

### Q-Function (Action Value)

```
Q^Ï€(s, a) = E_Ï€[râ‚œ + Î³V^Ï€(sâ‚œâ‚Šâ‚) | sâ‚œ = s, aâ‚œ = a]

          = Î£_s' P(s'|s,a) [R(s,a,s') + Î³V^Ï€(s')]
```

### Bellman Optimality Equations

```
V*(s) = max_a Î£_s' P(s'|s,a) [R(s,a,s') + Î³V*(s')]

Q*(s,a) = Î£_s' P(s'|s,a) [R(s,a,s') + Î³ max_a' Q*(s',a')]
```

---

## ğŸ’» Code: MDP Environment

```python
import numpy as np

class SimpleMDP:
    """A simple MDP implementation"""
    
    def __init__(self, n_states, n_actions):
        self.n_states = n_states
        self.n_actions = n_actions
        
        # Transition probabilities P(s'|s,a)
        self.P = np.random.rand(n_states, n_actions, n_states)
        self.P = self.P / self.P.sum(axis=2, keepdims=True)
        
        # Reward function R(s,a,s')
        self.R = np.random.randn(n_states, n_actions, n_states)
        
        self.gamma = 0.99
        self.state = 0
    
    def reset(self):
        self.state = np.random.randint(self.n_states)
        return self.state
    
    def step(self, action):

        # Sample next state from P(s'|s,a)
        next_state = np.random.choice(
            self.n_states, 
            p=self.P[self.state, action]
        )
        
        # Get reward
        reward = self.R[self.state, action, next_state]
        
        self.state = next_state
        done = False  # Infinite horizon
        
        return next_state, reward, done, {}

def value_iteration(mdp, theta=1e-6):
    """Solve MDP using Value Iteration"""
    V = np.zeros(mdp.n_states)
    
    while True:
        delta = 0
        for s in range(mdp.n_states):
            v = V[s]

            # Bellman optimality update
            V[s] = max(
                sum(mdp.P[s,a,s_next] * (mdp.R[s,a,s_next] + mdp.gamma * V[s_next])
                    for s_next in range(mdp.n_states))
                for a in range(mdp.n_actions)
            )
            delta = max(delta, abs(v - V[s]))
        
        if delta < theta:
            break
    
    # Extract optimal policy
    policy = np.zeros(mdp.n_states, dtype=int)
    for s in range(mdp.n_states):
        policy[s] = np.argmax([
            sum(mdp.P[s,a,s_next] * (mdp.R[s,a,s_next] + mdp.gamma * V[s_next])
                for s_next in range(mdp.n_states))
            for a in range(mdp.n_actions)
        ])
    
    return V, policy
```

---

## ğŸ”— Where This Topic Is Used

| Topic | How MDP Is Used |
|-------|-----------------|
| **Q-Learning** | Learns Q(s,a) values for MDP |
| **Policy Gradient** | Optimizes Ï€(a\|s) for MDP |
| **PPO / TRPO** | Policy optimization with MDP structure |
| **RLHF** | LLM as policy, human feedback as reward |
| **DPO** | Implicit MDP formulation |
| **AlphaGo / AlphaZero** | Game as MDP, MCTS planning |
| **Robotics Control** | Physical system as MDP |
| **Recommender Systems** | User session as MDP |
| **Autonomous Driving** | Driving as continuous MDP |
| **World Models** | Learn MDP dynamics P(s'\|s,a) |

### Prerequisite For

```
MDP --> Value Methods (Q-learning, DQN)
    --> Policy Methods (PPO, TRPO)
    --> Model-Based RL (Dreamer)
    --> RLHF / DPO
    --> Multi-Agent RL
```

---

## ğŸ“Š MDP Variants

| Variant | Modification | Use Case |
|---------|--------------|----------|
| **POMDP** | Partial observability | Real-world perception |
| **Continuous MDP** | Continuous S, A | Robotics |
| **Semi-MDP** | Variable duration actions | Options framework |
| **Multi-Agent** | Multiple agents | Games, economics |
| **Constrained MDP** | Safety constraints | Autonomous systems |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Sutton & Barto Ch. 3 | [RL Book](http://incompleteideas.net/book/) |
| ğŸ“„ | Bellman 1957 | [Dynamic Programming](https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming) |
| ğŸ¥ | David Silver Lecture 2 | [YouTube](https://www.youtube.com/watch?v=lfHX2hHRMVQ) |
| ğŸ‡¨ğŸ‡³ | é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/35261164) |
| ğŸ‡¨ğŸ‡³ | MDPä¸Bellmanæ–¹ç¨‹ | [CSDN](https://blog.csdn.net/qq_30615903/article/details/80686611) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ -MDP | [Bç«™](https://www.bilibili.com/video/BV1sd4y167NS) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ å…¥é—¨MDP | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2018-02-13-4)

---

â¬…ï¸ [Back: Reinforcement Learning](../) | â¡ï¸ [Next: Value Methods](../02_value_methods/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
