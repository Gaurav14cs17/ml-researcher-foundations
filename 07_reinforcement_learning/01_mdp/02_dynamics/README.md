<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Transition%20Dynamics&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Discounting](../01_discounting/) | â¡ï¸ [Next: Rewards](../03_rewards/)

---

## ğŸ¯ Visual Overview

<img src="./images/transition-dynamics.svg" width="100%">

*Caption: Transition dynamics P(s'|s,a) define the probability of reaching state s' when taking action a in state s. Dynamics can be deterministic (same outcome) or stochastic (random outcomes). The Markov property means transitions only depend on current state.*

---

## ğŸ“‚ Overview

Transition dynamics define how the environment evolves. They are essential for model-based RL and planning.

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **P(s'\|s,a)** | Probability of next state given current state and action |
| **Deterministic** | P(s'\|s,a) = 1 for exactly one s' |
| **Stochastic** | Distribution over multiple next states |
| **Markov Property** | P(s'\|s,a) = P(s'\|sâ‚€,aâ‚€,...,s,a) |

---

## ğŸ“ Mathematical Definition

### Transition Probability Function

```
P: S Ã— A Ã— S â†’ [0, 1]

P(s'|s, a) = Pr(S_{t+1} = s' | S_t = s, A_t = a)

"Probability of transitioning to state s' given current state s and action a"
```

### Formal Properties

```
1. Non-negativity: 
   P(s'|s,a) â‰¥ 0  âˆ€s âˆˆ S, a âˆˆ A, s' âˆˆ S

2. Normalization (probability distribution):
   Î£_{s' âˆˆ S} P(s'|s,a) = 1  âˆ€s âˆˆ S, a âˆˆ A

3. Markov Property:
   P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1}|S_t, A_t)
   
   "The future depends only on the present, not the past"
```

---

## ğŸ“ The Markov Property: Proof of Importance

### Theorem: Markov Property Enables Recursive Value Computation

```
Claim: If P satisfies the Markov property, then:
  V^Ï€(s) = Î£_a Ï€(a|s) [R(s,a) + Î³ Î£_{s'} P(s'|s,a) V^Ï€(s')]

Proof:
  V^Ï€(s) = E_Ï€[G_t | S_t = s]
         = E_Ï€[R_{t+1} + Î³G_{t+1} | S_t = s]
         = E_Ï€[R_{t+1} | S_t = s] + Î³ E_Ï€[G_{t+1} | S_t = s]
         
  By Markov property, G_{t+1} only depends on S_{t+1}:
         = E_Ï€[R_{t+1} | S_t = s] + Î³ E_Ï€[E_Ï€[G_{t+1} | S_{t+1}] | S_t = s]
         = E_Ï€[R_{t+1} | S_t = s] + Î³ E_Ï€[V^Ï€(S_{t+1}) | S_t = s]
         
  Expanding expectations:
         = Î£_a Ï€(a|s) R(s,a) + Î³ Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a) V^Ï€(s')
         = Î£_a Ï€(a|s) [R(s,a) + Î³ Î£_{s'} P(s'|s,a) V^Ï€(s')]  âˆ
```

### Why Markov Property Matters

```
Without Markov property:
  V(s) would depend on entire history h = (s_0, a_0, s_1, a_1, ..., s_t)
  State space becomes exponentially large: |S|^t possibilities
  
With Markov property:
  V(s) only depends on current state s
  State space is fixed: |S| states
  Enables tractable algorithms (DP, TD, etc.)
```

---

## ğŸ“ Transition Matrix Representation

### For Finite MDPs

```
For fixed policy Ï€, define transition matrix P^Ï€:

P^Ï€[i,j] = Î£_a Ï€(a|s_i) P(s_j|s_i, a)

This is a stochastic matrix (rows sum to 1).

Value function satisfies:
  V^Ï€ = R^Ï€ + Î³ P^Ï€ V^Ï€
  
Solving:
  V^Ï€ = (I - Î³P^Ï€)^{-1} R^Ï€

Where R^Ï€[i] = Î£_a Ï€(a|s_i) R(s_i, a)
```

### Eigenvalue Analysis

```
Theorem: P^Ï€ has eigenvalue 1 with eigenvector 1 (all ones).

Proof: 
  (P^Ï€)áµ€ 1 = 1  (columns of Páµ€ sum to 1)
  So 1 is eigenvalue of (P^Ï€)áµ€, hence of P^Ï€.

Consequence: 
  Stationary distribution d^Ï€ exists where (P^Ï€)áµ€ d^Ï€ = d^Ï€
  This is the long-run state distribution under policy Ï€.
```

---

## ğŸ“ Deterministic vs Stochastic Dynamics

### Deterministic Dynamics

```
P(s'|s,a) âˆˆ {0, 1}  for all s, a, s'

Transition function f: S Ã— A â†’ S
  s' = f(s, a)

Examples:
  â€¢ Chess, Go (game rules)
  â€¢ Idealized physics simulations
  â€¢ Deterministic control systems
```

### Stochastic Dynamics

```
P(s'|s,a) âˆˆ [0, 1]  (non-trivial distribution)

Examples:
  â€¢ Real-world robotics (noise, uncertainty)
  â€¢ Games with chance elements
  â€¢ Market dynamics
  
Modeling: Often use Gaussian transitions
  s' ~ N(f(s,a), Î£(s,a))
```

---

## ğŸ“ Learning Dynamics (Model-Based RL)

### Maximum Likelihood Estimation

```
Given dataset D = {(s_i, a_i, s'_i)}_{i=1}^N

MLE estimate:
  PÌ‚(s'|s,a) = Count(s,a,s') / Count(s,a)
  
  Where:
    Count(s,a,s') = Î£_i ğŸ™[s_i=s, a_i=a, s'_i=s']
    Count(s,a) = Î£_i ğŸ™[s_i=s, a_i=a]
```

### Neural Network Dynamics Model

```
Learn f_Î¸: S Ã— A â†’ S (deterministic)
  or p_Î¸(s'|s,a) (probabilistic)

Loss function:
  L(Î¸) = E_{(s,a,s')~D}[||s' - f_Î¸(s,a)||Â²]  (deterministic)
  L(Î¸) = -E_{(s,a,s')~D}[log p_Î¸(s'|s,a)]   (probabilistic)
```

---

## ğŸŒ Known vs Unknown Dynamics

| Known (Model-Based) | Unknown (Model-Free) |
|---------------------|---------------------|
| Can plan ahead | Must learn from experience |
| Simulate trajectories | Trial and error |
| More sample efficient | More general |
| Games, physics sims | Real world |

---

## ğŸ’» Code

```python
# Deterministic dynamics (simple example)
def transition_deterministic(state, action):
    if action == "right":
        return state + 1
    elif action == "left":
        return state - 1
    return state

# Stochastic dynamics
def transition_stochastic(state, action):
    """Action succeeds 80% of time, fails 20%"""
    if np.random.rand() < 0.8:
        return intended_next_state(state, action)
    else:
        return random_adjacent_state(state)
```

## ğŸ”— Where This Topic Is Used

| Application | Dynamics Model |
|-------------|---------------|
| **Model-Based RL** | Learned transition model |
| **Planning** | Simulator for lookahead |
| **Robotics** | Physics simulation |
| **Games** | Game rules as transitions |

## ğŸ“š References

| Type | Resource | Link |
|------|----------|------|
| ğŸ“– | Textbook | See parent folder |
| ğŸ¥ | Video Lectures | YouTube/Coursera |

---

â¬…ï¸ [Back: Discounting](../01_discounting/) | â¡ï¸ [Next: Rewards](../03_rewards/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
