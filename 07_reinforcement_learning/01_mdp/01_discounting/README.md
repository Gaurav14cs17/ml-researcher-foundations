<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Discount%20Factor%20Î³&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: MDP](../) | â¡ï¸ [Next: Dynamics](../02_dynamics/)

---

## ğŸ¯ Visual Overview

<img src="./images/discount-factor.svg" width="100%">

*Caption: The discount factor Î³ âˆˆ [0,1] determines how much we value future rewards. With Î³=0.9, a reward 10 steps away is worth only 0.35 of its face value today. Higher Î³ = more far-sighted agent.*

---

## ğŸ“‚ Overview

The discount factor Î³ controls the agent's "patience" - how much it values future rewards compared to immediate ones. It is one of the most important hyperparameters in RL.

---

## ğŸ“ Mathematical Definition

### Discounted Return

```
G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...
    = Î£_{k=0}^âˆ Î³^k R_{t+k+1}

Where Î³ âˆˆ [0, 1] is the discount factor.

```

### Recursive Definition

```
G_t = R_{t+1} + Î³G_{t+1}

This recursive structure is the basis for TD learning!

```

---

## ğŸ“ Why Discount?

### 1. Mathematical Necessity

```
For infinite horizon MDPs, we need the return to be finite:

If |R_t| â‰¤ R_max for all t, then:

|G_t| â‰¤ Î£_{k=0}^âˆ Î³^k R_max = R_max / (1-Î³)

This is finite only if Î³ < 1!

```

### 2. Economic Interpretation

```
"A dollar today is worth more than a dollar tomorrow"

Reasons:
â€¢ Uncertainty about the future
â€¢ Opportunity cost of waiting
â€¢ Preference for immediate gratification
â€¢ Risk of episode termination

```

### 3. Computational Properties

```
Discounting makes value functions well-defined:

V(s) = E[Î£_{k=0}^âˆ Î³^k R_{t+k}]

Without discounting (Î³=1), this sum may diverge for non-episodic tasks.

```

---

## ğŸ“ Effective Horizon

The discount factor determines how far ahead the agent "looks":

```
Effective Horizon = 1 / (1 - Î³)

Examples:
â€¢ Î³ = 0.9   â†’ Horizon â‰ˆ 10 steps
â€¢ Î³ = 0.99  â†’ Horizon â‰ˆ 100 steps
â€¢ Î³ = 0.999 â†’ Horizon â‰ˆ 1000 steps

Derivation:
Weight at step k: Î³^k
Total weight: Î£_{k=0}^âˆ Î³^k = 1/(1-Î³)
Half of weight is in first 1/(1-Î³) steps

```

### Present Value Analysis

```
What is the present value of a future reward?

Reward r at time t+k has present value: Î³^k Â· r

Examples (Î³ = 0.9):
â€¢ k=0:  Î³^0 = 1.00  (100% value)
â€¢ k=5:  Î³^5 = 0.59  (59% value)
â€¢ k=10: Î³^10 = 0.35 (35% value)
â€¢ k=20: Î³^20 = 0.12 (12% value)

```

---

## ğŸ“ Effect on Bellman Equations

### Bellman Expectation Equation

```
V^Ï€(s) = E_Ï€[R_{t+1} + Î³V^Ï€(S_{t+1}) | S_t = s]
       = Î£_a Ï€(a|s) [R(s,a) + Î³ Î£_{s'} P(s'|s,a) V^Ï€(s')]

The Î³ determines how much future values contribute.

```

### Bellman Optimality Equation

```
V*(s) = max_a [R(s,a) + Î³ Î£_{s'} P(s'|s,a) V*(s')]

Smaller Î³ â†’ Optimization focuses on immediate rewards
Larger Î³ â†’ Optimization considers long-term consequences

```

---

## ğŸ“ Î³ and TD Learning

### TD(0) Update

```
V(s) â† V(s) + Î±[R + Î³V(s') - V(s)]

The TD target is: R + Î³V(s')
                  +-- Future value is discounted

```

### TD Error

```
Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)

This is the "surprise" - difference between expected and actual.

```

### n-Step Returns

```
G_t:t+n = R_{t+1} + Î³R_{t+2} + ... + Î³^{n-1}R_{t+n} + Î³^n V(S_{t+n})

Each future reward is discounted by its distance from present.

```

---

## ğŸ“ Choosing Î³

### Guidelines

| Î³ Value | Behavior | Use Case |
|---------|----------|----------|
| **Î³ = 0** | Myopic, greedy | Bandits, immediate reward |
| **Î³ = 0.9** | Short-sighted | Short episodes (~10 steps) |
| **Î³ = 0.99** | Far-sighted | Medium episodes (~100 steps) |
| **Î³ = 0.999** | Very patient | Long episodes (~1000 steps) |
| **Î³ = 1** | Undiscounted | Episodic with guaranteed termination |

### Practical Considerations

```
Start with Î³ = 0.99 for most problems.

Decrease Î³ if:
â€¢ Training is unstable (values too large)
â€¢ Agent focuses too much on distant rewards
â€¢ Episode length is short

Increase Î³ if:
â€¢ Agent is too myopic
â€¢ Important rewards are delayed
â€¢ Long-term planning is needed

```

---

## ğŸ“ Î³ in Policy Gradient Methods

### Return Estimation

```
In REINFORCE:
G_t = Î£_{k=0}^{T-t-1} Î³^k R_{t+k+1}

Gradient estimate:
âˆ‡J(Î¸) â‰ˆ Î£_t âˆ‡log Ï€_Î¸(a_t|s_t) Â· G_t

```

### Generalized Advantage Estimation (GAE)

```
GAE uses both Î³ and Î»:

Ã‚^GAE_t = Î£_{l=0}^âˆ (Î³Î»)^l Î´_{t+l}

Where Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)

â€¢ Î³: Standard discount for future rewards
â€¢ Î»: Controls bias-variance tradeoff in advantage

```

---

## ğŸ’» Code Examples

```python
import numpy as np
import matplotlib.pyplot as plt

def compute_returns(rewards, gamma=0.99):
    """
    Compute discounted returns G_t = Î£ Î³^k r_{t+k}
    
    Uses reverse iteration for efficiency:
    G_T = r_T
    G_t = r_t + Î³ * G_{t+1}
    """
    T = len(rewards)
    returns = np.zeros(T)
    G = 0
    
    for t in reversed(range(T)):
        G = rewards[t] + gamma * G
        returns[t] = G
    
    return returns

def discount_factor_visualization():
    """Visualize effect of different gamma values"""
    gammas = [0.0, 0.5, 0.9, 0.99, 1.0]
    steps = np.arange(50)
    
    plt.figure(figsize=(10, 6))
    for gamma in gammas:
        weights = gamma ** steps
        plt.plot(steps, weights, label=f'Î³={gamma}')
    
    plt.xlabel('Steps into future')
    plt.ylabel('Discount weight Î³^k')
    plt.title('Present Value of Future Rewards')
    plt.legend()
    plt.grid(True)
    plt.show()

def effective_horizon(gamma):
    """Compute effective planning horizon"""
    return 1 / (1 - gamma) if gamma < 1 else float('inf')

# Example: Impact of gamma on returns
rewards = [1, 1, 1, 1, 1, 1, 1, 1, 1, 10]  # Delayed large reward

print("Returns with different discount factors:")
for gamma in [0.0, 0.5, 0.9, 0.99]:
    returns = compute_returns(rewards, gamma)
    print(f"Î³={gamma:0.2f}: G_0={returns[0]:7.2f}, horizon={effective_horizon(gamma):5.1f}")

# Output:
# Î³=0.00: G_0=   1.00, horizon=  1.0
# Î³=0.50: G_0=   6.99, horizon=  2.0
# Î³=0.90: G_0=  12.41, horizon= 10.0
# Î³=0.99: G_0=  18.56, horizon=100.0

class DiscountedMDP:
    """MDP with configurable discount factor"""
    
    def __init__(self, gamma=0.99):
        self.gamma = gamma
    
    def td_target(self, reward, next_value, done):
        """Compute TD target: r + Î³V(s')"""
        if done:
            return reward
        return reward + self.gamma * next_value
    
    def n_step_return(self, rewards, final_value, dones):
        """Compute n-step return"""
        G = final_value
        for t in reversed(range(len(rewards))):
            if dones[t]:
                G = rewards[t]
            else:
                G = rewards[t] + self.gamma * G
        return G
    
    def gae(self, rewards, values, dones, lam=0.95):
        """
        Generalized Advantage Estimation
        Uses both Î³ (reward discount) and Î» (trace decay)
        """
        T = len(rewards)
        advantages = np.zeros(T)
        gae = 0
        
        for t in reversed(range(T)):
            if t == T - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * lam * (1 - dones[t]) * gae
            advantages[t] = gae
        
        return advantages

```

---

## ğŸ“Š Summary Table

| Concept | Formula | Interpretation |
|---------|---------|----------------|
| Return | G_t = Î£ Î³^k R_{t+k} | Sum of discounted rewards |
| Present value | Î³^k Â· r | Value today of reward at step k |
| Effective horizon | 1/(1-Î³) | How far agent looks ahead |
| TD target | r + Î³V(s') | Bootstrap estimate |
| GAE | Î£ (Î³Î»)^l Î´_{t+l} | Advantage estimate |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Sutton & Barto Ch. 3 | [RL Book](http://incompleteideas.net/book/) |
| ğŸ¥ | David Silver Lecture 2 | [YouTube](https://www.youtube.com/watch?v=lfHX2hHRMVQ) |
| ğŸ‡¨ğŸ‡³ | æŠ˜æ‰£å› å­è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/35261164) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ åŸºç¡€ | [Bç«™](https://www.bilibili.com/video/BV1sd4y167NS) |

## ğŸ”— Where This Topic Is Used

| Application | Discount Factor |
|-------------|----------------|
| **Finance** | Time value of money |
| **Long-horizon tasks** | Î³ close to 1 |
| **Short-term focus** | Î³ close to 0 |
| **Infinite horizon** | Ensures convergence |
| **GAE (PPO)** | Both Î³ and Î» |

---

â¬…ï¸ [Back: MDP](../) | â¡ï¸ [Next: Dynamics](../02_dynamics/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
