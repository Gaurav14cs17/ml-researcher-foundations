<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Robotics&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: RLHF](../02_rlhf/) | â¡ï¸ [Back: Applications](../)

---

## ğŸ¯ Visual Overview

<img src="./images/robotics.svg" width="100%">

*Caption: RL for robotics faces unique challenges: sample efficiency (real robots are slow), safety, and sim-to-real gap. Solutions include simulation training, domain randomization, and model-based RL.*

---

## ğŸ“‚ Overview

Robotics is a compelling application of RL where agents must learn to control physical systems in the real world. The challenges of sample efficiency and safety have driven innovations in sim-to-real transfer and model-based methods.

---

## ğŸ¤– Robotics RL Challenges

| Challenge | Problem | Solution |
|-----------|---------|----------|
| **Sample Efficiency** | Real robots are slow | Sim-to-real, model-based |
| **Safety** | Damage from exploration | Constrained RL, safe policies |
| **Sim-to-Real Gap** | Sim â‰  real physics | Domain randomization |
| **Partial Observability** | Noisy, limited sensors | Recurrent policies, state estimation |
| **Continuous Actions** | Joint angles, forces | PPO, SAC, DDPG |

---

## ğŸ“ Mathematical Foundations

### Continuous Control Formulation

```
Robot state: s âˆˆ â„â¿ (joint angles, velocities, etc.)
Action: a âˆˆ â„áµ (torques, target positions, etc.)
Dynamics: s_{t+1} = f(s_t, a_t) + Îµ  (deterministic + noise)

Objective: max_Ï€ E[Î£_t Î³^t R(s_t, a_t)]

Common rewards:
  R_task = -||s - s_goal||Â²     (reach target)
  R_energy = -||a||Â²            (minimize effort)
  R_safety = -1{s âˆˆ S_unsafe}   (safety constraint)

```

### Sim-to-Real Transfer

```
Domain Randomization formulation:

Source domain (sim): p_sim(s'|s,a,Î¾) where Î¾ ~ P(Î¾)
  Î¾ = randomization parameters (mass, friction, etc.)

Target domain (real): p_real(s'|s,a)

Goal: Find Ï€* such that:
  Ï€* = argmax_Ï€ E_{Î¾~P(Î¾)} E_Ï„~Ï€,p_sim(Â·|Î¾) [R(Ï„)]
  
If P(Î¾) is broad enough, Ï€* generalizes to real:
  E_Ï„~Ï€*,p_real [R(Ï„)] â‰ˆ E_Ï„~Ï€*,p_sim [R(Ï„)]

```

### System Identification vs Domain Randomization

```
System Identification:
  1. Collect real data: D_real = {(s, a, s')}
  2. Fit sim params: Î¾* = argmin_Î¾ ||f_sim(s,a;Î¾) - s'||
  3. Train in calibrated sim: Ï€* from p_sim(Â·|Î¾*)
  
  Pro: Accurate sim
  Con: Requires real data, may overfit

Domain Randomization:
  1. Define distribution P(Î¾) over params
  2. Train on diverse sims: Ï€* from E_Î¾[p_sim(Â·|Î¾)]
  
  Pro: No real data needed
  Con: May be overly conservative

```

### Safe Reinforcement Learning

```
Constrained MDP formulation:

max_Ï€ E[Î£_t Î³^t R(s_t, a_t)]
s.t. E[Î£_t Î³^t C(s_t, a_t)] â‰¤ d

Where:
  C(s,a) = cost function (e.g., collision indicator)
  d = maximum allowed cumulative cost

Solution approaches:
  1. Lagrangian relaxation: L = R - Î»(C - d)
  2. Constrained Policy Optimization (CPO)
  3. Safety layers: project actions to safe set

```

### Model-Based RL for Sample Efficiency

```
MBPO (Model-Based Policy Optimization):

1. Collect real data: D_real â† {(s,a,r,s')}
2. Train dynamics ensemble: {f_Î¸â‚, ..., f_Î¸_K}
3. Generate synthetic data:
   For k = 1 to K_rollouts:
     Sample model f ~ {f_Î¸â‚, ..., f_Î¸_K}
     Rollout H steps: D_model â† trajectory
4. Train policy on D_real âˆª D_model using SAC

Rollout horizon H matters:
  Small H: Less model error, more real data needed
  Large H: More synthetic data, but errors compound
  
  Optimal H â‰ˆ log(1/Îµ) / log(1/Î³) where Îµ = model error

```

---

## ğŸ’» Code Examples

```python
import gymnasium as gym
import torch

# MuJoCo continuous control
env = gym.make('Ant-v4')
obs, info = env.reset()

# SAC agent for continuous actions
class SACAgent:
    def __init__(self, obs_dim, action_dim):
        self.actor = GaussianPolicy(obs_dim, action_dim)
        self.critic = QNetwork(obs_dim, action_dim)
    
    def act(self, obs, deterministic=False):
        with torch.no_grad():
            action, _ = self.actor.sample(obs, deterministic)
        return action.cpu().numpy()

# Domain randomization
class RandomizedEnv(gym.Wrapper):
    def reset(self):
        # Randomize physics
        self.env.model.body_mass *= np.random.uniform(0.8, 1.2)
        self.env.model.dof_damping *= np.random.uniform(0.5, 2.0)
        return self.env.reset()

# Sim-to-real training
def train_sim2real(agent, sim_env, real_env):
    # Train in randomized sim
    for _ in range(1000000):
        sim_step(agent, RandomizedEnv(sim_env))
    
    # Fine-tune on real (few samples)
    for _ in range(100):
        real_step(agent, real_env)

```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | Sim-to-Real Survey | [arXiv](https://arxiv.org/abs/2009.13303) |
| ğŸ“„ | OpenAI Rubik's Cube | [arXiv](https://arxiv.org/abs/1910.07113) |
| ğŸ“„ | SAC Paper | [arXiv](https://arxiv.org/abs/1801.01290) |
| ğŸ‡¨ğŸ‡³ | æœºå™¨äººå¼ºåŒ–å­¦ä¹  | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/563656219) |
| ğŸ‡¨ğŸ‡³ | Sim-to-Realè¯¦è§£ | [CSDN](https://blog.csdn.net/qq_37006625/article/details/123629543) |
| ğŸ‡¨ğŸ‡³ | æœºå™¨äººæ§åˆ¶ | [Bç«™](https://www.bilibili.com/video/BV1yp4y1s7Qw) |

---

## ğŸ”— Where This Topic Is Used

| Application | RL in Robotics |
|-------------|---------------|
| **Manipulation** | Dexterous hands |
| **Locomotion** | Quadruped walking |
| **Navigation** | Autonomous driving |
| **Sim-to-Real** | Domain adaptation |

---

â¬…ï¸ [Back: RLHF](../02_rlhf/) | â¡ï¸ [Back: Applications](../)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
