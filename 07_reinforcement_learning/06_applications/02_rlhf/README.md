<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=RLHF%20-%20Reinforcement%20Learning%20from%20Human%20Feedback&fontSize=28&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: Games](../01_games/) | â¡ï¸ [Next: Robotics](../03_robotics/)

---

## ğŸ¯ Visual Overview

<img src="./images/rlhf-pipeline.svg" width="100%">

*Caption: The RLHF pipeline showing three stages: (1) Supervised Fine-Tuning (SFT) on demonstrations, (2) Reward Model training from human preferences, and (3) RL optimization using PPO to maximize reward while staying close to the SFT policy. This is how ChatGPT and Claude are aligned.*

---

## ğŸ“‚ Overview

Reinforcement Learning from Human Feedback (RLHF) is the technique that transforms base language models into helpful, harmless, and honest AI assistants. It captures human preferences that are difficult to specify programmatically.

---

## ğŸ“ Mathematical Framework

### The RLHF Optimization Problem

The goal is to find a policy Ï€ that maximizes human preference while staying close to a reference policy:

```
max_Ï€  E_{x~D, y~Ï€(Â·|x)} [r(x, y)]  -  Î² Â· KL(Ï€ || Ï€_ref)
       ----------------------------    ---------------------
         Maximize learned reward        Stay close to SFT policy

Where:
â€¢ x: Input prompt
â€¢ y: Generated response
â€¢ r(x, y): Learned reward model
â€¢ Ï€_ref: Reference policy (typically SFT model)
â€¢ Î²: KL penalty coefficient (typically 0.01-0.1)
```

---

## ğŸ“ Stage 1: Supervised Fine-Tuning (SFT)

### Objective

Train on high-quality demonstrations to create a base policy:

```
L_SFT(Î¸) = -E_{(x,y)~D_demo} [log Ï€_Î¸(y|x)]
         = -E [Î£_t log Ï€_Î¸(y_t | x, y_{1:t-1})]

This is standard language modeling on curated data.
```

### Purpose

```
Pretrained LLM (knows language, not task)
         â†“ SFT
SFT Model (understands instruction format, basic helpfulness)
         â†“ RLHF
Aligned Model (optimized for human preferences)
```

---

## ğŸ“ Stage 2: Reward Model Training

### The Bradley-Terry Model

Human preferences are modeled as pairwise comparisons:

```
P(y_w â‰» y_l | x) = Ïƒ(r(x, y_w) - r(x, y_l))
                 = exp(r(x, y_w)) / (exp(r(x, y_w)) + exp(r(x, y_l)))

Where:
â€¢ y_w: Preferred (winning) response
â€¢ y_l: Dispreferred (losing) response
â€¢ Ïƒ: Sigmoid function
â€¢ r(x, y): Scalar reward for response y given prompt x
```

### Maximum Likelihood Estimation

Given preference dataset D = {(x^i, y_w^i, y_l^i)}:

```
L_RM(Ï†) = -E_{(x,y_w,y_l)~D} [log Ïƒ(r_Ï†(x, y_w) - r_Ï†(x, y_l))]

Gradient:
âˆ‡_Ï† L_RM = -E[(1 - P(y_w â‰» y_l)) Â· (âˆ‡_Ï† r(x, y_w) - âˆ‡_Ï† r(x, y_l))]
```

### Proof of Bradley-Terry MLE

```
The likelihood of observing preference y_w â‰» y_l:
L = P(y_w â‰» y_l) = Ïƒ(r(y_w) - r(y_l))

Log-likelihood:
log L = log Ïƒ(r(y_w) - r(y_l))
      = r(y_w) - r(y_l) - log(1 + exp(r(y_w) - r(y_l)))

This is the negative cross-entropy loss for binary classification.
```

### Reward Model Architecture

```
Reward Model = LLM backbone + Linear head

r(x, y) = W Â· h_final(x, y) + b

Where:
â€¢ h_final: Last token's hidden state from transformer
â€¢ W âˆˆ R^{1Ã—d}: Learned projection
â€¢ b âˆˆ R: Bias term
```

---

## ğŸ“ Stage 3: RL Fine-Tuning with PPO

### RLHF Objective with KL Penalty

```
J(Î¸) = E_{x~D, y~Ï€_Î¸} [r(x, y) - Î² Â· log(Ï€_Î¸(y|x) / Ï€_ref(y|x))]

Equivalent form:
J(Î¸) = E[r(x, y)] - Î² Â· KL(Ï€_Î¸(Â·|x) || Ï€_ref(Â·|x))

The KL term prevents:
â€¢ Reward hacking
â€¢ Catastrophic forgetting
â€¢ Collapse to degenerate solutions
```

### Per-Token KL Penalty

In practice, KL is computed per-token:

```
KL(Ï€_Î¸ || Ï€_ref) = Î£_t E[log Ï€_Î¸(y_t|x,y_{1:t-1}) - log Ï€_ref(y_t|x,y_{1:t-1})]

Modified reward at each step:
r'_t = r_RM(x, y) Â· ğŸ™[t = T] - Î² Â· (log Ï€_Î¸(y_t) - log Ï€_ref(y_t))
                  +- reward at end -+   +----- KL penalty at each step -----+
```

### PPO Clipped Objective for RLHF

```
L^CLIP(Î¸) = E_t [min(Ï_t Â· A_t, clip(Ï_t, 1-Îµ, 1+Îµ) Â· A_t)]

Where:
â€¢ Ï_t = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  (probability ratio)
â€¢ A_t = GAE advantage estimate
â€¢ Îµ = 0.2 (typical clipping parameter)
```

### Advantage Estimation for LLMs

```
A_t^GAE = Î£_{l=0}^{T-t} (Î³Î»)^l Î´_{t+l}

Where:
Î´_t = r'_t + Î³ Â· V(s_{t+1}) - V(s_t)
    = [r_RM Â· ğŸ™[t=T] - Î²Â·KL_t] + Î³ Â· V(s_{t+1}) - V(s_t)
```

---

## ğŸ“ Direct Preference Optimization (DPO)

### Key Insight: Closed-Form Optimal Policy

Under the RLHF objective, the optimal policy has a closed form:

```
Theorem: The optimal policy for the KL-regularized reward maximization is:

Ï€*(y|x) = Ï€_ref(y|x) Â· exp(r(x,y)/Î²) / Z(x)

Where Z(x) = Î£_y Ï€_ref(y|x) Â· exp(r(x,y)/Î²) is the partition function.

Proof:
The objective is: max_Ï€ E_y~Ï€[r(x,y)] - Î²Â·KL(Ï€||Ï€_ref)

Taking the functional derivative and setting to zero:
âˆ‚/âˆ‚Ï€(y) [Î£_y Ï€(y)(r(y) - Î² log(Ï€(y)/Ï€_ref(y))) + Î»(Î£_y Ï€(y) - 1)] = 0

r(y) - Î² log(Ï€(y)/Ï€_ref(y)) - Î² + Î» = 0
Ï€(y) = Ï€_ref(y) Â· exp((r(y) + Î» - Î²)/Î²)

Normalizing: Ï€*(y) = Ï€_ref(y) Â· exp(r(y)/Î²) / Z
```

### Rearranging for Implicit Reward

```
From Ï€*(y|x) = Ï€_ref(y|x) Â· exp(r(x,y)/Î²) / Z(x):

r(x, y) = Î² Â· log(Ï€*(y|x) / Ï€_ref(y|x)) + Î² Â· log Z(x)

Key insight: The reward is implicitly defined by the policy!
```

### DPO Loss Function

Substituting the implicit reward into Bradley-Terry:

```
P(y_w â‰» y_l | x) = Ïƒ(r(x, y_w) - r(x, y_l))
                 = Ïƒ(Î² log(Ï€*(y_w|x)/Ï€_ref(y_w|x)) - Î² log(Ï€*(y_l|x)/Ï€_ref(y_l|x)))

The log Z(x) terms cancel!

DPO Loss:
L_DPO(Î¸) = -E_{(x,y_w,y_l)~D} [log Ïƒ(Î² Â· (log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) 
                                        - log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x))))]
```

### Gradient of DPO

```
âˆ‡_Î¸ L_DPO = -Î² Â· E[(1 - Ïƒ(Î²Â·Î”)) Â· (âˆ‡_Î¸ log Ï€_Î¸(y_w|x) - âˆ‡_Î¸ log Ï€_Î¸(y_l|x))]

Where Î” = log(Ï€_Î¸(y_w)/Ï€_ref(y_w)) - log(Ï€_Î¸(y_l)/Ï€_ref(y_l))

Intuition:
â€¢ Increase probability of preferred response y_w
â€¢ Decrease probability of dispreferred response y_l
â€¢ Weighted by how wrong the current model is (1 - Ïƒ(Î²Â·Î”))
```

---

## ğŸ’» Complete Implementation

### Reward Model Training

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

class RewardModel(nn.Module):
    """Reward model built on LLM backbone"""
    
    def __init__(self, model_name):
        super().__init__()
        self.backbone = AutoModelForCausalLM.from_pretrained(model_name)
        self.reward_head = nn.Linear(self.backbone.config.hidden_size, 1)
    
    def forward(self, input_ids, attention_mask):
        # Get last hidden state
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        hidden = outputs.hidden_states[-1]
        
        # Use last token (or EOS) representation
        last_token_idx = attention_mask.sum(dim=1) - 1
        last_hidden = hidden[torch.arange(hidden.size(0)), last_token_idx]
        
        # Scalar reward
        reward = self.reward_head(last_hidden).squeeze(-1)
        return reward

def train_reward_model(model, dataloader, optimizer, epochs=3):
    """Train reward model on preference pairs"""
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch in dataloader:
            # Get rewards for chosen and rejected
            r_chosen = model(batch['chosen_ids'], batch['chosen_mask'])
            r_rejected = model(batch['rejected_ids'], batch['rejected_mask'])
            
            # Bradley-Terry loss
            loss = -F.logsigmoid(r_chosen - r_rejected).mean()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            correct += (r_chosen > r_rejected).sum().item()
            total += len(r_chosen)
        
        accuracy = correct / total
        print(f"Epoch {epoch+1}: Loss={total_loss/len(dataloader):.4f}, Acc={accuracy:.4f}")
```

### PPO for RLHF

```python
class RLHFTrainer:
    """PPO trainer for RLHF"""
    
    def __init__(self, policy, ref_policy, reward_model, 
                 beta=0.1, clip_eps=0.2, gamma=1.0, gae_lambda=0.95):
        self.policy = policy
        self.ref_policy = ref_policy  # Frozen
        self.reward_model = reward_model  # Frozen
        
        self.beta = beta
        self.clip_eps = clip_eps
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        
        self.optimizer = torch.optim.Adam(policy.parameters(), lr=1e-5)
    
    def compute_rewards(self, prompts, responses):
        """Compute reward with KL penalty"""
        # Get reward model score
        with torch.no_grad():
            reward_scores = self.reward_model(prompts + responses)
        
        # Compute per-token KL
        with torch.no_grad():
            ref_logprobs = self.ref_policy.log_prob(responses, prompts)
        policy_logprobs = self.policy.log_prob(responses, prompts)
        
        kl = policy_logprobs - ref_logprobs
        
        # Per-token rewards: KL penalty + final reward
        rewards = -self.beta * kl
        rewards[:, -1] += reward_scores  # Add RM reward at end
        
        return rewards, kl.mean()
    
    def compute_advantages(self, rewards, values):
        """GAE advantage computation"""
        T = rewards.shape[1]
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for t in reversed(range(T)):
            if t == T - 1:
                next_value = 0
            else:
                next_value = values[:, t + 1]
            
            delta = rewards[:, t] + self.gamma * next_value - values[:, t]
            gae = delta + self.gamma * self.gae_lambda * gae
            advantages[:, t] = gae
        
        returns = advantages + values
        return advantages, returns
    
    def ppo_step(self, prompts, responses, old_logprobs, old_values):
        """Single PPO update step"""
        # Get current policy outputs
        logprobs = self.policy.log_prob(responses, prompts)
        values = self.policy.value(prompts, responses)
        
        # Compute rewards and advantages
        rewards, kl = self.compute_rewards(prompts, responses)
        advantages, returns = self.compute_advantages(rewards, old_values)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # Policy ratio
        ratio = torch.exp(logprobs - old_logprobs.detach())
        
        # Clipped surrogate objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # Value loss
        value_loss = F.mse_loss(values, returns.detach())
        
        # Total loss
        loss = policy_loss + 0.5 * value_loss
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'kl': kl.item(),
            'reward': rewards.mean().item()
        }
```

### DPO Implementation

```python
class DPOTrainer:
    """Direct Preference Optimization"""
    
    def __init__(self, model, ref_model, beta=0.1, lr=1e-6):
        self.model = model
        self.ref_model = ref_model  # Frozen
        self.beta = beta
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    
    def compute_log_probs(self, model, input_ids, attention_mask, labels):
        """Compute log probabilities for sequences"""
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits[:, :-1, :]  # Shift for next token prediction
        labels = labels[:, 1:]  # Shift labels
        
        log_probs = F.log_softmax(logits, dim=-1)
        selected_log_probs = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)
        
        # Mask padding
        mask = (labels != -100).float()
        return (selected_log_probs * mask).sum(-1) / mask.sum(-1)
    
    def dpo_loss(self, chosen_ids, chosen_mask, rejected_ids, rejected_mask):
        """Compute DPO loss"""
        # Policy log probs
        policy_chosen_logp = self.compute_log_probs(
            self.model, chosen_ids, chosen_mask, chosen_ids
        )
        policy_rejected_logp = self.compute_log_probs(
            self.model, rejected_ids, rejected_mask, rejected_ids
        )
        
        # Reference log probs (frozen)
        with torch.no_grad():
            ref_chosen_logp = self.compute_log_probs(
                self.ref_model, chosen_ids, chosen_mask, chosen_ids
            )
            ref_rejected_logp = self.compute_log_probs(
                self.ref_model, rejected_ids, rejected_mask, rejected_ids
            )
        
        # Log ratios
        chosen_ratio = policy_chosen_logp - ref_chosen_logp
        rejected_ratio = policy_rejected_logp - ref_rejected_logp
        
        # DPO loss
        logits = self.beta * (chosen_ratio - rejected_ratio)
        loss = -F.logsigmoid(logits).mean()
        
        return loss, {
            'chosen_reward': chosen_ratio.mean().item(),
            'rejected_reward': rejected_ratio.mean().item(),
            'accuracy': (logits > 0).float().mean().item()
        }
    
    def train_step(self, batch):
        """Single DPO training step"""
        loss, metrics = self.dpo_loss(
            batch['chosen_ids'], batch['chosen_mask'],
            batch['rejected_ids'], batch['rejected_mask']
        )
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        metrics['loss'] = loss.item()
        return metrics
```

---

## ğŸ“Š RLHF vs DPO Comparison

| Aspect | RLHF (PPO) | DPO |
|--------|------------|-----|
| **Reward Model** | Required (separate training) | Not needed |
| **RL Loop** | Yes (PPO optimization) | No (supervised learning) |
| **Sampling** | Online (during training) | Offline (precomputed) |
| **Stability** | Can be unstable | Very stable |
| **Memory** | High (value network, ref model) | Lower |
| **Compute** | High (sampling + optimization) | Lower |
| **Flexibility** | More flexible | Less flexible |

---

## ğŸ“Š Modern Variants

| Method | Key Innovation | Advantage |
|--------|----------------|-----------|
| **DPO** | No reward model needed | Simpler, more stable |
| **IPO** | Identity mapping | Better calibration |
| **KTO** | Only positive examples | Works with binary feedback |
| **ORPO** | Odds ratio | No reference model needed |
| **SimPO** | Length-normalized | Better for varying lengths |
| **RLAIF** | AI feedback | Scales without humans |

---

## ğŸŒ Where RLHF/DPO is Used

| Model | Year | Method | Notes |
|-------|------|--------|-------|
| **ChatGPT** | 2022 | RLHF (PPO) | Original RLHF for chat |
| **GPT-4** | 2023 | Advanced RLHF | Improved pipeline |
| **Claude** | 2023 | Constitutional AI | RLHF + AI feedback |
| **LLaMA-2** | 2023 | RLHF + DPO | Open-source RLHF |
| **Gemini** | 2023 | RLHF variants | Google's approach |
| **Mistral** | 2024 | DPO | Efficient alignment |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | InstructGPT (RLHF) | [arXiv](https://arxiv.org/abs/2203.02155) |
| ğŸ“„ | DPO Paper | [arXiv](https://arxiv.org/abs/2305.18290) |
| ğŸ“„ | Constitutional AI | [arXiv](https://arxiv.org/abs/2212.08073) |
| ğŸ“„ | PPO Paper | [arXiv](https://arxiv.org/abs/1707.06347) |
| ğŸ¥ | RLHF Explained | [HuggingFace Blog](https://huggingface.co/blog/rlhf) |
| ğŸ‡¨ğŸ‡³ | RLHFè¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/595579042) |
| ğŸ‡¨ğŸ‡³ | DPOç®—æ³•è¯¦è§£ | [CSDN](https://blog.csdn.net/qq_37006625/article/details/134567890) |
| ğŸ‡¨ğŸ‡³ | å¤§æ¨¡å‹å¯¹é½æŠ€æœ¯ | [Bç«™](https://www.bilibili.com/video/BV1cP4y1Y7DN) |

---

## ğŸ”— Training Pipeline Position

```
Pretraining â†’ SFT â†’ RLHF/DPO
(language)   (task) (preference)
                        â†‘
                   You are here!
```

---

â¬…ï¸ [Back: Games](../01_games/) | â¡ï¸ [Next: Robotics](../03_robotics/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
