<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Topic&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“Š Learning Path

```
ğŸš€ Start â†’ ğŸ“Š MDP â†’ ğŸ’° Value â†’ ğŸ¯ Policy â†’ ğŸ” Explore â†’ ğŸŒ Model-Based â†’ ğŸ”¥ RLHF â†’ ğŸ† Master

```

## ğŸ¯ What You'll Learn

> ğŸ’¡ How agents learn through **interaction with environments** - powers ChatGPT alignment!

<table>
<tr>
<td align="center">

### ğŸ’° Value Methods
Q-Learning, DQN

</td>
<td align="center">

### ğŸ¯ Policy Methods
PPO (Default!)

</td>
<td align="center">

### ğŸ”¥ RLHF
ChatGPT's secret

</td>
</tr>
</table>

---

## ğŸ“š Topics

### 1ï¸âƒ£ Markov Decision Process

<img src="https://img.shields.io/badge/Time-4_hours-blue?style=flat-square"/>

```
States â†’ Actions â†’ Rewards â†’ Transitions â†’ Policy â†’ Value

```

**Foundation:** States, Actions, Rewards, Bellman Equations

<a href="./01_mdp/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Start_Here-FF5722?style=for-the-badge" alt="Start"/></a>

---

### 2ï¸âƒ£ Value-Based Methods

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/>

```
DP â†’ Monte Carlo â†’ TD Learning â†’ Q-Learning â†’ DQN

```

**Core:** TD Learning, Q-Learning, SARSA, Deep Q-Network (Atari!)

<a href="./02_value_methods/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Learn_More-FF5722?style=for-the-badge" alt="Learn"/></a>

---

### 3ï¸âƒ£ Policy-Based Methods â­

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ”¥_PPO-critical?style=flat-square"/>

```
Policy Gradient â†’ REINFORCE â†’ Actor-Critic â†’ A3C â†’ PPO â†’ TRPO

```

> â­ **PPO is the default algorithm** - powers robotics and RLHF

<a href="./03_policy_methods/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Learn_More-FF5722?style=for-the-badge" alt="Learn"/></a>

---

### 4ï¸âƒ£ Exploration Strategies

<img src="https://img.shields.io/badge/Time-4_hours-blue?style=flat-square"/>

```
Îµ-Greedy â†’ UCB â†’ Thompson Sampling â†’ Curiosity â†’ Intrinsic Motivation

```

<a href="./04_exploration/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Learn_More-FF5722?style=for-the-badge" alt="Learn"/></a>

---

### 5ï¸âƒ£ Model-Based RL

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/>

```
Dynamics â†’ Planning â†’ MCTS â†’ World Models â†’ MuZero

```

**Core:** Planning, Monte Carlo Tree Search (AlphaGo!)

<a href="./05_model_based/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Learn_More-FF5722?style=for-the-badge" alt="Learn"/></a>

---

### 6ï¸âƒ£ Applications & RLHF ğŸ”¥ğŸ”¥ğŸ”¥

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ”¥_HOT-critical?style=flat-square"/>

```
Games â†’ Robotics â†’ LLM Alignment â†’ RLHF â†’ DPO â†’ ChatGPT

```

> ğŸ”¥ **RLHF powers ChatGPT** - Aligns AI with human values

| Technique | Used In |
|:---------:|---------|
| RLHF | ChatGPT, Claude |
| DPO | Faster alternative |
| PPO | Training backbone |

<a href="./06_applications/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Learn_More-FF5722?style=for-the-badge" alt="Learn"/></a>

---

## ğŸ”— Next Steps

```
ğŸ® RL â†’ ğŸ—œï¸ Compression â†’ âš¡ Efficient ML â†’ ğŸš€ Production

```

<p align="center">
  <a href="../06_deep_learning/README.md"><img src="https://img.shields.io/badge/â†_Prerequisites:_Deep_Learning-gray?style=for-the-badge" alt="Prev"/></a>
  <a href="../08_model_compression/README.md"><img src="https://img.shields.io/badge/Next:_Compression_â†’-00C853?style=for-the-badge" alt="Next"/></a>
</p>

---

## ğŸ“š Key Resources

| Type | Resource | Link |
|:----:|----------|------|
| ğŸ“˜ | RL: An Introduction | Sutton & Barto |
| ğŸ“ | David Silver RL Course | DeepMind |
| ğŸ“ | Berkeley CS285 | Deep RL |

---

## ğŸ—ºï¸ Quick Navigation

| Previous | Current | Next |
|:--------:|:-------:|:----:|
| [ğŸ§¬ Deep Learning](../06_deep_learning/README.md) | **ğŸ® RL** | [ğŸ—œï¸ Compression â†’](../08_model_compression/README.md) |

---

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
