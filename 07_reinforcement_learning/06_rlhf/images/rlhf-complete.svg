<?xml version="1.0" ?>
<ns0:svg xmlns:ns0="http://www.w3.org/2000/svg" viewBox="0.0 0.0 2475.0 2025.0">
  <ns0:defs>
    <ns0:marker id="arrowGreen7" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <ns0:path d="M0,0 L0,6 L9,3 z" fill="#2e7d32"/>
    </ns0:marker>
  </ns0:defs>
  <ns0:text x="550" y="30" font-size="22" font-weight="bold" text-anchor="middle" fill="#1a237e">
    RLHF: Aligning Language Models with Human Preferences
  </ns0:text>
  <ns0:g transform="translate(50, 80)">
    <ns0:text x="500" y="55.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#1565c0">
      1. What is RLHF?
    </ns0:text>
    <ns0:rect x="0" y="20" width="2250.0" height="270.0" fill="#f8f9fa" stroke="#007bff" stroke-width="3" rx="8"/>
    <ns0:g transform="translate(30, 50)">
      <ns0:text x="0" y="80.0" font-size="14" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#1565c0">Goal:</ns0:tspan>
        Train LLMs to generate helpful, harmless, and honest outputs aligned with human values
      </ns0:text>
      <ns0:g transform="translate(0, 30)">
        <ns0:text x="0" y="105.0" font-size="13" fill="#1a237e">
          <ns0:tspan font-weight="bold" fill="#c62828">Problem:</ns0:tspan>
          Pure next-token prediction → Generates toxic/unhelpful/incorrect content
        </ns0:text>
        <ns0:text x="0" y="130.0" font-size="13" fill="#2e7d32" font-weight="bold">
          <ns0:tspan fill="#145a32">RLHF Solution:</ns0:tspan>
          Use human feedback → Train reward model → Optimize policy with RL (PPO)
        </ns0:text>
        <ns0:text x="0" y="155.0" font-size="12" fill="#1a237e">
          Key to: ChatGPT, Claude, GPT-4, Gemini success!
        </ns0:text>
      </ns0:g>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(50, 240)">
    <ns0:text x="500" y="180.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#2e7d32">
      2. Three-Stage RLHF Pipeline
    </ns0:text>
    <ns0:rect x="0" y="20" width="2250.0" height="520" fill="#1a237e" stroke="#28a745" stroke-width="3" rx="8"/>
    <ns0:g transform="translate(30, 50)">
      <ns0:g>
        <ns0:text x="0" y="205.0" font-size="14" font-weight="bold" fill="#145a32">
          Stage 1: Supervised Fine-Tuning (SFT)
        </ns0:text>
        <ns0:rect x="0" y="15" width="940" height="270.0" fill="#1a237e" stroke="#28a745" stroke-width="2" rx="5"/>
        <ns0:g transform="translate(20, 35)">
          <ns0:text x="0" y="230.0" font-size="12" fill="#1a237e">
            <ns0:tspan font-weight="bold" fill="#145a32">Goal:</ns0:tspan>
            Create initial policy that follows instructions
          </ns0:text>
          <ns0:text x="0" y="255.0" font-size="11" fill="#1a237e">
            • Collect high-quality prompt-response pairs from humans
          </ns0:text>
          <ns0:text x="0" y="280.0" font-size="11" fill="#1a237e">
            • Fine-tune pretrained LLM on this dataset
          </ns0:text>
          <ns0:text x="0" y="305.0" font-size="11" fill="#1a237e">
            • Example: &quot;Explain quantum physics&quot; → [Human-written detailed explanation]
          </ns0:text>
          <ns0:text x="0" y="330.0" font-size="11" fill="#1a237e">
            → Output: π_SFT (supervised fine-tuned policy)
          </ns0:text>
        </ns0:g>
      </ns0:g>
      <ns0:path d="M 470,155 L 470,180" stroke="#28a745" stroke-width="3" marker-end="url(#arrowGreen7)"/>
      <ns0:g transform="translate(0, 180)">
        <ns0:text x="0" y="355.0" font-size="14" font-weight="bold" fill="#145a32">
          Stage 2: Reward Model Training
        </ns0:text>
        <ns0:rect x="0" y="15" width="940" height="160" fill="#1a237e" stroke="#28a745" stroke-width="2" rx="5"/>
        <ns0:g transform="translate(20, 35)">
          <ns0:text x="0" y="380.0" font-size="12" fill="#1a237e">
            <ns0:tspan font-weight="bold" fill="#145a32">Goal:</ns0:tspan>
            Learn to predict which outputs humans prefer
          </ns0:text>
          <ns0:text x="0" y="405.0" font-size="11" fill="#1a237e">
            • Generate multiple responses to same prompt using π_SFT
          </ns0:text>
          <ns0:text x="0" y="430.0" font-size="11" fill="#1a237e">
            • Humans rank responses: A &gt; B &gt; C &gt; D (pairwise comparisons)
          </ns0:text>
          <ns0:text x="0" y="455.0" font-size="11" fill="#1a237e">
            • Train reward model: r_θ(prompt, response) → scalar score
          </ns0:text>
          <ns0:text x="0" y="480.0" font-size="11" font-family="Arial, sans-serif" fill="#6a1b9a">
            Loss = -log(σ(r(x,y_w) - r(x,y_l)))     // y_w preferred over y_l
          </ns0:text>
          <ns0:text x="0" y="505.0" font-size="11" fill="#1a237e">
            → Output: r_θ (reward model that scores responses)
          </ns0:text>
          <ns0:text x="0" y="530.0" font-size="10" fill="#1a237e">
            Typically: Initialize from π_SFT, replace final layer with scalar head
          </ns0:text>
        </ns0:g>
      </ns0:g>
      <ns0:path d="M 470,355 L 470,380" stroke="#28a745" stroke-width="3" marker-end="url(#arrowGreen7)"/>
      <ns0:g transform="translate(0, 380)">
        <ns0:text x="0" y="555.0" font-size="14" font-weight="bold" fill="#145a32">
          Stage 3: RL Optimization (PPO)
        </ns0:text>
        <ns0:rect x="0" y="15" width="940" height="110" fill="#1a237e" stroke="#28a745" stroke-width="2" rx="5"/>
        <ns0:g transform="translate(20, 35)">
          <ns0:text x="0" y="580.0" font-size="12" fill="#1a237e">
            <ns0:tspan font-weight="bold" fill="#145a32">Goal:</ns0:tspan>
            Optimize policy to maximize reward while staying close to π_SFT
          </ns0:text>
          <ns0:text x="0" y="605.0" font-size="11" fill="#1a237e">
            • Sample prompt x → Generate response y ~ π_θ(·|x) → Get reward r_θ(x,y)
          </ns0:text>
          <ns0:text x="0" y="630.0" font-size="11" font-family="Arial, sans-serif" fill="#6a1b9a">
            <ns0:tspan x="0" dy="0">Objective: maximize E[r_θ(x,y)] - β·KL(π_θ || π_SFT) // KL</ns0:tspan>
            <ns0:tspan x="0" dy="1.2em">penalty prevents drift</ns0:tspan>
          </ns0:text>
          <ns0:text x="0" y="655.0" font-size="11" fill="#1a237e">
            • Use PPO to update π_θ iteratively
          </ns0:text>
          <ns0:text x="0" y="680.0" font-size="11" fill="#2e7d32" font-weight="bold">
            → Output: π_RLHF (final aligned model)
          </ns0:text>
        </ns0:g>
      </ns0:g>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(50, 800)">
    <ns0:text x="500" y="705.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#6a1b9a">
      3. Why RLHF Works
    </ns0:text>
    <ns0:rect x="0" y="20" width="2250.0" height="80" fill="#1a237e" stroke="#6f42c1" stroke-width="3" rx="8"/>
    <ns0:g transform="translate(30, 50)">
      <ns0:text x="0" y="730.0" font-size="13" fill="#2e7d32">
        ✓ 
        <ns0:tspan font-weight="bold">Aligns with human intent:</ns0:tspan>
        Model learns implicit preferences (helpfulness, truthfulness, safety)
      </ns0:text>
      <ns0:text x="0" y="755.0" font-size="13" fill="#2e7d32">
        ✓ 
        <ns0:tspan font-weight="bold">Reduces harmful outputs:</ns0:tspan>
        Explicitly optimizes for safety via reward model
      </ns0:text>
      <ns0:text x="0" y="780.0" font-size="13" fill="#c62828">
        ✗ 
        <ns0:tspan font-weight="bold">Challenges:</ns0:tspan>
        Expensive (human labeling), reward hacking (model exploits reward model), KL penalty tuning
      </ns0:text>
    </ns0:g>
  </ns0:g>
</ns0:svg>
