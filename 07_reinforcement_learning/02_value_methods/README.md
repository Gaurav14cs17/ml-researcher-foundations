<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=120&section=header&text=Value-Based%20Methods&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-07-F39C12?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

## ğŸ”— Navigation

â¬…ï¸ [Back: MDP](../01_mdp/) | â¡ï¸ [Next: Policy Methods](../03_policy_methods/)

---

## ğŸ“‚ Topics

| Folder | Topic | Key Concepts |
|--------|-------|--------------|
| [01_bellman/](./01_bellman/) | Bellman equations | V(s), Q(s,a) |
| [02_dqn/](./02_dqn/) | Deep Q-Networks | Neural Q-function |
| [03_dynamic_programming/](./03_dynamic_programming/) | Policy/Value iteration | Known dynamics |
| [04_q_learning/](./04_q_learning/) | Q-learning | Off-policy TD |
| [05_td_learning/](./05_td_learning/) | Temporal difference | TD(0), TD(Î») |

---

## ğŸ“ Mathematical Foundations

### Bellman Expectation Equations

```
V^Ï€(s) = Î£_a Ï€(a|s) [R(s,a) + Î³ Î£_{s'} P(s'|s,a) V^Ï€(s')]

Q^Ï€(s,a) = R(s,a) + Î³ Î£_{s'} P(s'|s,a) Î£_{a'} Ï€(a'|s') Q^Ï€(s',a')

```

### Bellman Optimality Equations

```
V*(s) = max_a [R(s,a) + Î³ Î£_{s'} P(s'|s,a) V*(s')]

Q*(s,a) = R(s,a) + Î³ Î£_{s'} P(s'|s,a) max_{a'} Q*(s',a')

```

### TD Error

```
Î´â‚œ = râ‚œ + Î³V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)

TD(0) update:
V(sâ‚œ) â† V(sâ‚œ) + Î± Î´â‚œ

TD(Î») combines n-step returns:
G^Î» = (1-Î») Î£â‚™â‚Œâ‚^âˆ Î»â¿â»Â¹ Gâ‚œ:â‚œâ‚Šâ‚™

```

---

## ğŸ”‘ Key Equations

```
Value function:
V(s) = E[Î£â‚œ Î³áµ—râ‚œ | sâ‚€ = s]

Action-value (Q-function):
Q(s,a) = E[Î£â‚œ Î³áµ—râ‚œ | sâ‚€ = s, aâ‚€ = a]

Q-learning update:
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]

```

---

## ğŸ”— Where This Topic Is Used

| Topic | How Value Methods Are Used |
|-------|---------------------------|
| **DQN** | Neural network approximates Q(s,a) |
| **Actor-Critic** | Critic = Value function V(s) or Q(s,a) |
| **PPO** | Uses value function as baseline |
| **A3C / A2C** | Advantage = Q - V |
| **AlphaGo** | Value network estimates winning probability |
| **Model Predictive Control** | Value for planning |
| **Inverse RL** | Recover reward from value function |
| **Reward Shaping** | Potential-based using V(s) |

### Prerequisite For

```
Value Methods --> DQN, Rainbow
             --> Actor-Critic methods
             --> PPO (uses value baseline)
             --> Model-Based RL planning

```

### Used By These Papers

| Paper | How It Uses Value Methods |
|-------|--------------------------|
| DQN (2013) | Deep Q-learning for Atari |
| A3C (2016) | Async actor-critic with V(s) |
| PPO (2017) | Generalized advantage estimation |
| AlphaGo (2016) | Value network + MCTS |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Sutton & Barto Ch. 6-8 | [RL Book](http://incompleteideas.net/book/) |
| ğŸ“„ | DQN Paper | [Nature](https://www.nature.com/articles/nature14236) |
| ğŸ¥ | David Silver Lectures | [YouTube](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) |
| ğŸ‡¨ğŸ‡³ | ä»·å€¼å‡½æ•°æ–¹æ³•è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/26052182) |
| ğŸ‡¨ğŸ‡³ | Q-Learningåˆ°DQN | [CSDN](https://blog.csdn.net/qq_30615903/article/details/80739243) |
| ğŸ‡¨ğŸ‡³ | å¼ºåŒ–å­¦ä¹ åŸºç¡€ | [Bç«™](https://www.bilibili.com/video/BV1sd4y167NS) |

---

â¬…ï¸ [Back: MDP](../01_mdp/) | â¡ï¸ [Next: Policy Methods](../03_policy_methods/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=F39C12&height=80&section=footer" width="100%"/>
</p>
