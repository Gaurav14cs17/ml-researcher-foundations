<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=120&section=header&text=Activation%20Functions&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-06-45B7D1?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<p align="center">
  <a href="../">â¬†ï¸ Back to Neural Networks</a> &nbsp;|&nbsp;
  <a href="../02_initialization/">Next: Initialization â¡ï¸</a>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/activation-functions.svg" width="100%">

*Caption: Different activation functions have different shapes and use cases. ReLU is the default for hidden layers, GELU powers modern Transformers, Sigmoid for binary outputs, and Softmax for multi-class classification.*

---

## ğŸ“ Mathematical Definitions

### Classic Activations

```
Sigmoid:        Ïƒ(x) = 1 / (1 + eâ»Ë£)
                Range: (0, 1)
                Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))

Tanh:           tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)
                Range: (-1, 1)
                tanh'(x) = 1 - tanhÂ²(x)

ReLU:           ReLU(x) = max(0, x)
                Range: [0, âˆ)
                ReLU'(x) = 1 if x > 0, else 0

```

### Modern Activations

```
Leaky ReLU:     LeakyReLU(x) = max(Î±x, x),  Î± â‰ˆ 0.01
                Prevents "dying ReLU" problem

GELU:           GELU(x) = x Â· Î¦(x) = x Â· 0.5(1 + erf(x/âˆš2))
                â‰ˆ 0.5x(1 + tanh(âˆš(2/Ï€)(x + 0.044715xÂ³)))
                Used in: GPT, BERT, Transformers

SiLU/Swish:     SiLU(x) = x Â· Ïƒ(x) = x / (1 + eâ»Ë£)
                Used in: EfficientNet, LLaMA

Softmax:        softmax(xáµ¢) = exp(xáµ¢) / Î£â±¼exp(xâ±¼)
                Output: probability distribution (sums to 1)
                Gradient: âˆ‚softmax(x)áµ¢/âˆ‚xâ±¼ = sáµ¢(Î´áµ¢â±¼ - sâ±¼)

```

---

## ğŸ“Š Comparison Table

| Function | Formula | Range | Gradient | Used In |
|----------|---------|-------|----------|---------|
| **Sigmoid** | 1/(1+eâ»Ë£) | (0,1) | Ïƒ(1-Ïƒ) â‰¤ 0.25 | Binary output |
| **Tanh** | (eË£-eâ»Ë£)/(eË£+eâ»Ë£) | (-1,1) | 1-tanhÂ² â‰¤ 1 | RNN hidden |
| **ReLU** | max(0,x) | [0,âˆ) | 0 or 1 | CNN hidden |
| **LeakyReLU** | max(Î±x,x) | (-âˆ,âˆ) | Î± or 1 | GAN |
| **GELU** | xÂ·Î¦(x) | (-0.17,âˆ) | smooth | Transformers |
| **SiLU** | xÂ·Ïƒ(x) | (-0.28,âˆ) | smooth | LLaMA, EfficientNet |
| **Softmax** | exp(xáµ¢)/Î£exp(xâ±¼) | (0,1)â¿ | matrix | Classification |

---

## ğŸ”‘ Why Non-linearity?

```
Without activation:
Layer 1: yâ‚ = Wâ‚x
Layer 2: yâ‚‚ = Wâ‚‚yâ‚ = Wâ‚‚Wâ‚x = W'x

Multiple linear layers collapse to ONE linear layer!
Can only represent linear functions.

With activation:
Layer 1: yâ‚ = Ïƒ(Wâ‚x)
Layer 2: yâ‚‚ = Ïƒ(Wâ‚‚yâ‚) â‰  linear in x

Non-linearity enables:
â€¢ Universal approximation (any continuous function)
â€¢ Learning complex decision boundaries
â€¢ Deep feature hierarchies

```

### Universal Approximation Theorem

```
A feedforward network with ONE hidden layer and 
non-linear activation can approximate any continuous
function on a compact set to arbitrary accuracy.

But: May require exponentially many neurons!
     Deep networks are more parameter-efficient.

```

---

## ğŸ’» Code Examples

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

x = torch.randn(32, 128)

# ReLU - most common
relu_out = F.relu(x)
relu_out = nn.ReLU()(x)

# GELU - Transformers (GPT, BERT)
gelu_out = F.gelu(x)

# SiLU/Swish - EfficientNet, LLaMA
silu_out = F.silu(x)

# Sigmoid - binary classification output
sigmoid_out = torch.sigmoid(x)

# Softmax - multi-class classification
logits = torch.randn(32, 10)  # 32 samples, 10 classes
probs = F.softmax(logits, dim=-1)  # sums to 1 along dim=-1

# Stable log-softmax for cross-entropy
log_probs = F.log_softmax(logits, dim=-1)

# Custom activation
class Mish(nn.Module):
    """Mish: x * tanh(softplus(x))"""
    def forward(self, x):
        return x * torch.tanh(F.softplus(x))

# In a network
class MLP(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, out_dim)
        self.act = nn.GELU()  # Modern choice
    
    def forward(self, x):
        x = self.act(self.fc1(x))
        return self.fc2(x)  # No activation on output (let loss handle it)

```

---

## ğŸŒ ML Applications

| Model | Activation | Reason |
|-------|------------|--------|
| **GPT, BERT, LLaMA** | GELU | Smooth, better gradient flow |
| **ResNet, VGG** | ReLU | Fast, sparse |
| **EfficientNet** | SiLU/Swish | Self-gating |
| **LSTM** | Sigmoid + Tanh | Gates need (0,1), states need (-1,1) |
| **GAN** | LeakyReLU | Prevents dying neurons |
| **Classification head** | Softmax | Probability output |
| **Binary classification** | Sigmoid | (0,1) output |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ¥ | 3Blue1Brown: Neural Networks | [YouTube](https://www.youtube.com/watch?v=aircAruvnKk) |
| ğŸ¥ | StatQuest: ReLU | [YouTube](https://www.youtube.com/watch?v=68BZ5f7P94E) |
| ğŸ“„ | GELU Paper | [arXiv](https://arxiv.org/abs/1606.08415) |
| ğŸ“„ | Swish Paper | [arXiv](https://arxiv.org/abs/1710.05941) |
| ğŸ‡¨ğŸ‡³ | æ¿€æ´»å‡½æ•°è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25110450) |
| ğŸ‡¨ğŸ‡³ | æ·±åº¦å­¦ä¹ æ¿€æ´»å‡½æ•° | [Bç«™](https://www.bilibili.com/video/BV1Y64y1Q7hi) |
| ğŸ‡¨ğŸ‡³ | ReLU/GELUå¯¹æ¯” | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88318597)

---

## ğŸ”— Where This Topic Is Used

| Topic | How Activations Are Used |
|-------|--------------------------|
| **Transformer FFN** | GELU between two linear layers |
| **CNN layers** | ReLU after each conv |
| **LSTM gates** | Sigmoid for gates, tanh for state |
| **Attention** | Softmax for attention weights |
| **Binary classifier** | Sigmoid output |
| **Multi-class** | Softmax output |

---

<p align="center">
  <a href="../">â¬†ï¸ Back to Neural Networks</a> &nbsp;|&nbsp;
  <a href="../02_initialization/">Next: Initialization â¡ï¸</a>
</p>

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=80&section=footer" width="100%"/>
</p>
