<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=120&section=header&text=Neurons&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-06-45B7D1?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<p align="center">
  <a href="../">â¬†ï¸ Back to Neural Networks</a> &nbsp;|&nbsp;
  <a href="../03_layers/">â¬…ï¸ Prev: Layers</a>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/neuron-perceptron.svg" width="100%">

*Caption: An artificial neuron (perceptron) computes a weighted sum of inputs plus bias, then applies an activation function. This simple computation, when combined in layers, can approximate any function (Universal Approximation Theorem).*

---

## ğŸ“‚ Overview

The neuron is the basic building block of all neural networks. Understanding how a single neuron works is essential before studying complex architectures.

---

## ğŸ“ Mathematical Definition

### Single Neuron Computation

```
Input: x = [xâ‚, xâ‚‚, ..., xâ‚™]áµ€ âˆˆ â„â¿
Weights: w = [wâ‚, wâ‚‚, ..., wâ‚™]áµ€ âˆˆ â„â¿
Bias: b âˆˆ â„

Linear combination:
    z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b
    z = wáµ€x + b  (vector notation)
    z = Î£áµ¢ wáµ¢xáµ¢ + b

Output with activation:
    y = Ïƒ(z) = Ïƒ(wáµ€x + b)

```

### The Perceptron (Binary Classification)

```
y = sign(wáµ€x + b)

    â§  1  if wáµ€x + b > 0
y = â¨
    â© -1  if wáµ€x + b â‰¤ 0

Decision boundary: wáµ€x + b = 0 (hyperplane)

```

### Perceptron Learning Rule

```
For misclassified point (xáµ¢, yáµ¢):
    w â† w + Î· yáµ¢ xáµ¢
    b â† b + Î· yáµ¢

Where Î· is the learning rate

Convergence: Guaranteed if data is linearly separable

```

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **Weights (w)** | Learnable parameters that scale inputs |
| **Bias (b)** | Shifts the decision boundary |
| **Activation (Ïƒ)** | Non-linear function for expressiveness |
| **Linear Combination** | Weighted sum: wáµ€x + b |

---

## ğŸ“Š Common Activation Functions

| Function | Formula | Range | Use |
|----------|---------|-------|-----|
| **Sigmoid** | Ïƒ(z) = 1/(1+eâ»á¶») | (0, 1) | Binary output |
| **Tanh** | tanh(z) = (eá¶»-eâ»á¶»)/(eá¶»+eâ»á¶») | (-1, 1) | Zero-centered |
| **ReLU** | max(0, z) | [0, âˆ) | Hidden layers |
| **Linear** | z | (-âˆ, âˆ) | Regression output |

```
Why Non-linearity?

Without activation:
    Layer 1: hâ‚ = Wâ‚x + bâ‚
    Layer 2: hâ‚‚ = Wâ‚‚hâ‚ + bâ‚‚ = Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚
                            = (Wâ‚‚Wâ‚)x + (Wâ‚‚bâ‚ + bâ‚‚)
                            = W'x + b'  â† Still linear!

Multiple linear layers = single linear layer!
Activation breaks this, enabling complex functions.

```

---

## ğŸ’» Code Examples

### Single Neuron in NumPy

```python
import numpy as np

class Neuron:
    """A single artificial neuron"""
    
    def __init__(self, n_inputs, activation='sigmoid'):
        # Xavier initialization
        self.w = np.random.randn(n_inputs) / np.sqrt(n_inputs)
        self.b = 0.0
        self.activation = activation
    
    def _activate(self, z):
        if self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-z))
        elif self.activation == 'relu':
            return np.maximum(0, z)
        elif self.activation == 'tanh':
            return np.tanh(z)
        else:
            return z  # Linear
    
    def forward(self, x):
        """Compute neuron output"""
        z = np.dot(self.w, x) + self.b  # Linear combination
        return self._activate(z)  # Activation

# Usage
neuron = Neuron(n_inputs=3, activation='sigmoid')
x = np.array([1.0, 2.0, 3.0])
output = neuron.forward(x)
print(f"Neuron output: {output}")

```

### Perceptron Learning

```python
def perceptron_train(X, y, max_epochs=100, lr=1.0):
    """
    Perceptron learning algorithm
    X: (n_samples, n_features)
    y: (n_samples,) with values {-1, +1}
    """
    n_features = X.shape[1]
    w = np.zeros(n_features)
    b = 0.0
    
    for epoch in range(max_epochs):
        errors = 0
        for xi, yi in zip(X, y):
            prediction = np.sign(np.dot(w, xi) + b)
            if prediction != yi:
                # Misclassified! Update
                w += lr * yi * xi
                b += lr * yi
                errors += 1
        
        if errors == 0:
            print(f"Converged at epoch {epoch}")
            break
    
    return w, b

```

### PyTorch Neuron

```python
import torch
import torch.nn as nn

# Single neuron as a Linear layer with 1 output
class SingleNeuron(nn.Module):
    def __init__(self, n_inputs, activation='sigmoid'):
        super().__init__()
        self.linear = nn.Linear(n_inputs, 1)
        self.activation = nn.Sigmoid() if activation == 'sigmoid' else nn.ReLU()
    
    def forward(self, x):
        z = self.linear(x)
        return self.activation(z)

# Usage
neuron = SingleNeuron(n_inputs=10)
x = torch.randn(32, 10)  # Batch of 32 samples
output = neuron(x)  # Shape: (32, 1)

```

---

## ğŸ§  Biological vs Artificial Neurons

| Aspect | Biological | Artificial |
|--------|-----------|------------|
| **Inputs** | Dendrites | Input vector x |
| **Weights** | Synaptic strength | Learned parameters w |
| **Sum** | Cell body | wáµ€x + b |
| **Activation** | Action potential | Ïƒ(z) |
| **Output** | Axon | Single value y |

---

## ğŸ“Š Limitations of Single Neuron

```
XOR Problem (Cannot solve with one neuron!):

    (0,0) â†’ 0     â—---------â—‹ (1,0)
                  |    â•²    |
                  |     â•²   |  No single line
                  |      â•²  |  separates â— from â—‹
    (0,1) â†’ 1     â—‹---------â— (1,1)

Solution: Multiple layers (MLPs)!

```

---

## ğŸ”— Connection to Other Topics

```
Neuron (basic unit)
    |
    +-- Layers (many neurons)
    +-- MLPs (stacked layers)
    +-- Activations (non-linearity)
    +-- Backpropagation (learning)
    +-- Universal Approximation (theory)

```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Layers | [../layers/](../layers/) |
| ğŸ“– | Activations | [../activations/](../activations/) |
| ğŸ¥ | 3Blue1Brown: Neural Networks | [YouTube](https://www.youtube.com/watch?v=aircAruvnKk) |
| ğŸ“„ | Rosenblatt Perceptron | [Paper (1958)](https://psycnet.apa.org/record/1959-09865-001) |
| ğŸ‡¨ğŸ‡³ | ç¥ç»å…ƒä¸æ„ŸçŸ¥æœºè¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/30844948) |
| ğŸ‡¨ğŸ‡³ | æ„ŸçŸ¥æœºç®—æ³•å®ç° | [CSDN](https://blog.csdn.net/qq_37466121/article/details/87902069) |
| Bç«™ | ç¥ç»ç½‘ç»œåŸºç¡€ | ä»å•ä¸ªç¥ç»å…ƒå¼€å§‹ |
| æœºå™¨ä¹‹å¿ƒ | æ·±åº¦å­¦ä¹ å…¥é—¨ | ç¥ç»å…ƒåŸç†è§£æ |

## ğŸ”— Where This Topic Is Used

| Application | How Neurons Are Used |
|-------------|---------------------|
| **All Neural Networks** | Building block |
| **Universal Approximation** | Function approximation |
| **Deep Learning** | Stacked nonlinear transforms |
| **Perceptron** | Historical first NN |

---

<p align="center">
  <a href="../">â¬†ï¸ Back to Neural Networks</a> &nbsp;|&nbsp;
  <a href="../03_layers/">â¬…ï¸ Prev: Layers</a>
</p>

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=80&section=footer" width="100%"/>
</p>
