<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=120&section=header&text=Deep%20Learning&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-06-45B7D1?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“Š Learning Path

```
ğŸš€ Start --â–¶ ğŸ§  Neural Nets --â–¶ ğŸ”„ Backprop --â–¶ ğŸ–¼ï¸ CNN --â–¶ ğŸ“ RNN --â–¶ âš¡ Transformer --â–¶ ğŸ¨ Diffusion --â–¶ ğŸ”¥ Hot Topics --â–¶ ğŸ† Master

```

## ğŸ¯ What You'll Learn

> ğŸš€ From **Perceptrons to Transformers**: Neural Networks that Changed AI

<table>
<tr>
<td align="center">

### ğŸ§  Fundamentals
Neural Networks, Backprop

</td>
<td align="center">

### ğŸ—ï¸ Architectures
CNN, RNN, Transformer

</td>
<td align="center">

### ğŸ”¥ Cutting Edge
Flash Attention, LoRA, MoE

</td>
</tr>
</table>

---

## ğŸ“š Main Topics

### 1ï¸âƒ£ Neural Networks Basics

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/Priority-â­â­â­-gold?style=flat-square"/>

```
Perceptron --â–¶ MLP --â–¶ Activations --â–¶ Forward --â–¶ Loss

```

**Core:** Perceptron, MLP, Activations (ReLU, GELU), Loss Functions

<a href="./01_neural_networks/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 2ï¸âƒ£ Backpropagation â­

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ”¥_ESSENTIAL-critical?style=flat-square"/>

```
Forward --â–¶ Loss --â–¶ Backward --â–¶ Chain Rule --â–¶ Gradients --â–¶ Update

```

> â­ **MOST IMPORTANT: How neural networks learn**

**Core:** Chain Rule, Computational Graphs, Autodiff, Gradient Flow

<a href="./02_backpropagation/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 3ï¸âƒ£ CNN - Convolutional Networks

<img src="https://img.shields.io/badge/Time-4_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ–¼ï¸_Vision-purple?style=flat-square"/>

```
Conv --â–¶ Pool --â–¶ Architectures --â–¶ ResNet --â–¶ Skip Connections

```

**Core:** Convolution, Pooling, ResNet, Skip Connections

<a href="./03_architectures/01_cnn/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 4ï¸âƒ£ RNN - Recurrent Networks

<img src="https://img.shields.io/badge/Time-4_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ“_Sequence-teal?style=flat-square"/>

```
RNN --â–¶ Vanishing --â–¶ LSTM --â–¶ GRU --â–¶ Seq2Seq

```

**Core:** RNN, LSTM, GRU, Vanishing Gradients

<a href="./03_architectures/05_rnn/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 5ï¸âƒ£ Transformers â­â­â­

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ”¥_ESSENTIAL-critical?style=flat-square"/>

```
Self-Attn --â–¶ Multi-Head --â–¶ Pos Enc --â–¶ Transformer --â–¶ BERT/GPT --â–¶ LLMs

```

> â­ **FOUNDATION OF MODERN AI** - Powers GPT, BERT, LLaMA, Claude

**Core:** Self-Attention, Multi-Head, Positional Encoding, BERT vs GPT

<a href="./03_architectures/06_transformer/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 6ï¸âƒ£ Generative Models

<img src="https://img.shields.io/badge/Time-5_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ¨_Creative-pink?style=flat-square"/>

```
VAE --â–¶ GAN --â–¶ Diffusion --â–¶ Stable Diffusion --â–¶ Image Gen

```

**Core:** VAE, GAN, Diffusion Models, Stable Diffusion

<a href="./03_architectures/02_diffusion/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 7ï¸âƒ£ Training Techniques

<img src="https://img.shields.io/badge/Time-4_hours-blue?style=flat-square"/>

```
Init --â–¶ BatchNorm --â–¶ Dropout --â–¶ Augment --â–¶ LR Schedule

```

**Core:** Xavier/He Init, BatchNorm, LayerNorm, Dropout

<a href="./04_training/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-FF6B6B?style=for-the-badge" alt="Learn"/></a>

---

### 8ï¸âƒ£ Hot Topics ğŸ”¥

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/2024-Latest-red?style=flat-square"/>

```
Flash Attn --â–¶ LoRA --â–¶ MoE --â–¶ ViT --â–¶ Efficient

```

| Topic | Impact |
|-------|--------|
| âš¡ Flash Attention | 5x faster, O(n) memory |
| ğŸ”§ LoRA | Fine-tune with 0.1% params |
| ğŸ§© MoE | Scale to trillions |

<a href="./06_hot_topics/01_flash_attention/README.md"><img src="https://img.shields.io/badge/âš¡_Flash_Attention-FF6B6B?style=for-the-badge" alt="Flash"/></a>
<a href="./06_hot_topics/02_lora/README.md"><img src="https://img.shields.io/badge/ğŸ”§_LoRA-FF6B6B?style=for-the-badge" alt="LoRA"/></a>

---

## ğŸ”„ Architecture Evolution

```
1958         1986         2012         2015         2017         2022
 |            |            |            |            |            |
 â–¼            â–¼            â–¼            â–¼            â–¼            â–¼
Perceptron â†’ Backprop â†’ AlexNet â†’ ResNet â†’ Transformer â†’ ChatGPT

```

---

## ğŸ’¡ Key Formulas

<table>
<tr>
<td>

### ğŸ§  Forward Pass

```
z = Wx + b
a = Ïƒ(z)

```

</td>
<td>

### ğŸ”„ Backprop

```
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚W

```

</td>
<td>

### âš¡ Attention

```
Attn(Q,K,V) = softmax(QKáµ€/âˆšdâ‚–)V

```

</td>
</tr>
</table>

---

## ğŸ”— Prerequisites & Next Steps

```
                    +--------------+
                    |   ML Theory  |
                    +------+-------+
                           |
                           â–¼
                    +--------------+
                    | ğŸ§¬ Deep      |
                    |   Learning   |
                    +------+-------+
                           |
              +------------+------------+
              â–¼                         â–¼
       +--------------+          +--------------+
       |   ğŸ® RL      |          | ğŸ—œï¸ Compress  |
       +------+-------+          +------+-------+
              |                         |
              â–¼                         â–¼
       +--------------+          +--------------+
       |    RLHF      |          | Efficient ML |
       +--------------+          +--------------+

```

<p align="center">
  <a href="../05_ml_theory/README.md"><img src="https://img.shields.io/badge/â†_Prerequisites:_ML_Theory-gray?style=for-the-badge" alt="Prev"/></a>
  <a href="../07_reinforcement_learning/README.md"><img src="https://img.shields.io/badge/Next:_RL_â†’-00C853?style=for-the-badge" alt="Next"/></a>
</p>

---

## ğŸ“š Recommended Resources

| Type | Resource | Focus |
|:----:|----------|-------|
| ğŸ“˜ | [Deep Learning Book](https://www.deeplearningbook.org/) | Goodfellow et al. |
| ğŸ“ | [Stanford CS231n](http://cs231n.stanford.edu/) | CNN & Vision |
| ğŸ“ | [Stanford CS224n](https://web.stanford.edu/class/cs224n/) | NLP & Transformers |
| ğŸ“„ | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Original Transformer |

---

## ğŸ—ºï¸ Quick Navigation

| Previous | Current | Next |
|:--------:|:-------:|:----:|
| [ğŸ¯ ML Theory](../05_ml_theory/README.md) | **ğŸ§¬ Deep Learning** | [ğŸ® RL â†’](../07_reinforcement_learning/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=80&section=footer" width="100%"/>
</p>
