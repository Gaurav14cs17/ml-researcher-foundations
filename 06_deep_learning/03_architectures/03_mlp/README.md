<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=120&section=header&text=Multi-Layer%20Perceptron%20MLP&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-06-45B7D1?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/mlp.svg" width="100%">

*Caption: An MLP consists of input, hidden, and output layers. Each layer performs a linear transformation followed by a nonlinear activation: h = Ïƒ(Wx + b). MLPs are universal function approximators.*

---

## ğŸ“‚ Overview

The Multi-Layer Perceptron is the simplest form of neural network. Despite being "basic," MLPs are powerful function approximators and the building block for more complex architectures.

---

## ğŸ“ Mathematical Foundations

### Forward Pass
```
Layer l:
hâ‚— = Ïƒ(Wâ‚—hâ‚—â‚‹â‚ + bâ‚—)

Full network:
f(x) = Wâ‚—ÏƒWâ‚—â‚‹â‚Ïƒ...ÏƒWâ‚x + biases

hâ‚ = Ïƒ(Wâ‚x + bâ‚)
hâ‚‚ = Ïƒ(Wâ‚‚hâ‚ + bâ‚‚)
y = Wâ‚ƒhâ‚‚ + bâ‚ƒ
```

### Backward Pass (Gradient)
```
Î´â‚— = âˆ‚L/âˆ‚hâ‚—

Î´â‚—â‚‹â‚ = (Wâ‚—áµ€Î´â‚—) âŠ™ Ïƒ'(zâ‚—â‚‹â‚)  where zâ‚— = Wâ‚—hâ‚—â‚‹â‚ + bâ‚—

âˆ‚L/âˆ‚Wâ‚— = Î´â‚— hâ‚—â‚‹â‚áµ€
âˆ‚L/âˆ‚bâ‚— = Î´â‚—
```

### Universal Approximation
```
Theorem (Hornik, 1989):
MLP with one hidden layer can approximate any
continuous function on compact subsets of â„â¿
to arbitrary precision, given enough neurons.

f(x) â‰ˆ Î£áµ¢â‚Œâ‚á´º Î±áµ¢ Ïƒ(wáµ¢áµ€x + báµ¢)
```

---

## ğŸ”‘ Key Topics

| Topic | Description |
|-------|-------------|
| Fundamentals | Core concepts |
| Applications | ML use cases |
| Implementation | Code examples |

---

## ğŸ’» Example

```python
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, out_dim)
        self.act = nn.GELU()
    
    def forward(self, x):
        x = self.act(self.fc1(x))
        x = self.act(self.fc2(x))
        return self.fc3(x)
```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | Universal Approximation | [Paper](https://cognitivemedium.com/magic_paper/assets/Hornik.pdf) |
| ğŸ¥ | 3Blue1Brown: Neural Networks | [YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) |
| ğŸ“– | PyTorch nn.Linear | [Docs](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) |
| ğŸ‡¨ğŸ‡³ | MLPè¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25110450) |
| ğŸ‡¨ğŸ‡³ | å¤šå±‚æ„ŸçŸ¥æœºå®ç° | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88619088) |
| ğŸ‡¨ğŸ‡³ | ç¥ç»ç½‘ç»œå…¥é—¨ | [Bç«™](https://www.bilibili.com/video/BV1J94y1f7u5) |

---

## ğŸ”— Where This Topic Is Used

| Application | MLP Usage |
|-------------|----------|
| **Classification** | Output layer |
| **Regression** | Function approximation |
| **Transformer FFN** | Position-wise FFN |
| **Feature Extraction** | Hidden layers |

---

â¬…ï¸ [Back: Diffusion](../02_diffusion/README.md) | â¡ï¸ [Next: MoE](../04_moe/README.md)

---

â¬…ï¸ [Back: Architectures](../../README.md)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=80&section=footer" width="100%"/>
</p>
