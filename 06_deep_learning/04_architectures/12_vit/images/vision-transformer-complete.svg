<?xml version="1.0" ?>
<ns0:svg xmlns:ns0="http://www.w3.org/2000/svg" viewBox="0.0 0.0 2475.0 2025.0">
  <ns0:defs>
    <ns0:marker id="arrow2" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <ns0:path d="M0,0 L0,6 L9,3 z" fill="#1a237e"/>
    </ns0:marker>
  </ns0:defs>
  <ns0:text x="550" y="30" font-size="22" font-weight="bold" text-anchor="middle" fill="#1a237e">
    Vision Transformer (ViT): Transformers for Computer Vision
  </ns0:text>
  <ns0:g transform="translate(50, 80)">
    <ns0:text x="500" y="55.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#1565c0">
      1. From CNNs to Transformers
    </ns0:text>
    <ns0:rect x="0" y="20" width="2250.0" height="270.0" fill="#f8f9fa" stroke="#007bff" stroke-width="3" rx="8"/>
    <ns0:g transform="translate(30, 50)">
      <ns0:text x="0" y="80.0" font-size="14" fill="#1a237e">
        <ns0:tspan font-weight="bold" fill="#1565c0">Challenge:</ns0:tspan>
        Can we apply Transformers (SOTA for NLP) to computer vision?
      </ns0:text>
      <ns0:g transform="translate(0, 30)">
        <ns0:text x="0" y="105.0" font-size="13" fill="#1a237e">
          <ns0:tspan font-weight="bold" fill="#c62828">CNNs:</ns0:tspan>
          Inductive bias (locality, translation equivariance) → Good for small datasets
        </ns0:text>
        <ns0:text x="0" y="130.0" font-size="13" fill="#2e7d32" font-weight="bold">
          <ns0:tspan fill="#145a32">ViT:</ns0:tspan>
          Pure attention, no convolutions → Requires large datasets, but scales better!
        </ns0:text>
        <ns0:text x="0" y="155.0" font-size="12" fill="#1a237e">
          Key insight: Treat image as sequence of patches (like words in NLP)
        </ns0:text>
      </ns0:g>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(50, 240)">
    <ns0:text x="500" y="180.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#2e7d32">
      2. Vision Transformer Architecture
    </ns0:text>
    <ns0:rect x="0" y="20" width="2250.0" height="500" fill="#1a237e" stroke="#28a745" stroke-width="3" rx="8"/>
    <ns0:g transform="translate(50, 50)">
      <ns0:g>
        <ns0:text x="80" y="205.0" font-size="12" font-weight="bold" text-anchor="middle" fill="#1a237e">
          Input Image
        </ns0:text>
        <ns0:rect x="20" y="0" width="120" height="270.0" fill="#1565c0" opacity="0.7" stroke="#007bff" stroke-width="3" rx="3"/>
        <ns0:text x="80" y="230.0" font-size="14" font-weight="bold" text-anchor="middle" fill="#1a237e">
          224×224×3
        </ns0:text>
      </ns0:g>
      <ns0:path d="M 80,130 L 80,170" stroke="#e0e0e0" stroke-width="3" marker-end="url(#arrow2)"/>
      <ns0:g transform="translate(0, 170)">
        <ns0:text x="80" y="255.0" font-size="12" font-weight="bold" text-anchor="middle" fill="#e65100">
          1. Split into Patches
        </ns0:text>
        <ns0:rect x="20" y="0" width="120" height="60" fill="#1a237e" stroke="#fd7e14" stroke-width="3" rx="3"/>
        <ns0:g transform="translate(30, 20)">
          <ns0:rect x="0" y="0" width="15" height="15" fill="#1565c0" opacity="0.7" stroke="#007bff" stroke-width="1"/>
          <ns0:rect x="20" y="0" width="15" height="15" fill="#1565c0" opacity="0.7" stroke="#007bff" stroke-width="1"/>
          <ns0:rect x="40" y="0" width="15" height="15" fill="#1565c0" opacity="0.7" stroke="#007bff" stroke-width="1"/>
          <ns0:rect x="60" y="0" width="15" height="15" fill="#1565c0" opacity="0.7" stroke="#007bff" stroke-width="1"/>
          <ns0:rect x="80" y="0" width="15" height="15" fill="#1565c0" opacity="0.7" stroke="#007bff" stroke-width="1"/>
          <ns0:text x="50" y="280.0" font-size="9" text-anchor="middle" fill="#e65100" font-weight="bold">
            N patches (16×16 each)
          </ns0:text>
        </ns0:g>
      </ns0:g>
      <ns0:path d="M 80,300 L 80,340" stroke="#e0e0e0" stroke-width="3" marker-end="url(#arrow2)"/>
      <ns0:g transform="translate(0, 340)">
        <ns0:text x="80" y="305.0" font-size="12" font-weight="bold" text-anchor="middle" fill="#6a1b9a">
          2. Linear Projection
        </ns0:text>
        <ns0:rect x="20" y="0" width="120" height="60" fill="#1a237e" stroke="#6f42c1" stroke-width="3" rx="3"/>
        <ns0:g transform="translate(30, 20)">
          <ns0:text x="45" y="330.0" font-size="10" text-anchor="middle" fill="#1a237e">
            Each patch → vector
          </ns0:text>
          <ns0:text x="45" y="355.0" font-size="10" text-anchor="middle" fill="#6a1b9a" font-weight="bold">
            E ∈ ℝ^(P²·C × D)
          </ns0:text>
        </ns0:g>
      </ns0:g>
      <ns0:g transform="translate(200, 100)">
        <ns0:rect x="0" y="0" width="700" height="380" fill="#1a237e" stroke="#28a745" stroke-width="2" rx="5"/>
        <ns0:g transform="translate(20, 30)">
          <ns0:text x="0" y="380.0" font-size="13" font-weight="bold" fill="#145a32">
            Key Components:
          </ns0:text>
          <ns0:g transform="translate(0, 25)">
            <ns0:text x="0" y="405.0" font-size="12" font-weight="bold" fill="#e65100">
              Patch Embedding:
            </ns0:text>
            <ns0:text x="0" y="430.0" font-size="11" fill="#1a237e">
              • Split 224×224 image into 14×14 = 196 patches of 16×16 pixels
            </ns0:text>
            <ns0:text x="0" y="455.0" font-size="11" fill="#1a237e">
              • Flatten each patch: 16×16×3 = 768 dimensions
            </ns0:text>
            <ns0:text x="0" y="480.0" font-size="11" fill="#1a237e">
              • Linear projection to D-dimensional embedding (e.g., D=768)
            </ns0:text>
          </ns0:g>
          <ns0:g transform="translate(0, 95)">
            <ns0:text x="0" y="505.0" font-size="12" font-weight="bold" fill="#c62828">
              [CLS] Token:
            </ns0:text>
            <ns0:text x="0" y="530.0" font-size="11" fill="#1a237e">
              • Prepend learnable [CLS] token to sequence
            </ns0:text>
            <ns0:text x="0" y="555.0" font-size="11" fill="#1a237e">
              • Final [CLS] representation used for classification
            </ns0:text>
          </ns0:g>
          <ns0:g transform="translate(0, 145)">
            <ns0:text x="0" y="580.0" font-size="12" font-weight="bold" fill="#1565c0">
              Position Embedding:
            </ns0:text>
            <ns0:text x="0" y="605.0" font-size="11" fill="#1a237e">
              • Add learnable positional embeddings (no spatial structure assumption)
            </ns0:text>
            <ns0:text x="0" y="630.0" font-size="11" fill="#1a237e">
              • 1D position encoding (unlike 2D in CNNs)
            </ns0:text>
          </ns0:g>
          <ns0:g transform="translate(0, 195)">
            <ns0:text x="0" y="655.0" font-size="12" font-weight="bold" fill="#6a1b9a">
              Transformer Encoder:
            </ns0:text>
            <ns0:text x="0" y="680.0" font-size="11" fill="#1a237e">
              • Stack of L transformer blocks (L=12 for ViT-Base)
            </ns0:text>
            <ns0:text x="0" y="705.0" font-size="11" fill="#1a237e">
              • Each block: Multi-Head Self-Attention → LayerNorm → MLP → LayerNorm
            </ns0:text>
            <ns0:text x="0" y="730.0" font-size="11" fill="#1a237e">
              • Global receptive field from layer 1! (unlike CNNs)
            </ns0:text>
          </ns0:g>
          <ns0:g transform="translate(0, 265)">
            <ns0:text x="0" y="755.0" font-size="12" font-weight="bold" fill="#2e7d32">
              Classification Head:
            </ns0:text>
            <ns0:text x="0" y="780.0" font-size="11" fill="#1a237e">
              • Take [CLS] token output from final layer
            </ns0:text>
            <ns0:text x="0" y="805.0" font-size="11" fill="#1a237e">
              • Pass through MLP head → softmax → class probabilities
            </ns0:text>
          </ns0:g>
          <ns0:g transform="translate(0, 320)">
            <ns0:rect x="0" y="0" width="660" height="30" fill="#1a237e" stroke="#28a745" stroke-width="1" rx="3"/>
            <ns0:text x="10" y="830.0" font-size="11" font-family="Arial, sans-serif" fill="#145a32">
              z₀ = [x_cls; x₁E; x₂E; ...; x_NE] + E_pos     // Input to Transformer
            </ns0:text>
          </ns0:g>
        </ns0:g>
      </ns0:g>
    </ns0:g>
  </ns0:g>
  <ns0:g transform="translate(50, 780)">
    <ns0:text x="500" y="855.0" font-size="16" font-weight="bold" text-anchor="middle" fill="#6a1b9a">
      3. ViT Variants &amp; Key Insights
    </ns0:text>
    <ns0:rect x="0" y="20" width="2250.0" height="90" fill="#1a237e" stroke="#fd7e14" stroke-width="3" rx="8"/>
    <ns0:g transform="translate(30, 50)">
      <ns0:text x="0" y="880.0" font-size="13" fill="#1a237e">
        • 
        <ns0:tspan font-weight="bold" fill="#d84315">ViT-Base/Large/Huge:</ns0:tspan>
        86M / 307M / 632M parameters (comparable to BERT)
      </ns0:text>
      <ns0:text x="0" y="905.0" font-size="13" fill="#1a237e">
        • 
        <ns0:tspan font-weight="bold" fill="#d84315">Requires large datasets:</ns0:tspan>
        ImageNet-21k or JFT-300M pretraining → Then fine-tune
      </ns0:text>
      <ns0:text x="0" y="930.0" font-size="13" fill="#2e7d32" font-weight="bold">
        • 
        <ns0:tspan fill="#145a32">Performance:</ns0:tspan>
        Matches or beats ResNets when pretrained on large datasets, scales better with data/compute
      </ns0:text>
    </ns0:g>
  </ns0:g>
</ns0:svg>
