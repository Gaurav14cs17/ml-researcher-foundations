<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=120&section=header&text=Computational%20Graph&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-06-45B7D1?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/backpropagation-computational-graph.svg" width="100%">

*Caption: A computational graph shows how data flows forward through operations (blue arrows) and how gradients flow backward (red arrows). Each node represents an operation, and edges represent data flow. This is the foundation of automatic differentiation in PyTorch and TensorFlow.*

---

## ğŸ“‚ Overview

A computational graph is a directed acyclic graph (DAG) where nodes represent operations and edges represent data dependencies. Understanding this is essential for:
- How PyTorch/TensorFlow compute gradients
- Memory optimization (gradient checkpointing)
- Custom autograd functions

---

## ğŸ“ Mathematical Foundation

### Forward Pass

```
y = f(x) = f_n âˆ˜ f_{n-1} âˆ˜ ... âˆ˜ f_1(x)

Each operation f_i:
    z_0 = x                    (input)
    z_1 = f_1(z_0)            (first operation)
    z_2 = f_2(z_1)            (second operation)
    ...
    z_n = f_n(z_{n-1}) = y    (output)
```

### Backward Pass (Chain Rule)

```
âˆ‚y/âˆ‚x = âˆ‚y/âˆ‚z_{n-1} Â· âˆ‚z_{n-1}/âˆ‚z_{n-2} Â· ... Â· âˆ‚z_1/âˆ‚x

        = âˆ‚f_n/âˆ‚z_{n-1} Â· âˆ‚f_{n-1}/âˆ‚z_{n-2} Â· ... Â· âˆ‚f_1/âˆ‚x
```

### Example: Simple Network

```
Graph:    x â†’ [Ã—w] â†’ z â†’ [+b] â†’ a â†’ [Ïƒ] â†’ y

Forward:
    z = w * x
    a = z + b
    y = Ïƒ(a)

Backward (given âˆ‚L/âˆ‚y):
    âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚y Â· Ïƒ'(a)
    âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚a Â· 1 = âˆ‚L/âˆ‚a
    âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Â· 1 = âˆ‚L/âˆ‚a
    âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚z Â· x
    âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· w
```

---

## ğŸ“Š Graph Structure

### Node Types

| Type | Forward | Backward | Example |
|------|---------|----------|---------|
| **Leaf** | Input data | Accumulate gradients | Weights, inputs |
| **Unary** | f(x) â†’ y | grad Ã— f'(x) | ReLU, exp |
| **Binary** | f(x,y) â†’ z | Partial derivatives | +, Ã—, matmul |
| **Reduction** | Î£x â†’ scalar | Broadcast gradient | sum, mean |

### DAG Properties

```
Computational Graph G = (V, E)

V: Vertices (operations + values)
E: Directed edges (data flow)

Properties:
1. Acyclic (no loops)
2. Sources = inputs/parameters
3. Sinks = outputs
4. Topological order for forward/backward
```

---

## ğŸ’» Code Examples

### Building a Graph Manually

```python
import numpy as np

class Value:
    """Scalar value with gradient tracking"""
    
    def __init__(self, data, children=(), op=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(children)
        self._op = op
    
    def __repr__(self):
        return f"Value(data={self.data}, grad={self.grad})"
    
    def __add__(self, other):
        out = Value(self.data + other.data, (self, other), '+')
        
        def _backward():
            self.grad += out.grad  # âˆ‚L/âˆ‚self = âˆ‚L/âˆ‚out Â· 1
            other.grad += out.grad  # âˆ‚L/âˆ‚other = âˆ‚L/âˆ‚out Â· 1
        
        out._backward = _backward
        return out
    
    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other), '*')
        
        def _backward():
            self.grad += other.data * out.grad  # âˆ‚L/âˆ‚self = âˆ‚L/âˆ‚out Â· other
            other.grad += self.data * out.grad  # âˆ‚L/âˆ‚other = âˆ‚L/âˆ‚out Â· self
        
        out._backward = _backward
        return out
    
    def relu(self):
        out = Value(max(0, self.data), (self,), 'ReLU')
        
        def _backward():
            self.grad += (out.data > 0) * out.grad
        
        out._backward = _backward
        return out
    
    def backward(self):
        """Topological sort + backward pass"""
        topo = []
        visited = set()
        
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        
        build_topo(self)
        
        self.grad = 1.0
        for v in reversed(topo):
            v._backward()

# Example usage
x = Value(2.0)
w = Value(-3.0)
b = Value(1.0)

# Forward: y = relu(w*x + b)
z = w * x
a = z + b
y = a.relu()

print(f"Forward: y = {y.data}")  # y = 0 (since -6 + 1 = -5, relu = 0)

# Backward
y.backward()
print(f"âˆ‚y/âˆ‚w = {w.grad}")
print(f"âˆ‚y/âˆ‚x = {x.grad}")
print(f"âˆ‚y/âˆ‚b = {b.grad}")
```

### PyTorch Computational Graph

```python
import torch

# Create tensors with gradient tracking
x = torch.tensor([2.0], requires_grad=True)
w = torch.tensor([-3.0], requires_grad=True)
b = torch.tensor([1.0], requires_grad=True)

# Forward pass (builds graph)
z = w * x        # Graph: x,w â†’ z
a = z + b        # Graph: z,b â†’ a
y = torch.relu(a)  # Graph: a â†’ y

# Inspect the graph
print(y.grad_fn)  # ReluBackward
print(y.grad_fn.next_functions)  # AddBackward, ...

# Backward pass (traverses graph)
y.backward()

print(f"âˆ‚y/âˆ‚w = {w.grad}")
print(f"âˆ‚y/âˆ‚x = {x.grad}")
print(f"âˆ‚y/âˆ‚b = {b.grad}")
```

### Visualizing the Graph

```python
from torchviz import make_dot

# Create a simple model
x = torch.randn(1, 10, requires_grad=True)
model = torch.nn.Sequential(
    torch.nn.Linear(10, 5),
    torch.nn.ReLU(),
    torch.nn.Linear(5, 1)
)
y = model(x)

# Visualize
dot = make_dot(y, params=dict(model.named_parameters()))
dot.render("computational_graph", format="png")
```

---

## ğŸ”‘ Key Concepts

| Concept | Description |
|---------|-------------|
| **Forward Mode** | Compute âˆ‚output/âˆ‚input, one input at a time |
| **Backward Mode** | Compute âˆ‚output/âˆ‚all_inputs simultaneously |
| **Gradient Accumulation** | Gradients add up at fan-in nodes |
| **Gradient Checkpointing** | Trade memory for compute |
| **Dynamic Graph** | Build graph during forward (PyTorch) |
| **Static Graph** | Define graph once (TensorFlow 1.x) |

---

## ğŸ“ DETAILED MATHEMATICAL THEORY

### 1. The Chain Rule: Foundation of Backpropagation

**Single-Variable Chain Rule:**

```
If y = f(g(x)), then:
  dy/dx = df/dg Â· dg/dx

Example: y = sin(xÂ²)
  Let u = xÂ²
  y = sin(u)
  
  dy/dx = dy/du Â· du/dx
        = cos(u) Â· 2x
        = 2xÂ·cos(xÂ²)
```

**Multivariable Chain Rule (The Key!):**

```
If y depends on x through intermediate variables zâ‚,...,zâ‚™:
  
  y = f(zâ‚, zâ‚‚, ..., zâ‚™)
  z_i = g_i(xâ‚, xâ‚‚, ..., x_m)

Then:
  âˆ‚y/âˆ‚x_j = Î£áµ¢ (âˆ‚y/âˆ‚z_i) Â· (âˆ‚z_i/âˆ‚x_j)

This is the mathematical heart of backpropagation!
```

**Vector-to-Scalar Case (Neural Network Loss):**

```
Loss L: â„â¿ â†’ â„ (scalar output)

Chain rule for gradient:
  âˆ‚L/âˆ‚x = (âˆ‚L/âˆ‚z)áµ€ Â· (âˆ‚z/âˆ‚x)

where:
  âˆ‚L/âˆ‚z âˆˆ â„áµ = gradient wrt intermediate z
  âˆ‚z/âˆ‚x âˆˆ â„áµË£â¿ = Jacobian matrix
  âˆ‚L/âˆ‚x âˆˆ â„â¿ = gradient wrt input x

Matrix multiplication accumulates gradients through layers!
```

---

### 2. Computational Graph as DAG

**Formal Definition:**

```
Computational Graph G = (V, E, Ï†)

V = {vâ‚, vâ‚‚, ..., vâ‚™} = vertices (nodes)
E âŠ† V Ã— V = directed edges
Ï†: V â†’ Operations = node labels

Each node v_i:
  â€¢ Has value x_i (forward pass)
  â€¢ Has gradient âˆ‚L/âˆ‚x_i (backward pass)
  â€¢ Represents operation: x_i = Ï†(x_{pa(i)})
  
where pa(i) = parents of node i

Constraints:
  1. DAG: No cycles (topological order exists)
  2. Sources: Input nodes (no incoming edges)
  3. Sink: Output node (typically loss L)
```

**Topological Ordering:**

```
A topological order is a linear ordering of vertices such that:
  For every edge (u,v), u comes before v

Algorithm (DFS-based):
  1. Visit all nodes reachable from output
  2. Add node to list after visiting all children
  3. Reverse the list

Example:
  Graph: a â†’ b â†’ d
         â†˜câ†—
  
  Topological order: [a, b, c, d] or [a, c, b, d]
  
Used in:
  â€¢ Forward pass: Process nodes in topological order
  â€¢ Backward pass: Process nodes in reverse topological order
```

---

### 3. Forward Pass: Computing Function Values

**Algorithm:**

```
Input: x (input data), Î¸ (parameters)
Output: y (network output)

For each node v in topological order:
  1. Get inputs: x_pa = {x_u : u âˆˆ parents(v)}
  2. Compute: x_v = Ï†_v(x_pa)
  3. Store x_v for backward pass

Return x_output (typically goes into loss function)
```

**Example: 2-Layer Network**

```
Network: x â†’ [Wâ‚] â†’ zâ‚ â†’ [Ïƒ] â†’ aâ‚ â†’ [Wâ‚‚] â†’ zâ‚‚ â†’ [Ïƒ] â†’ aâ‚‚ â†’ [loss] â†’ L

Forward computation:
  zâ‚ = Wâ‚Â·x
  aâ‚ = Ïƒ(zâ‚)
  zâ‚‚ = Wâ‚‚Â·aâ‚
  aâ‚‚ = Ïƒ(zâ‚‚)
  L = â„“(aâ‚‚, y)  # Loss function

Each arrow is an operation node in the graph
Values stored: {x, zâ‚, aâ‚, zâ‚‚, aâ‚‚, L}
```

**Memory Cost:**

```
For network with L layers, each layer width d:
  Forward pass stores: O(LÂ·d) activations

Deep networks (L=100, d=1000):
  Memory â‰ˆ 100k floats = 400KB per sample
  Batch of 32: â‰ˆ 12.8MB

This is why deep learning needs GPUs with large memory!
```

---

### 4. Backward Pass: Computing Gradients

**Algorithm (Reverse-Mode Autodiff):**

```
Input: âˆ‚L/âˆ‚x_output = 1 (gradient at output)
Output: {âˆ‚L/âˆ‚Î¸_i} for all parameters Î¸_i

Initialize: âˆ‚L/âˆ‚v = 0 for all nodes v

Set: âˆ‚L/âˆ‚(output) = 1

For each node v in reverse topological order:
  For each parent u of v:
    âˆ‚L/âˆ‚u += âˆ‚L/âˆ‚v Â· âˆ‚v/âˆ‚u  # Chain rule!

Return {âˆ‚L/âˆ‚Î¸_i}
```

**Key Insight: Gradient Accumulation at Fan-In**

```
If node z has multiple children {yâ‚, yâ‚‚, ...}:

  âˆ‚L/âˆ‚z = Î£áµ¢ âˆ‚L/âˆ‚yáµ¢ Â· âˆ‚yáµ¢/âˆ‚z

Example:
     x
    / \
   yâ‚  yâ‚‚
    \ /
     L

âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚yâ‚Â·âˆ‚yâ‚/âˆ‚x + âˆ‚L/âˆ‚yâ‚‚Â·âˆ‚yâ‚‚/âˆ‚x

This is why gradients must be accumulated (+=) not overwritten (=)!
```

**Example: Complete 2-Layer Backprop**

```
Forward:
  zâ‚ = Wâ‚Â·x
  aâ‚ = Ïƒ(zâ‚)
  zâ‚‚ = Wâ‚‚Â·aâ‚
  aâ‚‚ = Ïƒ(zâ‚‚)
  L = â„“(aâ‚‚, y)

Backward (given âˆ‚L/âˆ‚L = 1):
  âˆ‚L/âˆ‚aâ‚‚ = âˆ‚â„“/âˆ‚aâ‚‚                    # Loss gradient
  âˆ‚L/âˆ‚zâ‚‚ = âˆ‚L/âˆ‚aâ‚‚ âŠ™ Ïƒ'(zâ‚‚)           # Element-wise
  âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚zâ‚‚ Â· aâ‚áµ€               # Outer product
  âˆ‚L/âˆ‚aâ‚ = Wâ‚‚áµ€ Â· âˆ‚L/âˆ‚zâ‚‚               # Matrix-vector product
  âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚aâ‚ âŠ™ Ïƒ'(zâ‚)           # Element-wise
  âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚zâ‚ Â· xáµ€                # Outer product
  âˆ‚L/âˆ‚x = Wâ‚áµ€ Â· âˆ‚L/âˆ‚zâ‚                # If needed

where âŠ™ is element-wise multiplication (Hadamard product)
```

**Computation Cost:**

```
Forward: O(LÂ·dÂ²) for L layers of width d (matrix multiplications)
Backward: O(LÂ·dÂ²) (same operations, just transposed matrices)

Total: 2Ã— forward pass (roughly)

Key: Backward pass is NOT more expensive than forward!
```

---

### 5. Jacobian Matrix and Vector-Jacobian Products

**Jacobian Definition:**

```
For f: â„â¿ â†’ â„áµ
  
Jacobian J âˆˆ â„áµË£â¿:
  J_ij = âˆ‚f_i/âˆ‚x_j

Example: f(xâ‚,xâ‚‚) = [xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²]
  
  J = [2xâ‚    0   ]
      [xâ‚‚     xâ‚  ]
      [0      2xâ‚‚ ]
```

**Forward-Mode Autodiff (Jacobian-Vector Product):**

```
Compute: JÂ·v for vector v âˆˆ â„â¿

Cost: One forward pass per column
Total: O(n) forward passes

Good when: n << m (few inputs, many outputs)
Example: Computing âˆ‚f/âˆ‚xáµ¢ for specific i
```

**Reverse-Mode Autodiff (Vector-Jacobian Product):**

```
Compute: váµ€Â·J for vector v âˆˆ â„áµ

Cost: One backward pass
Total: O(1) backward pass for all gradients!

Good when: m << n (many inputs, few outputs)
Example: Neural network (m=1 output loss, n=millions of parameters)

This is why reverse-mode is used in deep learning!
```

**Proof that Backprop is Efficient:**

```
Network: x âˆˆ â„â¿ â†’ y âˆˆ â„Â¹ (scalar loss)

Forward-mode: Need n passes to get all âˆ‚y/âˆ‚xáµ¢
Reverse-mode: Need 1 pass to get all âˆ‚y/âˆ‚xáµ¢

For n = 1,000,000 parameters:
  Forward-mode: 1 million passes
  Reverse-mode: 1 pass

Speedup: 1,000,000Ã— !

This is why backpropagation revolutionized neural networks.
```

---

### 6. Gradient Flow Through Common Operations

**Elementwise Operations:**

```
Forward:  y = f(x)  (applied element-wise)
Backward: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y âŠ™ f'(x)

Examples:
  ReLU:    y = max(0, x),  âˆ‚y/âˆ‚x = ğŸ™{x>0}
  Sigmoid: y = Ïƒ(x),       âˆ‚y/âˆ‚x = Ïƒ(x)(1-Ïƒ(x))
  Tanh:    y = tanh(x),    âˆ‚y/âˆ‚x = 1 - tanhÂ²(x)

Gradient: Same shape as input
Cost: O(size(x))
```

**Matrix Multiplication:**

```
Forward:  Y = XW  where X âˆˆ â„áµË£â¿, W âˆˆ â„â¿Ë£áµ–, Y âˆˆ â„áµË£áµ–

Backward:
  âˆ‚L/âˆ‚X = (âˆ‚L/âˆ‚Y)Â·Wáµ€   # â„áµË£â¿
  âˆ‚L/âˆ‚W = Xáµ€Â·(âˆ‚L/âˆ‚Y)   # â„â¿Ë£áµ–

Proof (single element):
  Y_ij = Î£â‚– X_ikÂ·W_kj
  
  âˆ‚Y_ij/âˆ‚X_il = Î£â‚– Î´_ikÂ·W_kj = W_lj
  âˆ‚Y_ij/âˆ‚W_kl = Î£â‚– X_ikÂ·Î´_klÂ·Î´_lj = X_ikÂ·Î´_lj
  
Sum over i,j:
  âˆ‚L/âˆ‚X_il = Î£â±¼ (âˆ‚L/âˆ‚Y_ij)Â·W_lj = [(âˆ‚L/âˆ‚Y)Â·Wáµ€]_il âœ“
  âˆ‚L/âˆ‚W_kl = Î£áµ¢ (âˆ‚L/âˆ‚Y_ij)Â·X_ik = [Xáµ€Â·(âˆ‚L/âˆ‚Y)]_kl âœ“

Memory: Store X and W for backward pass
```

**Sum/Mean (Reduction Operations):**

```
Forward:  y = Î£áµ¢ xáµ¢  (or mean)
Backward: âˆ‚L/âˆ‚xáµ¢ = âˆ‚L/âˆ‚y  (broadcast gradient)

Intuition: All inputs contribute equally to output,
           so gradient flows equally to all

Example:
  x = [1, 2, 3], y = sum(x) = 6
  âˆ‚L/âˆ‚y = 5
  âˆ‚L/âˆ‚x = [5, 5, 5]  (broadcast)

For mean: âˆ‚L/âˆ‚xáµ¢ = (âˆ‚L/âˆ‚y)/n
```

**Indexing/Slicing:**

```
Forward:  y = x[i]  (select element)
Backward: âˆ‚L/âˆ‚x has âˆ‚L/âˆ‚y at position i, zeros elsewhere

Example:
  x = [1, 2, 3, 4], y = x[1] = 2
  âˆ‚L/âˆ‚y = 7
  âˆ‚L/âˆ‚x = [0, 7, 0, 0]

Gather/scatter operations in neural networks!
```

**Concatenation:**

```
Forward:  z = concat(x, y)
Backward: Split âˆ‚L/âˆ‚z into âˆ‚L/âˆ‚x and âˆ‚L/âˆ‚y

Example:
  x = [1, 2], y = [3, 4], z = [1, 2, 3, 4]
  âˆ‚L/âˆ‚z = [a, b, c, d]
  âˆ‚L/âˆ‚x = [a, b]
  âˆ‚L/âˆ‚y = [c, d]
```

---

### 7. Gradient Checkpointing: Trading Compute for Memory

**Problem:**

```
Deep network (100 layers):
  Forward pass stores 100 intermediate activations
  Memory: O(depth) = O(100)

For ResNet-152 on ImageNet:
  Batch 256 requires ~16GB GPU memory!
  Can't fit larger batches
```

**Solution: Gradient Checkpointing**

```
Idea: Don't store all activations
      Recompute them during backward pass

Strategy:
  1. Store only every k-th activation (checkpoints)
  2. During backward: Recompute from nearest checkpoint

Memory: O(depth/k)
Extra Compute: O(k) recomputations

Optimal k = âˆšdepth:
  Memory: O(âˆšdepth)
  Compute: O(âˆšdepth) extra forward passes
```

**Algorithm:**

```
Forward:
  Store: xâ‚€, x_âˆšn, x_{2âˆšn}, ..., xâ‚™  (âˆšn checkpoints)
  Discard: All other activations

Backward (for layer i):
  1. Find nearest checkpoint j â‰¤ i
  2. Recompute: x_j â†’ x_{j+1} â†’ ... â†’ x_i
  3. Compute gradient: âˆ‚L/âˆ‚x_i
  4. Discard recomputed activations

Cost analysis (for n layers):
  Normal: n forward, n backward = 2n total
  Checkpointing: n + nâˆšn forward, n backward = n(1+âˆšn) + n
  
  n = 100: 2Ã—100 = 200 vs 100(1+10) + 100 = 1200
  Overhead: 6Ã— compute for 10Ã— memory savings
```

**PyTorch Implementation:**

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(100, 100) for _ in range(100)
        ])
    
    def forward(self, x):

        # Wrap expensive layers in checkpoint
        for i, layer in enumerate(self.layers):
            if i % 10 == 0:  # Checkpoint every 10 layers
                x = checkpoint(layer, x)
            else:
                x = layer(x)
        return x

# Memory usage: ~10Ã— less than without checkpointing!
```

---

### 8. Dynamic vs Static Computational Graphs

**Static Graph (TensorFlow 1.x):**

```
Pros:
  â€¢ Graph optimization (constant folding, op fusion)
  â€¢ Can serialize for deployment
  â€¢ Memory planning (know all shapes ahead)

Cons:
  â€¢ Less flexible (no Python control flow)
  â€¢ Harder to debug
  â€¢ Separate build and run phases

Example:
  graph = tf.Graph()
  with graph.as_default():
      x = tf.placeholder(tf.float32, [None, 784])
      W = tf.Variable(tf.zeros([784, 10]))
      y = tf.matmul(x, W)
  
  session = tf.Session(graph=graph)
  session.run(y, feed_dict={x: data})
```

**Dynamic Graph (PyTorch):**

```
Pros:
  â€¢ Natural Python control flow
  â€¢ Easy debugging (standard Python debugger)
  â€¢ Flexible architectures (RNNs, trees)

Cons:
  â€¢ Less optimization opportunities
  â€¢ Slight overhead rebuilding graph each iteration

Example:
  x = torch.randn(batch_size, 784, requires_grad=True)
  W = torch.randn(784, 10, requires_grad=True)
  y = x @ W  # Graph built on-the-fly
  y.backward(torch.ones_like(y))  # Graph freed after backward
```

**Modern Approach:**

```
Both frameworks now support both modes:
  â€¢ TensorFlow 2.x: Eager execution by default
  â€¢ PyTorch: torch.jit for static graphs

Trend: Dynamic for research, compile for production
```

---

### 9. Higher-Order Derivatives

**Second-Order Derivatives (Hessian):**

```
First derivative: âˆ‚L/âˆ‚Î¸
Second derivative: âˆ‚Â²L/âˆ‚Î¸Â² = âˆ‚/âˆ‚Î¸(âˆ‚L/âˆ‚Î¸)

In computational graph:
  1. Compute forward pass: L(Î¸)
  2. Compute backward pass: g = âˆ‚L/âˆ‚Î¸
  3. Compute backward on g: H = âˆ‚g/âˆ‚Î¸ = âˆ‚Â²L/âˆ‚Î¸Â²

PyTorch:
  x = torch.tensor([2.0], requires_grad=True)
  y = x**3
  grad_y = torch.autograd.grad(y, x, create_graph=True)[0]
  grad2_y = torch.autograd.grad(grad_y, x)[0]  # Second derivative
  print(grad2_y)  # 12.0 = 6*2
```

**Applications:**

```
1. Newton's Method:
   Î¸_new = Î¸ - Hâ»Â¹Â·g
   
2. Hessian-free optimization:
   Compute HÂ·v without full H
   
3. Adversarial training:
   Regularize by gradient penalty
   
4. Meta-learning:
   MAML requires second-order gradients
```

---

### 10. Custom Autograd Functions

**When to Use:**

```
1. Operation not in PyTorch/TensorFlow
2. Need custom backward pass for efficiency
3. Want to stop gradients in specific way
4. Implement non-differentiable operations with surrogate gradients
```

**PyTorch Example:**

```python
class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        """
        ctx: context object to store information for backward
        """
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        """
        grad_output: âˆ‚L/âˆ‚output
        returns: âˆ‚L/âˆ‚input
        """
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0  # Zero gradient where input < 0
        return grad_input

# Usage
relu = MyReLU.apply
x = torch.randn(10, requires_grad=True)
y = relu(x)
y.sum().backward()
```

**Straight-Through Estimator (STE):**

```python
class QuantizeFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):

        # Non-differentiable quantization
        return torch.round(input)
    
    @staticmethod
    def backward(ctx, grad_output):

        # Straight-through: pass gradient as-is
        return grad_output

# Allows training with discrete operations!
```

---

## ğŸ“Š Memory Considerations

```
Memory Usage:

Forward:  Store all intermediate activations
          O(depth Ã— width)

Backward: Reuse stored activations + gradients
          O(depth Ã— width)

Total:    ~2Ã— forward pass memory

Gradient Checkpointing:
    Don't store all activations
    Recompute during backward
    Trade O(âˆšn) memory for O(âˆšn) extra compute
```

---

## ğŸ”— Connection to Other Topics

```
Computational Graph
    |
    +-- Automatic Differentiation
    |   +-- Forward Mode (for few outputs)
    |   +-- Backward Mode (for many outputs) â† Used in DL
    |
    +-- Chain Rule (mathematical foundation)
    |
    +-- Implementations
        +-- PyTorch (dynamic graph)
        +-- TensorFlow (static/eager)
        +-- JAX (functional transforms)
```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Autodiff | [../autodiff/](../autodiff/) |
| ğŸ“– | Gradient Flow | [../gradient-flow/](../gradient-flow/) |
| ğŸ¥ | Karpathy: micrograd | [YouTube](https://www.youtube.com/watch?v=VMj-3S1tku0) |
| ğŸ“„ | PyTorch Autograd | [Docs](https://pytorch.org/docs/stable/autograd.html) |
| ğŸ‡¨ğŸ‡³ | è®¡ç®—å›¾ä¸è‡ªåŠ¨å¾®åˆ† | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/61287482) |
| ğŸ‡¨ğŸ‡³ | PyTorchè®¡ç®—å›¾æœºåˆ¶ | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88661776) |
| ğŸ‡¨ğŸ‡³ | æ‰‹å†™autograd | [Bç«™](https://www.bilibili.com/video/BV1Le4y1s7HH) |
| ğŸ‡¨ğŸ‡³ | æ·±åº¦å­¦ä¹ æ¡†æ¶åŸç† | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2019-06-28-3)

## ğŸ”— Where This Topic Is Used

| Application | Computational Graph |
|-------------|-------------------|
| **Forward Pass** | Build graph |
| **Backward Pass** | Traverse for gradients |
| **Checkpointing** | Trade compute for memory |
| **JIT Compilation** | Graph optimization |

---

â¬…ï¸ [Back: Autodiff](../01_autodiff/README.md) | â¡ï¸ [Next: Gradient Flow](../03_gradient_flow/README.md)

---

â¬…ï¸ [Back: Backpropagation](../../README.md)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=80&section=footer" width="100%"/>
</p>
