<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=120&section=header&text=Backpropagation&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-06-45B7D1?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./computational-graph/images/backpropagation-computational-graph.svg" width="100%">

*Caption: Backpropagation uses the chain rule to compute gradients through a computational graph. Forward pass (blue) computes activations; backward pass (red) propagates gradients. This is the foundation of training every neural network.*

---

## ğŸ“‚ Topics

| Folder | Topic | Key Concepts |
|--------|-------|--------------|
| [autodiff/](./autodiff/) | Automatic differentiation | Forward, reverse mode |
| [computational-graph/](./computational-graph/) | DAG representation | PyTorch autograd |
| [gradient-flow/](./gradient-flow/) | Gradient problems | Vanishing, exploding |

---

## ğŸ“ The Algorithm

```
Forward pass: Compute loss
    x â†’ hâ‚ â†’ hâ‚‚ â†’ ... â†’ Å· â†’ L

Backward pass: Compute gradients (chain rule!)
    âˆ‚L/âˆ‚Wâ‚ â† âˆ‚L/âˆ‚hâ‚ â† âˆ‚L/âˆ‚hâ‚‚ â† ... â† âˆ‚L/âˆ‚Å· â† âˆ‚L/âˆ‚L

Update: Gradient descent
    W â† W - Î± Â· âˆ‚L/âˆ‚W
```

---

## ğŸ”‘ Chain Rule

```
âˆ‚L/âˆ‚Wâ‚— = âˆ‚L/âˆ‚hâ‚— Â· âˆ‚hâ‚—/âˆ‚Wâ‚—

Where:
âˆ‚L/âˆ‚hâ‚— = âˆ‚L/âˆ‚hâ‚—â‚Šâ‚ Â· âˆ‚hâ‚—â‚Šâ‚/âˆ‚hâ‚—  (recursive!)
```

---

## ğŸ“ DETAILED MATHEMATICAL DERIVATION

### 1. Complete Backpropagation Derivation

**Problem:** 2-layer neural network
```
Input: x âˆˆ â„â¿
Layer 1: h = Ïƒ(Wâ‚x + bâ‚)     where Ïƒ(z) = max(0, z)  (ReLU)
Layer 2: Å· = Wâ‚‚h + bâ‚‚
Loss: L = 1/2||Å· - y||Â²
```

**Goal:** Compute âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚, âˆ‚L/âˆ‚Wâ‚‚, âˆ‚L/âˆ‚bâ‚‚

---

**Step 1: Forward pass (compute all activations)**

```python

# Forward pass
zâ‚ = Wâ‚x + bâ‚        # Pre-activation layer 1
h = ReLU(zâ‚)          # Activation layer 1
zâ‚‚ = Wâ‚‚h + bâ‚‚        # Pre-activation layer 2
Å· = zâ‚‚                # Output (linear)
L = 1/2 * ||Å· - y||Â²  # Loss
```

**Step 2: Backward pass (compute gradients)**

**2.1: Gradient at output**
```
âˆ‚L/âˆ‚Å· = Å· - y                    [shape: m Ã— 1]

Why? 
L = 1/2 Î£áµ¢(Å·áµ¢ - yáµ¢)Â²
âˆ‚L/âˆ‚Å·áµ¢ = Å·áµ¢ - yáµ¢
```

**2.2: Gradient of Wâ‚‚**
```
âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚Wâ‚‚
       = (Å· - y) Â· háµ€            [shape: m Ã— n]

Detailed:
Å·â±¼ = Î£â‚– Wâ‚‚â±¼â‚–hâ‚– + bâ‚‚â±¼
âˆ‚Å·â±¼/âˆ‚Wâ‚‚â±¼â‚– = hâ‚–
âˆ‚L/âˆ‚Wâ‚‚â±¼â‚– = (Å·â±¼ - yâ±¼) Â· hâ‚–
```

**2.3: Gradient of bâ‚‚**
```
âˆ‚L/âˆ‚bâ‚‚ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚bâ‚‚
       = Å· - y                   [shape: m Ã— 1]

Why?
Å·â±¼ = Î£â‚– Wâ‚‚â±¼â‚–hâ‚– + bâ‚‚â±¼
âˆ‚Å·â±¼/âˆ‚bâ‚‚â±¼ = 1
```

**2.4: Gradient of h (chain rule!)**
```
âˆ‚L/âˆ‚h = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚h
      = Wâ‚‚áµ€(Å· - y)              [shape: n Ã— 1]

Detailed:
Å·â±¼ = Î£â‚– Wâ‚‚â±¼â‚–hâ‚– + bâ‚‚â±¼
âˆ‚Å·â±¼/âˆ‚hâ‚– = Wâ‚‚â±¼â‚–
âˆ‚L/âˆ‚hâ‚– = Î£â±¼ âˆ‚L/âˆ‚Å·â±¼ Â· âˆ‚Å·â±¼/âˆ‚hâ‚–
       = Î£â±¼ (Å·â±¼ - yâ±¼) Â· Wâ‚‚â±¼â‚–
       = [Wâ‚‚áµ€(Å· - y)]â‚–
```

**2.5: Gradient of zâ‚ (through ReLU)**
```
âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚h Â· âˆ‚h/âˆ‚zâ‚
       = âˆ‚L/âˆ‚h âŠ™ ğŸ™{zâ‚ > 0}      [shape: n Ã— 1]

Why?
h = ReLU(zâ‚) = max(0, zâ‚)
âˆ‚h/âˆ‚zâ‚ = { 1 if zâ‚ > 0
         { 0 if zâ‚ â‰¤ 0

âŠ™ denotes element-wise multiplication
```

**2.6: Gradient of Wâ‚**
```
âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚Wâ‚
       = âˆ‚L/âˆ‚zâ‚ Â· xáµ€             [shape: n Ã— d]

Detailed:
zâ‚áµ¢ = Î£â‚– Wâ‚áµ¢â‚–xâ‚– + bâ‚áµ¢
âˆ‚zâ‚áµ¢/âˆ‚Wâ‚áµ¢â‚– = xâ‚–
âˆ‚L/âˆ‚Wâ‚áµ¢â‚– = âˆ‚L/âˆ‚zâ‚áµ¢ Â· xâ‚–
```

**2.7: Gradient of bâ‚**
```
âˆ‚L/âˆ‚bâ‚ = âˆ‚L/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚bâ‚
       = âˆ‚L/âˆ‚zâ‚                  [shape: n Ã— 1]
```

---

### 2. Complete Implementation with All Steps

```python
import numpy as np

def relu(z):
    """ReLU activation"""
    return np.maximum(0, z)

def relu_derivative(z):
    """ReLU derivative"""
    return (z > 0).astype(float)

def forward_pass(x, W1, b1, W2, b2, y):
    """
    Forward pass with detailed intermediate values
    Returns: all intermediate values + loss
    """

    # Layer 1
    z1 = W1 @ x + b1          # Pre-activation [n Ã— 1]
    h = relu(z1)               # Activation [n Ã— 1]
    
    # Layer 2
    z2 = W2 @ h + b2          # Pre-activation [m Ã— 1]
    y_pred = z2                # Output [m Ã— 1]
    
    # Loss
    loss = 0.5 * np.sum((y_pred - y)**2)
    
    # Cache for backward pass
    cache = {
        'x': x, 'z1': z1, 'h': h, 'z2': z2,
        'y_pred': y_pred, 'y': y,
        'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2
    }
    
    return y_pred, loss, cache

def backward_pass(cache):
    """
    Backward pass: compute all gradients
    """

    # Extract from cache
    x = cache['x']
    z1 = cache['z1']
    h = cache['h']
    y_pred = cache['y_pred']
    y = cache['y']
    W2 = cache['W2']
    
    # === BACKWARD PASS ===
    
    # Step 1: Gradient at output
    dL_dy_pred = y_pred - y                    # [m Ã— 1]
    print(f"âˆ‚L/âˆ‚Å· shape: {dL_dy_pred.shape}")
    
    # Step 2: Gradient of W2
    dL_dW2 = dL_dy_pred @ h.T                  # [m Ã— n]
    print(f"âˆ‚L/âˆ‚Wâ‚‚ shape: {dL_dW2.shape}")
    
    # Step 3: Gradient of b2
    dL_db2 = dL_dy_pred                        # [m Ã— 1]
    print(f"âˆ‚L/âˆ‚bâ‚‚ shape: {dL_db2.shape}")
    
    # Step 4: Gradient of h (backprop through W2)
    dL_dh = W2.T @ dL_dy_pred                  # [n Ã— 1]
    print(f"âˆ‚L/âˆ‚h shape: {dL_dh.shape}")
    
    # Step 5: Gradient of z1 (backprop through ReLU)
    dL_dz1 = dL_dh * relu_derivative(z1)       # [n Ã— 1]
    print(f"âˆ‚L/âˆ‚zâ‚ shape: {dL_dz1.shape}")
    
    # Step 6: Gradient of W1
    dL_dW1 = dL_dz1 @ x.T                      # [n Ã— d]
    print(f"âˆ‚L/âˆ‚Wâ‚ shape: {dL_dW1.shape}")
    
    # Step 7: Gradient of b1
    dL_db1 = dL_dz1                            # [n Ã— 1]
    print(f"âˆ‚L/âˆ‚bâ‚ shape: {dL_db1.shape}")
    
    gradients = {
        'dW1': dL_dW1,
        'db1': dL_db1,
        'dW2': dL_dW2,
        'db2': dL_db2
    }
    
    return gradients

# Example usage
print("="*60)
print("BACKPROPAGATION DETAILED EXAMPLE")
print("="*60)

# Setup
np.random.seed(42)
d, n, m = 2, 3, 1  # Input dim, hidden dim, output dim

W1 = np.random.randn(n, d) * 0.1
b1 = np.zeros((n, 1))
W2 = np.random.randn(m, n) * 0.1
b2 = np.zeros((m, 1))

x = np.array([[1.0], [2.0]])  # Input
y = np.array([[3.0]])          # Target

print(f"\nInput x: {x.T}")
print(f"Target y: {y.T}")
print(f"\nWâ‚ shape: {W1.shape}, bâ‚ shape: {b1.shape}")
print(f"Wâ‚‚ shape: {W2.shape}, bâ‚‚ shape: {b2.shape}")

# Forward pass
y_pred, loss, cache = forward_pass(x, W1, b1, W2, b2, y)
print(f"\nForward pass:")
print(f"  Prediction: {y_pred.T}")
print(f"  Loss: {loss:.6f}")

# Backward pass
print(f"\nBackward pass (computing gradients):")
gradients = backward_pass(cache)

print(f"\nGradient magnitudes:")
for name, grad in gradients.items():
    print(f"  {name}: ||âˆ‡|| = {np.linalg.norm(grad):.6f}")
```

---

### 3. Matrix Dimensions Cheat Sheet

For batch processing (B samples):

```
Notation:
  B = batch size
  d = input dimension
  n = hidden dimension
  m = output dimension

Forward pass:
  X âˆˆ â„^(dÃ—B)      Input batch
  Wâ‚ âˆˆ â„^(nÃ—d)     Layer 1 weights
  bâ‚ âˆˆ â„^(nÃ—1)     Layer 1 bias (broadcast)
  Zâ‚ = Wâ‚X + bâ‚    [n Ã— B]
  H = Ïƒ(Zâ‚)        [n Ã— B]
  Wâ‚‚ âˆˆ â„^(mÃ—n)     Layer 2 weights
  bâ‚‚ âˆˆ â„^(mÃ—1)     Layer 2 bias
  Zâ‚‚ = Wâ‚‚H + bâ‚‚    [m Ã— B]
  Å¶ = Zâ‚‚           [m Ã— B]
  L = ||Å¶ - Y||Â²/2B [scalar]

Backward pass:
  âˆ‚L/âˆ‚Å¶ âˆˆ â„^(mÃ—B)         = (Å¶ - Y)/B
  âˆ‚L/âˆ‚Wâ‚‚ âˆˆ â„^(mÃ—n)        = (âˆ‚L/âˆ‚Å¶) Â· Háµ€
  âˆ‚L/âˆ‚bâ‚‚ âˆˆ â„^(mÃ—1)        = (âˆ‚L/âˆ‚Å¶) Â· ğŸ™  (sum over batch)
  âˆ‚L/âˆ‚H âˆˆ â„^(nÃ—B)         = Wâ‚‚áµ€ Â· (âˆ‚L/âˆ‚Å¶)
  âˆ‚L/âˆ‚Zâ‚ âˆˆ â„^(nÃ—B)        = (âˆ‚L/âˆ‚H) âŠ™ Ïƒ'(Zâ‚)
  âˆ‚L/âˆ‚Wâ‚ âˆˆ â„^(nÃ—d)        = (âˆ‚L/âˆ‚Zâ‚) Â· Xáµ€
  âˆ‚L/âˆ‚bâ‚ âˆˆ â„^(nÃ—1)        = (âˆ‚L/âˆ‚Zâ‚) Â· ğŸ™
```

**Memory tip:** Output gradient @ Input_transpose = Weight gradient

---

### 4. Common Activation Functions & Derivatives

| Activation | Formula | Derivative | Notes |
|------------|---------|------------|-------|
| **ReLU** | max(0, z) | ğŸ™{z>0} | Dead neurons if z â‰¤ 0 always |
| **Leaky ReLU** | max(Î±z, z) | ğŸ™{z>0} + Î±Â·ğŸ™{zâ‰¤0} | Î± = 0.01 typical |
| **Sigmoid** | Ïƒ(z) = 1/(1+eâ»á¶») | Ïƒ(z)(1-Ïƒ(z)) | Saturates â†’ vanishing grad |
| **Tanh** | tanh(z) | 1 - tanhÂ²(z) | Centered at 0 |
| **GELU** | zÂ·Î¦(z) | Î¦(z) + zÂ·Ï†(z) | Used in BERT, GPT |
| **Softmax** | eá¶»â±/Î£â±¼eá¶»Ê² | sáµ¢(Î´áµ¢â±¼ - sâ±¼) | For classification |

**GELU derivation** (used in Transformers):
```
GELU(x) = x Â· Î¦(x)    where Î¦(x) = P(X â‰¤ x), X ~ N(0,1)

Approximation:
GELU(x) â‰ˆ 0.5x(1 + tanh(âˆš(2/Ï€)(x + 0.044715xÂ³)))

Derivative:
GELU'(x) = Î¦(x) + xÂ·Ï†(x)    where Ï†(x) = e^(-xÂ²/2)/âˆš(2Ï€)
```

---

### 5. Vanishing/Exploding Gradients

**Problem:** In deep networks (L layers):
```
âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚hâ‚— Â· âˆ‚hâ‚—/âˆ‚hâ‚—â‚‹â‚ Â· ... Â· âˆ‚hâ‚‚/âˆ‚hâ‚ Â· âˆ‚hâ‚/âˆ‚Wâ‚

Product of L terms!
```

**Vanishing:** If each âˆ‚hâ‚—/âˆ‚hâ‚—â‚‹â‚ < 1:
```
||âˆ‚L/âˆ‚Wâ‚|| â‰ˆ (0.5)^L â†’ 0  as L â†’ âˆ

Early layers don't learn!
```

**Exploding:** If each âˆ‚hâ‚—/âˆ‚hâ‚—â‚‹â‚ > 1:
```
||âˆ‚L/âˆ‚Wâ‚|| â‰ˆ (2)^L â†’ âˆ  as L â†’ âˆ

Gradient overflow (NaN)!
```

**Solutions:**

1. **Residual connections** (ResNet):
```
h_{l+1} = Ïƒ(W_l h_l) + h_l    (skip connection)

âˆ‚h_{l+1}/âˆ‚h_l = âˆ‚Ïƒ/âˆ‚h_l + I

Gradient can flow directly through identity!
```

2. **Layer normalization:**
```
h_norm = (h - Î¼)/Ïƒ

Keeps activations in reasonable range
â†’ Gradients don't explode/vanish
```

3. **Gradient clipping:**
```python
if ||g|| > threshold:
    g = g * (threshold / ||g||)
```

4. **Careful initialization** (Xavier/He):
```
Xavier:  W ~ N(0, 2/(n_in + n_out))
He:      W ~ N(0, 2/n_in)  # For ReLU

Keeps variance of activations constant across layers
```

---

### 6. Computational Graph Perspective

```
Computational graph for z = f(x, y) = xÂ·y + sin(x):

         x --+--â†’ [Ã—] ---â†’ [+] --â†’ z
             |      â†‘       â†‘
             |      |       |
             +--â†’ [sin]  ---+
                    â†‘
         y ---------+

Forward pass: Compute z (left to right)
Backward pass: Compute âˆ‚z/âˆ‚x, âˆ‚z/âˆ‚y (right to left)

Chain rule automatically applied!
```

**Example: âˆ‚z/âˆ‚x**
```
z = xÂ·y + sin(x)

âˆ‚z/âˆ‚x = âˆ‚/âˆ‚x(xÂ·y) + âˆ‚/âˆ‚x(sin(x))
      = y + cos(x)

Graph perspective:
âˆ‚z/âˆ‚x = âˆ‚z/âˆ‚(xÂ·y) Â· âˆ‚(xÂ·y)/âˆ‚x + âˆ‚z/âˆ‚sin(x) Â· âˆ‚sin(x)/âˆ‚x
      = 1 Â· y + 1 Â· cos(x)
      = y + cos(x)  âœ“
```

---

### 7. Research Paper Connection: Transformers

**Attention mechanism backpropagation:**

From "Attention is All You Need" (Vaswani et al., 2017):

```
Forward:
  Q = XÂ·W_Q    K = XÂ·W_K    V = XÂ·W_V
  scores = QÂ·Káµ€ / âˆšd_k
  weights = softmax(scores)
  output = weights Â· V

Backward (gradients):
  âˆ‚L/âˆ‚V = weightsáµ€ Â· âˆ‚L/âˆ‚output
  âˆ‚L/âˆ‚weights = âˆ‚L/âˆ‚output Â· Váµ€
  âˆ‚L/âˆ‚scores = softmax'(scores) Â· âˆ‚L/âˆ‚weights
  âˆ‚L/âˆ‚Q = (âˆ‚L/âˆ‚scores Â· Káµ€) / âˆšd_k
  âˆ‚L/âˆ‚K = (âˆ‚L/âˆ‚scores)áµ€ Â· Q / âˆšd_k
  âˆ‚L/âˆ‚W_Q = Xáµ€ Â· âˆ‚L/âˆ‚Q
  âˆ‚L/âˆ‚W_K = Xáµ€ Â· âˆ‚L/âˆ‚K
  âˆ‚L/âˆ‚W_V = Xáµ€ Â· âˆ‚L/âˆ‚V
```

**Softmax gradient** (critical for attention):
```
If y = softmax(z), then:
âˆ‚yáµ¢/âˆ‚zâ±¼ = yáµ¢(Î´áµ¢â±¼ - yâ±¼)

In matrix form:
âˆ‚y/âˆ‚z = diag(y) - yÂ·yáµ€

For backprop:
âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚y Â· (diag(y) - yÂ·yáµ€)
```

---

##  8. Numerical Gradient Checking

**Always verify your backprop implementation!**

```python
def numerical_gradient(f, x, eps=1e-5):
    """
    Compute gradient numerically using finite differences
    
    (f(x + Îµ) - f(x - Îµ)) / (2Îµ) â‰ˆ f'(x)
    """
    grad = np.zeros_like(x)
    
    for i in range(x.size):
        x_plus = x.copy()
        x_plus.flat[i] += eps
        
        x_minus = x.copy()
        x_minus.flat[i] -= eps
        
        grad.flat[i] = (f(x_plus) - f(x_minus)) / (2 * eps)
    
    return grad

def check_gradient(analytical_grad, numerical_grad):
    """
    Compare analytical and numerical gradients
    """
    diff = np.linalg.norm(analytical_grad - numerical_grad)
    sum_norm = np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad)
    relative_error = diff / (sum_norm + 1e-8)
    
    print(f"Relative error: {relative_error:.2e}")
    
    if relative_error < 1e-7:
        print("âœ“ Gradient is correct!")
    elif relative_error < 1e-4:
        print("âš  Gradient might be correct (borderline)")
    else:
        print("âœ— Gradient is WRONG!")
    
    return relative_error

# Example
def f(W1):
    y_pred, loss, _ = forward_pass(x, W1, b1, W2, b2, y)
    return loss

analytical = gradients['dW1']
numerical = numerical_gradient(f, W1)
check_gradient(analytical, numerical)
```

---

## ğŸ’» Code

```python
import torch

x = torch.randn(10, requires_grad=True)
y = x ** 2
loss = y.sum()

loss.backward()  # Computes all gradients!
print(x.grad)    # âˆ‚loss/âˆ‚x = 2x
```

---

## ğŸ”— Where This Topic Is Used

| Topic | How Backprop Is Used |
|-------|---------------------|
| **Every Neural Network** | Training = forward + backward pass |
| **PyTorch autograd** | Automatic backprop implementation |
| **Transformer Training** | GPT/BERT learn via backprop |
| **CNN Training** | Image models learn via backprop |
| **Fine-tuning** | Backprop through pretrained model |
| **LoRA** | Backprop through low-rank adapters |
| **Diffusion Training** | Score matching via backprop |
| **RLHF** | Policy gradient + backprop |
| **Neural Architecture Search** | Differentiable NAS uses backprop |
| **Physics-Informed NNs** | Backprop through physics constraints |

### Prerequisite For

```
Backpropagation --> Training any neural network
               --> Understanding gradient flow
               --> Debugging training issues
               --> Custom layer implementation
```

### Concepts Built On Backprop

| Concept | How It Uses Backprop |
|---------|---------------------|
| Gradient Clipping | Modify gradients from backprop |
| Gradient Checkpointing | Trade compute for memory in backprop |
| Mixed Precision | FP16 forward, FP32 backward |
| Second-order methods | Use Hessian (backprop of backprop) |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ¥ | Karpathy: micrograd | [YouTube](https://www.youtube.com/watch?v=VMj-3S1tku0) |
| ğŸ“– | Deep Learning Book Ch. 6 | [Book](https://www.deeplearningbook.org/contents/mlp.html) |
| ğŸ“– | PyTorch Autograd | [Docs](https://pytorch.org/docs/stable/autograd.html) |
| ğŸ‡¨ğŸ‡³ | åå‘ä¼ æ’­è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25081671) |
| ğŸ‡¨ğŸ‡³ | æ‰‹å†™åå‘ä¼ æ’­ | [Bç«™](https://www.bilibili.com/video/BV1Le4y1s7HH) |
| ğŸ‡¨ğŸ‡³ | è®¡ç®—å›¾ä¸è‡ªåŠ¨å¾®åˆ† | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88661776) |

---

â¬…ï¸ [Back: Neural Networks](../01_neural_networks/README.md) | â¡ï¸ [Next: Architectures](../03_architectures/README.md)

---

â¬…ï¸ [Back: Deep Learning](../README.md)

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=45B7D1&height=80&section=footer" width="100%"/>
</p>
