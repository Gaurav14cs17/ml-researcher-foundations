<!-- Navigation -->
<p align="center">
  <a href="../01_learning_frameworks/">â¬…ï¸ Prev: Learning Frameworks</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../">ğŸ“š ML Theory</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../02_generalization/">Next: Generalization â¡ï¸</a>
</p>

---

<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=4ECDC4&height=120&section=header&text=Learning%20Theory&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-05-4ECDC4?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./pac-learning/images/pac-vc-dimension-complete.svg" width="100%">

*Caption: Learning theory provides mathematical guarantees for when and why machine learning works. Key concepts include PAC learning, VC dimension, and sample complexity.*

---

## ğŸ“ Mathematical Foundations

### PAC Learning

```
Probably Approximately Correct (PAC):

A concept class C is PAC-learnable if:
âˆƒ algorithm A, polynomial p(Â·,Â·,Â·,Â·) such that:

For all:
â€¢ c âˆˆ C (target concept)
â€¢ D (distribution)
â€¢ Îµ > 0 (accuracy)
â€¢ Î´ > 0 (confidence)

Using m â‰¥ p(1/Îµ, 1/Î´, n, size(c)) samples:
P[error(h) â‰¤ Îµ] â‰¥ 1 - Î´
```

### VC Dimension

```
VC Dimension = Maximum number of points that can be shattered

Definition of shattering:
A set S is shattered by H if:
âˆ€ labeling of S, âˆƒ h âˆˆ H that achieves it

Examples:
â€¢ Linear classifiers in â„Â²: VC = 3
â€¢ Linear classifiers in â„áµˆ: VC = d + 1
â€¢ Decision stumps: VC = 1
â€¢ k-NN (k=1): VC = âˆ
```

### Generalization Bound

```
With probability â‰¥ 1 - Î´:

R(h) â‰¤ RÌ‚(h) + âˆš((dÂ·log(2n/d) + log(1/Î´)) / n)

Where:
â€¢ R(h) = true risk (test error)
â€¢ RÌ‚(h) = empirical risk (training error)
â€¢ d = VC dimension
â€¢ n = sample size
```

---

## ğŸ“‚ Topics

| Folder | Topic | Key Concepts |
|--------|-------|--------------|
| [pac-learning/](./pac-learning/) | PAC framework | Sample complexity, efficiency |

---

## ğŸ¯ Key Concepts

| Concept | Definition | Significance |
|---------|------------|--------------|
| **PAC Learnable** | Can learn with polynomial samples | Efficient learning is possible |
| **VC Dimension** | Capacity of hypothesis class | Controls generalization |
| **Sample Complexity** | Samples needed for accuracy Îµ | How much data needed? |
| **Rademacher Complexity** | Data-dependent capacity | Tighter bounds |

---

## ğŸ’» Code Examples

```python
import numpy as np
from sklearn.linear_model import Perceptron
from sklearn.svm import SVC

# VC Dimension intuition
# Linear classifier in d dimensions has VC = d + 1

def can_shatter_points(X, y_all_labelings, classifier):
    """Check if classifier can shatter points X"""
    for y in y_all_labelings:
        clf = classifier()
        try:
            clf.fit(X, y)
            if not np.allclose(clf.predict(X), y):
                return False
        except:
            return False
    return True

# Example: 3 points in 2D
X = np.array([[0, 0], [1, 0], [0, 1]])

# All 2^3 = 8 labelings
from itertools import product
all_labelings = list(product([0, 1], repeat=3))

# Linear classifier can shatter 3 points in 2D (VC = 3)
print("Testing linear classifier on 3 points in 2D")
for labeling in all_labelings:
    y = np.array(labeling)
    clf = Perceptron()
    clf.fit(X, y)
    pred = clf.predict(X)
    print(f"Labeling {labeling}: {'âœ“' if np.allclose(pred, y) else 'âœ—'}")

# Sample complexity bound
def sample_complexity(vc_dim, epsilon, delta):
    """PAC sample complexity bound"""
    # Fundamental theorem of learning
    m = (4 / epsilon) * (vc_dim * np.log(12 / epsilon) + np.log(2 / delta))
    return int(np.ceil(m))

vc = 10  # Hypothesis class VC dimension
eps = 0.1  # 10% error tolerance
delta = 0.05  # 95% confidence

m = sample_complexity(vc, eps, delta)
print(f"Need at least {m} samples for VC={vc}, Îµ={eps}, Î´={delta}")
```

---

## ğŸŒ ML Applications

| Application | How Learning Theory Is Used |
|-------------|----------------------------|
| **Model Selection** | VC dimension guides capacity |
| **Sample Size** | Theory tells us how much data |
| **Generalization** | Bounds on test error |
| **Algorithm Design** | Efficient learning guarantees |

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Understanding ML (Shalev-Shwartz) | [Book](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) |
| ğŸ“– | Foundations of ML (Mohri) | [Book](https://cs.nyu.edu/~mohri/mlbook/) |
| ğŸ“„ | VC Dimension Original | [Paper](https://link.springer.com/article/10.1007/BF00116037) |
| ğŸ‡¨ğŸ‡³ | PACå­¦ä¹ ç†è®º | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/35261164) |
| ğŸ‡¨ğŸ‡³ | VCç»´è¯¦è§£ | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88882888) |

---

## ğŸ”— Where This Topic Is Used

| Application | How Learning Theory Is Used |
|-------------|----------------------------|
| **Generalization Analysis** | Bounds on test error |
| **Model Complexity** | VC dimension as capacity measure |
| **Sample Efficiency** | How much data is enough? |
| **Algorithm Correctness** | Provable learning guarantees |
| **Neural Network Theory** | Understanding deep learning |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<!-- Navigation -->
<p align="center">
  <a href="../01_learning_frameworks/">â¬…ï¸ Prev: Learning Frameworks</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../">ğŸ“š ML Theory</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../02_generalization/">Next: Generalization â¡ï¸</a>
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=4ECDC4&height=80&section=footer" width="100%"/>
</p>
