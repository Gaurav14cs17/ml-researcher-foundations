<!-- Navigation -->
<p align="center">
  <a href="../01_gaussian_processes/">â¬…ï¸ Prev: Gaussian Processes</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../">ğŸ“š Kernel Methods</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../03_kernel_trick/">Next: Kernel Trick â¡ï¸</a>
</p>

---

<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=4ECDC4&height=120&section=header&text=Kernel%20Functions&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-05-4ECDC4?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/kernels.svg" width="100%">

*Caption: A kernel k(x,x') computes inner products in feature space without explicitly computing features: k(x,x') = âŸ¨Ï†(x), Ï†(x')âŸ©. Common kernels: Linear, Polynomial, RBF (Gaussian).*

---

## ğŸ“‚ Overview

Kernels enable working in high-dimensional feature spaces efficiently. They're the foundation of SVMs, Gaussian processes, and many classical ML methods.

---

## ğŸ“ Mathematical Definition

### The Kernel Trick

```
Kernel k(x, x') = âŸ¨Ï†(x), Ï†(x')âŸ©

Instead of:
1. Map x â†’ Ï†(x) in high-D space
2. Compute inner product

Just compute:
k(x, x') directly (often cheaper!)

Example: Ï†(x) might be âˆ-dimensional, but k is finite!

```

### Mercer's Condition

```
A function k(x, x') is a valid kernel if and only if
for any set of points {xâ‚, ..., xâ‚™}, the kernel matrix K is
positive semi-definite:

K_{ij} = k(xáµ¢, xâ±¼)

âˆ€v: v^T K v â‰¥ 0

```

---

## ğŸ“Š Common Kernels

| Kernel | Formula | Feature Space |
|--------|---------|---------------|
| **Linear** | k(x,x') = x^T x' | Original space |
| **Polynomial** | k(x,x') = (x^T x' + c)^d | Polynomial features |
| **RBF/Gaussian** | k(x,x') = exp(-Î³\|\|x-x'\|\|Â²) | Infinite-dimensional |
| **Laplacian** | k(x,x') = exp(-Î³\|\|x-x'\|\|â‚) | Infinite-dimensional |
| **Sigmoid** | k(x,x') = tanh(Î±x^T x' + c) | Neural network-like |

### RBF Kernel (Most Popular)

```
k(x, x') = exp(-Î³ ||x - x'||Â²)
         = exp(-||x - x'||Â² / (2ÏƒÂ²))

Where:
    Î³ = 1/(2ÏƒÂ²) controls width
    Large Î³: Points must be very close to be similar
    Small Î³: Points can be far apart and still similar

As Î³ â†’ âˆ: k(x,x') â†’ Î´(x-x')  (identity kernel)
As Î³ â†’ 0: k(x,x') â†’ 1        (all points similar)

```

### Polynomial Kernel

```
k(x, x') = (x^T x' + c)^d

For d=2, c=1:
k([xâ‚,xâ‚‚], [yâ‚,yâ‚‚]) = (xâ‚yâ‚ + xâ‚‚yâ‚‚ + 1)Â²
                    = 1 + 2xâ‚yâ‚ + 2xâ‚‚yâ‚‚ + 2xâ‚xâ‚‚yâ‚yâ‚‚ + xâ‚Â²yâ‚Â² + xâ‚‚Â²yâ‚‚Â²

Equivalent to feature map:
Ï†([xâ‚,xâ‚‚]) = [1, âˆš2xâ‚, âˆš2xâ‚‚, âˆš2xâ‚xâ‚‚, xâ‚Â², xâ‚‚Â²]

```

---

## ğŸ”‘ Key Properties

| Property | Description | Formula |
|----------|-------------|---------|
| **Symmetry** | k(x,x') = k(x',x) | Order doesn't matter |
| **PSD** | Kernel matrix is positive semi-definite | v^TKv â‰¥ 0 |
| **Closure under sum** | kâ‚ + kâ‚‚ is a kernel | Combine kernels |
| **Closure under product** | kâ‚ Ã— kâ‚‚ is a kernel | Combine kernels |
| **Closure under scaling** | ck is a kernel (c > 0) | Scale kernels |

### Kernel Composition

```
Given valid kernels kâ‚, kâ‚‚:

Addition:     k(x,x') = kâ‚(x,x') + kâ‚‚(x,x')  âœ“ valid
Product:      k(x,x') = kâ‚(x,x') Ã— kâ‚‚(x,x')  âœ“ valid
Scaling:      k(x,x') = c Ã— kâ‚(x,x')         âœ“ valid (c > 0)
Exponent:     k(x,x') = exp(kâ‚(x,x'))        âœ“ valid
Polynomial:   k(x,x') = (kâ‚(x,x') + c)^d     âœ“ valid (c â‰¥ 0)

```

---

## ğŸ’» Code Examples

### Implementing Kernels

```python
import numpy as np

def linear_kernel(x1, x2):
    """Linear kernel: k(x,x') = x^T x'"""
    return np.dot(x1, x2)

def polynomial_kernel(x1, x2, degree=3, coef0=1):
    """Polynomial kernel: k(x,x') = (x^T x' + c)^d"""
    return (np.dot(x1, x2) + coef0) ** degree

def rbf_kernel(x1, x2, gamma=1.0):
    """RBF (Gaussian) kernel: k(x,x') = exp(-Î³||x-x'||Â²)"""
    diff = x1 - x2
    return np.exp(-gamma * np.dot(diff, diff))

def kernel_matrix(X, kernel_func, **kwargs):
    """Compute full kernel matrix"""
    n = len(X)
    K = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            K[i, j] = kernel_func(X[i], X[j], **kwargs)
    return K

# Example
X = np.random.randn(5, 2)
K_linear = kernel_matrix(X, linear_kernel)
K_rbf = kernel_matrix(X, rbf_kernel, gamma=0.5)
print("Linear kernel matrix:\n", K_linear)
print("RBF kernel matrix:\n", K_rbf)

```

### Using Scikit-Learn

```python
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF, Matern
from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel

# SVM with different kernels
svm_linear = SVC(kernel='linear')
svm_rbf = SVC(kernel='rbf', gamma=0.5)
svm_poly = SVC(kernel='poly', degree=3)

# Custom kernel
def my_kernel(X, Y):
    return rbf_kernel(X, Y, gamma=0.5) + 0.1 * polynomial_kernel(X, Y, degree=2)

svm_custom = SVC(kernel=my_kernel)

# Gaussian Process with kernel
gp = GaussianProcessClassifier(kernel=RBF(length_scale=1.0))

# Compute kernel matrix directly
X = np.random.randn(100, 10)
K = rbf_kernel(X, gamma=0.5)
print(f"Kernel matrix shape: {K.shape}")

```

### Kernel PCA

```python
from sklearn.decomposition import KernelPCA
import matplotlib.pyplot as plt

# Generate non-linear data
np.random.seed(42)
t = np.linspace(0, 2*np.pi, 200)
X = np.vstack([
    np.cos(t) + 0.1*np.random.randn(200),
    np.sin(t) + 0.1*np.random.randn(200)
]).T

# Linear PCA can't separate this
from sklearn.decomposition import PCA
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X)

# Kernel PCA with RBF kernel
kpca = KernelPCA(n_components=2, kernel='rbf', gamma=10)
X_kpca = kpca.fit_transform(X)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], X[:, 1], c=t)
axes[0].set_title('Original Data')
axes[1].scatter(X_kpca[:, 0], X_kpca[:, 1], c=t)
axes[1].set_title('Kernel PCA (RBF)')
plt.show()

```

---

## ğŸ“Š Choosing a Kernel

| Data Type | Recommended Kernel | Why |
|-----------|-------------------|-----|
| **Linear separable** | Linear | Simple, fast |
| **Non-linear** | RBF | Flexible, universal approximator |
| **Polynomial relationships** | Polynomial | Explicit polynomial features |
| **Sparse similarity** | Laplacian | Robust to outliers |
| **Periodic data** | Periodic | Captures cycles |
| **Unknown** | RBF | Good default |

### Hyperparameter Selection

```
RBF Kernel: Î³ (or Ïƒ) is critical

Too small Î³ (large Ïƒ): 
    - All points look similar
    - Underfitting
    
Too large Î³ (small Ïƒ):
    - Only very close points similar
    - Overfitting
    
Typically: Cross-validation over grid
    Î³ âˆˆ {10â»Â³, 10â»Â², 10â»Â¹, 1, 10, 10Â², 10Â³}

```

---

## ğŸ”— Applications

| Method | How Kernels Are Used |
|--------|---------------------|
| **SVM** | Decision boundary in feature space |
| **Kernel PCA** | Non-linear dimensionality reduction |
| **Gaussian Processes** | Covariance function |
| **Kernel Ridge Regression** | Non-linear regression |
| **MMD** | Distribution comparison |

---

## ğŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Kernel Trick | [../kernel-trick/](../kernel-trick/) |
| ğŸ“– | SVM | [../svm/](../svm/) |
| ğŸ“– | Gaussian Processes | [../gaussian-processes/](../gaussian-processes/) |
| ğŸ“„ | Learning with Kernels | [Book](https://mitpress.mit.edu/9780262536578/learning-with-kernels/) |
| ğŸ‡¨ğŸ‡³ | æ ¸å‡½æ•°è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/24291579) |
| ğŸ‡¨ğŸ‡³ | RBFæ ¸å‚æ•°é€‰æ‹© | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88660612) |
| ğŸ‡¨ğŸ‡³ | æ”¯æŒå‘é‡æœºæ ¸æŠ€å·§ | [Bç«™](https://www.bilibili.com/video/BV1Hb411w7FN) |
| ğŸ‡¨ğŸ‡³ | æ ¸æ–¹æ³•ç»¼è¿° | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2018-01-25-6)

---

â¬…ï¸ [Back: Kernel Methods](../)

---

â¬…ï¸ [Back: Kernel Trick](../kernel-trick/) | â¡ï¸ [Next: Rkhs](../rkhs/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<!-- Navigation -->
<p align="center">
  <a href="../01_gaussian_processes/">â¬…ï¸ Prev: Gaussian Processes</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../">ğŸ“š Kernel Methods</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../03_kernel_trick/">Next: Kernel Trick â¡ï¸</a>
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=4ECDC4&height=80&section=footer" width="100%"/>
</p>
