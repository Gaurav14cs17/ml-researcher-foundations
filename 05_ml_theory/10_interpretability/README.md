<!-- Navigation -->
<p align="center">
  <a href="../09_hyperparameter_tuning/">â¬…ï¸ Prev: Hyperparameter Tuning</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../">ğŸ“š ML Theory</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../11_adversarial_robustness/">Next: Adversarial Robustness â¡ï¸</a>
</p>

---

<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=4ECDC4&height=120&section=header&text=Model%20Interpretability&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-05-4ECDC4?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ¯ Visual Overview

<img src="./images/model-interpretability-complete.svg" width="100%">

*Caption: Interpretability methods help explain model decisions. SHAP provides consistent feature attributions, LIME explains locally, attention weights show what models focus on.*

---

## ğŸ“ Mathematical Foundations

### SHAP (SHapley Additive exPlanations)

```
Based on Shapley values from game theory:

Ï†áµ¢ = Î£_{SâŠ†N\{i}} |S|!(n-|S|-1)!/n! [f(Sâˆª{i}) - f(S)]

Properties:
â€¢ Local accuracy: f(x) = Ï†â‚€ + Î£áµ¢ Ï†áµ¢
â€¢ Consistency: If feature contribution increases, so does SHAP
â€¢ Missingness: Missing features get Ï† = 0

For tree models: TreeSHAP is O(TLÂ²) instead of O(2â¿)

```

### LIME (Local Interpretable Model-agnostic Explanations)

```
Fit interpretable model g locally around x:

Î¾(x) = argmin_{gâˆˆG} L(f, g, Ï€â‚“) + Î©(g)

Where:
â€¢ L = loss between f and g weighted by Ï€â‚“
â€¢ Ï€â‚“ = locality kernel (e.g., exponential)
â€¢ Î©(g) = complexity penalty (encourage sparse g)

g is typically a linear model or decision tree

```

### Integrated Gradients

```
IG(x)áµ¢ = (xáµ¢ - x'áµ¢) Ã— âˆ«â‚€Â¹ (âˆ‚f/âˆ‚xáµ¢)(x' + Î±(x-x')) dÎ±

Where x' is baseline (e.g., zero image)

Properties:
â€¢ Sensitivity: If feature matters, IG â‰  0
â€¢ Completeness: Î£áµ¢ IGáµ¢ = f(x) - f(x')

```

---

## ğŸ¯ Method Comparison

| Method | Scope | Model-agnostic | Computation |
|--------|-------|----------------|-------------|
| **SHAP** | Global/Local | Yes | Expensive |
| **LIME** | Local | Yes | Moderate |
| **Integrated Gradients** | Local | No (differentiable) | Fast |
| **Attention** | Local | No (attention models) | Free |
| **Feature Importance** | Global | No (tree models) | Fast |

---

## ğŸ’» Code Examples

```python
import numpy as np
import shap
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Create dataset and model
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
feature_names = [f'feature_{i}' for i in range(20)]
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X[:100])

# Summary plot (global importance)
shap.summary_plot(shap_values[1], X[:100], feature_names=feature_names)

# Force plot (local explanation)
shap.force_plot(
    explainer.expected_value[1], 
    shap_values[1][0], 
    X[0],
    feature_names=feature_names
)

# Feature importance from trees
importances = model.feature_importances_
top_features = np.argsort(importances)[-5:][::-1]
print(f"Top 5 features: {top_features}")

# LIME example
from lime.lime_tabular import LimeTabularExplainer

lime_explainer = LimeTabularExplainer(
    X,
    feature_names=feature_names,
    class_names=['class_0', 'class_1'],
    mode='classification'
)

# Explain single prediction
explanation = lime_explainer.explain_instance(
    X[0],
    model.predict_proba,
    num_features=10
)
explanation.show_in_notebook()

# Integrated Gradients (for neural networks)
# Using captum library
try:
    import torch
    import torch.nn as nn
    from captum.attr import IntegratedGradients
    
    class SimpleNN(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Sequential(
                nn.Linear(20, 50),
                nn.ReLU(),
                nn.Linear(50, 2)
            )
        
        def forward(self, x):
            return self.fc(x)
    
    nn_model = SimpleNN()
    ig = IntegratedGradients(nn_model)
    
    x_tensor = torch.tensor(X[:1], dtype=torch.float32)
    baseline = torch.zeros_like(x_tensor)
    
    attributions = ig.attribute(x_tensor, baseline, target=1)
    print(f"IG attributions shape: {attributions.shape}")
except ImportError:
    print("Captum not installed")

```

---

## ğŸŒ ML Applications

| Application | Method | Why |
|-------------|--------|-----|
| **Healthcare** | SHAP, LIME | Explain diagnoses |
| **Finance** | SHAP | Regulatory compliance |
| **NLP** | Attention, IG | Token importance |
| **Computer Vision** | GradCAM, IG | Spatial attention |
| **Debugging** | All methods | Find model issues |

---

## ğŸ“Š Interpretability Spectrum

```
Inherently Interpretable    â†â†’    Black Box + Explanation
        
Linear Regression               Deep Learning + SHAP
Decision Trees                  Neural Networks + LIME
Rule Lists                      Transformers + Attention
GAMs                            CNNs + GradCAM

```

---

## ğŸ“š References

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | SHAP Paper | [Paper](https://arxiv.org/abs/1705.07874) |
| ğŸ“„ | LIME Paper | [Paper](https://arxiv.org/abs/1602.04938) |
| ğŸ“„ | Integrated Gradients | [Paper](https://arxiv.org/abs/1703.01365) |
| ğŸ“– | Interpretable ML Book | [Book](https://christophm.github.io/interpretable-ml-book/) |
| ğŸ‡¨ğŸ‡³ | å¯è§£é‡Šæ€§æœºå™¨å­¦ä¹  | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/97629463) |

---

## ğŸ”— Where This Topic Is Used

| Application | How Interpretability Is Used |
|-------------|------------------------------|
| **Model Debugging** | Understand model errors |
| **Feature Engineering** | Identify important features |
| **Regulatory Compliance** | Explain decisions |
| **User Trust** | Build confidence in predictions |
| **Scientific Discovery** | Understand learned patterns |

---

â¬…ï¸ [Back: 09-Hyperparameter Tuning](../09_hyperparameter_tuning/) | â¡ï¸ [Next: 11-Adversarial Robustness](../11_adversarial_robustness/)

---

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<!-- Navigation -->
<p align="center">
  <a href="../09_hyperparameter_tuning/">â¬…ï¸ Prev: Hyperparameter Tuning</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../">ğŸ“š ML Theory</a> &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="../11_adversarial_robustness/">Next: Adversarial Robustness â¡ï¸</a>
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=4ECDC4&height=80&section=footer" width="100%"/>
</p>
