<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Learning%20Rate%20Schedules&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## üéØ Why Learning Rate Schedules Matter

```
+---------------------------------------------------------+
|                                                         |
|   Constant LR:                With Schedule:            |
|                                                         |
|     Loss                        Loss                    |
|       |                           |                     |
|       |‚ï≤                          |‚ï≤                    |
|       | ‚ï≤                         | ‚ï≤                   |
|       |  ‚ï≤------- oscillates      |  ‚ï≤                  |
|       |                           |   ‚ï≤_____ converges  |
|       +--------------            +--------------        |
|                                                         |
|   Too high ‚Üí oscillate          Start high ‚Üí explore   |
|   Too low ‚Üí slow                End low ‚Üí converge     |
|                                                         |
+---------------------------------------------------------+

```

---

## üìê Mathematical Foundation

### Robbins-Monro Conditions

```
For stochastic approximation to converge:

1. Œ£‚Çú Œ±‚Çú = ‚àû        (step sizes sum to infinity)
2. Œ£‚Çú Œ±‚Çú¬≤ < ‚àû       (step sizes are square-summable)

Interpretation:
‚Ä¢ Condition 1: Take enough total steps to reach optimum
‚Ä¢ Condition 2: Reduce noise effect over time

Examples that satisfy both:
‚Ä¢ Œ±‚Çú = 1/t        ‚úì
‚Ä¢ Œ±‚Çú = 1/‚àöt       ‚úì
‚Ä¢ Œ±‚Çú = constant   ‚úó (violates condition 2)

```

### Convergence Rate Analysis

```
For convex f with variance œÉ¬≤ in gradients:

Constant LR Œ±:
  E[f(x‚Çú)] - f* ‚â§ O(1/t) + O(Œ± œÉ¬≤)
                   ‚Üì         ‚Üì
              optimization  variance floor

Decaying LR Œ±‚Çú = Œ±‚ÇÄ/‚àöt:
  E[f(x‚Çú)] - f* ‚â§ O(1/‚àöt)
  
  No variance floor - converges to optimum!

```

---

## üìê Common Schedules

### 1. Step Decay

```
Œ∑(t) = Œ∑‚ÇÄ √ó Œ≥^‚åät/s‚åã

Parameters:
‚Ä¢ Œ∑‚ÇÄ = initial learning rate
‚Ä¢ Œ≥ = decay factor (typically 0.1)
‚Ä¢ s = step size (epochs between decays)

Example (ResNet on ImageNet):
  Œ∑‚ÇÄ = 0.1
  Œ≥ = 0.1 (divide by 10)
  s = 30 epochs
  
  Epochs 0-30:  Œ∑ = 0.1
  Epochs 30-60: Œ∑ = 0.01
  Epochs 60-90: Œ∑ = 0.001

```

### 2. Exponential Decay

```
Œ∑(t) = Œ∑‚ÇÄ √ó e^(-Œªt)

or equivalently:
Œ∑(t) = Œ∑‚ÇÄ √ó Œ≥^t  where Œ≥ = e^(-Œª)

Properties:
‚Ä¢ Smooth decay
‚Ä¢ Œ≥ close to 1 ‚Üí slow decay
‚Ä¢ Œ≥ close to 0 ‚Üí fast decay

Common: Œ≥ = 0.95 to 0.99 per epoch

```

### 3. Polynomial Decay

```
Œ∑(t) = Œ∑‚ÇÄ √ó (1 - t/T)^p

Parameters:
‚Ä¢ T = total training steps
‚Ä¢ p = power (p=1 for linear, p=2 for quadratic)

Special case (linear decay):
Œ∑(t) = Œ∑_max √ó (1 - t/T)

Starts at Œ∑_max, linearly decreases to 0

```

### 4. Cosine Annealing

```
Œ∑(t) = Œ∑_min + (Œ∑_max - Œ∑_min) √ó (1 + cos(œÄt/T))/2

Properties:
‚Ä¢ Smooth S-curve decay
‚Ä¢ Starts at Œ∑_max
‚Ä¢ Ends at Œ∑_min
‚Ä¢ Gentle at start and end, faster in middle

Variant with warm restarts (SGDR):
Œ∑(t) = Œ∑_min + (Œ∑_max - Œ∑_min) √ó (1 + cos(œÄ √ó t_i/T_i))/2

where t_i is time since last restart

```

### 5. Warmup + Decay (Transformers)

```
Original "Attention is All You Need" schedule:

Œ∑(t) = d^(-0.5) √ó min(t^(-0.5), t √ó warmup^(-1.5))

where:
‚Ä¢ d = model dimension
‚Ä¢ warmup = warmup steps (typically 4000)

Behavior:
‚Ä¢ t < warmup: Linear increase (Œ∑ ‚àù t)
‚Ä¢ t > warmup: Inverse square root decay (Œ∑ ‚àù 1/‚àöt)

```

### 6. Linear Warmup + Linear Decay (BERT/GPT)

```
        Œ∑_max --------.
                      |
                      ‚ï≤
                       ‚ï≤
                        ‚ï≤
                         ‚ï≤
    Œ∑ = 0 ---------------‚ï≤---
           warmup    total_steps

For t ‚â§ warmup:
  Œ∑(t) = Œ∑_max √ó t / warmup

For t > warmup:
  Œ∑(t) = Œ∑_max √ó (total - t) / (total - warmup)

```

### 7. One Cycle (fast.ai)

```
Phase 1 (Warmup):
  Œ∑: Œ∑_min ‚Üí Œ∑_max
  momentum: Œ≤_max ‚Üí Œ≤_min

Phase 2 (Annealing):
  Œ∑: Œ∑_max ‚Üí Œ∑_min (cosine)
  momentum: Œ≤_min ‚Üí Œ≤_max

Benefits:
‚Ä¢ Explore early (high LR)
‚Ä¢ Converge late (low LR)
‚Ä¢ Counter-cyclic momentum helps

```

---

## üìê Mathematical Analysis

### Why Warmup Helps

```
Problem at initialization:
‚Ä¢ Gradients can be very large (random weights)
‚Ä¢ Adam's m, v estimates are zero initially
‚Ä¢ Large initial updates can destabilize training

Warmup solution:
‚Ä¢ Start with tiny LR ‚Üí small updates
‚Ä¢ Gradually increase ‚Üí let optimizer adapt
‚Ä¢ Adam's m, v accumulate meaningful statistics

Theoretical justification:
At step t, Adam uses:
  mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ^t)  ‚Üê bias correction

Early steps (small t):
  (1 - Œ≤‚ÇÅ^t) is small ‚Üí mÃÇ_t is amplified
  Combined with large gradients ‚Üí instability

Warmup counteracts this amplification

```

### Why Decay Helps

```
SGD update with noise:
  x_{t+1} = x_t - Œ±_t(‚àáf(x_t) + Œµ_t)

where Œµ_t is gradient noise with variance œÉ¬≤

Stationary distribution analysis:
  Near optimum x*, the iterates fluctuate around x*
  with variance proportional to Œ± √ó œÉ¬≤

Constant LR:
  Variance stays constant ‚Üí never converges exactly

Decaying LR (Œ±_t ‚Üí 0):
  Variance ‚Üí 0 ‚Üí converges to x*

```

---

## üíª Code Examples

### PyTorch Schedulers

```python
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import (
    StepLR, 
    ExponentialLR,
    CosineAnnealingLR,
    OneCycleLR,
    LambdaLR
)

optimizer = optim.Adam(model.parameters(), lr=0.001)

# 1. Step decay
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# 2. Exponential decay
scheduler = ExponentialLR(optimizer, gamma=0.95)

# 3. Cosine annealing
scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)

# 4. One cycle
scheduler = OneCycleLR(
    optimizer, 
    max_lr=0.01, 
    epochs=100, 
    steps_per_epoch=len(dataloader)
)

# Training loop
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        optimizer.step()
        
        # For OneCycleLR: step after each batch
        if isinstance(scheduler, OneCycleLR):
            scheduler.step()
    
    # For other schedulers: step after each epoch
    if not isinstance(scheduler, OneCycleLR):
        scheduler.step()

```

### Custom Warmup + Cosine Schedule

```python
import math

def get_lr(step, total_steps, warmup_steps, max_lr, min_lr=0):
    """
    Linear warmup + cosine decay schedule
    """
    if step < warmup_steps:
        # Linear warmup
        return max_lr * step / warmup_steps
    else:
        # Cosine decay
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return min_lr + (max_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))

# Create scheduler
scheduler = LambdaLR(
    optimizer,
    lr_lambda=lambda step: get_lr(step, total_steps, warmup_steps, max_lr) / max_lr
)

```

### Transformer Schedule (Original)

```python
def transformer_lr(step, d_model=512, warmup_steps=4000):
    """
    From "Attention is All You Need" (Vaswani et al., 2017)
    """
    step = max(step, 1)  # Avoid division by zero
    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))

# Usage with LambdaLR
scheduler = LambdaLR(
    optimizer,
    lr_lambda=lambda step: transformer_lr(step, d_model=512, warmup_steps=4000)
)

```

---

## üìä Best Practices by Task

### Computer Vision (ResNet, ViT)

```
Classic (ResNet):
  ‚Ä¢ Initial LR: 0.1
  ‚Ä¢ Schedule: Step decay
  ‚Ä¢ Decay: √∑10 at epochs 30, 60, 90
  ‚Ä¢ Total: 90-120 epochs

Modern (ViT):
  ‚Ä¢ Initial LR: 3e-4
  ‚Ä¢ Schedule: Warmup + cosine
  ‚Ä¢ Warmup: 5-10 epochs
  ‚Ä¢ Total: 300-1000 epochs

```

### NLP (BERT, GPT)

```
BERT:
  ‚Ä¢ Peak LR: 1e-4 to 5e-4
  ‚Ä¢ Schedule: Linear warmup + linear decay
  ‚Ä¢ Warmup: ~10% of training

GPT:
  ‚Ä¢ Peak LR: 2.5e-4 to 6e-4
  ‚Ä¢ Schedule: Warmup + cosine
  ‚Ä¢ Warmup: 375M tokens (GPT-2)
  ‚Ä¢ Min LR: 10% of peak

```

### Fine-tuning

```
Transfer learning:
  ‚Ä¢ LR: 10-100√ó smaller than pre-training
  ‚Ä¢ Schedule: Linear decay (no warmup or short warmup)
  ‚Ä¢ Freeze early layers initially
  
BERT fine-tuning:
  ‚Ä¢ LR: 2e-5 to 5e-5
  ‚Ä¢ Epochs: 2-4
  ‚Ä¢ Linear decay

```

### Reinforcement Learning

```
PPO/A2C:
  ‚Ä¢ Linear decay often works well
  ‚Ä¢ Anneal from 3e-4 to 0

SAC/TD3:
  ‚Ä¢ Constant LR often sufficient
  ‚Ä¢ LR: 3e-4

```

---

## üìê Advanced Topics

### Learning Rate Range Test

```python
def lr_range_test(model, train_loader, start_lr=1e-7, end_lr=10, num_iters=100):
    """
    Find good learning rate by gradually increasing LR
    and plotting loss vs LR
    """
    optimizer = optim.SGD(model.parameters(), lr=start_lr)
    
    lr_mult = (end_lr / start_lr) ** (1 / num_iters)
    lrs, losses = [], []
    
    for i, (x, y) in enumerate(train_loader):
        if i >= num_iters:
            break
            
        optimizer.zero_grad()
        loss = model(x, y)
        loss.backward()
        optimizer.step()
        
        lrs.append(optimizer.param_groups[0]['lr'])
        losses.append(loss.item())
        
        # Increase LR
        for param_group in optimizer.param_groups:
            param_group['lr'] *= lr_mult
    
    # Plot: Good LR is where loss decreases fastest
    # (steepest negative slope)
    return lrs, losses

```

### Layer-wise Learning Rates

```python
# Different LR for different parts of model
optimizer = optim.Adam([
    {'params': model.backbone.parameters(), 'lr': 1e-5},  # Pre-trained
    {'params': model.head.parameters(), 'lr': 1e-3}       # New layers
])

```

### Cyclic Learning Rates

```python
from torch.optim.lr_scheduler import CyclicLR

scheduler = CyclicLR(
    optimizer,
    base_lr=1e-5,
    max_lr=1e-3,
    step_size_up=2000,  # Steps to increase
    mode='triangular2'   # Halve amplitude each cycle
)

```

---

## üìä Summary Comparison

| Schedule | Best For | Pros | Cons |
|----------|----------|------|------|
| **Constant** | Debugging | Simple | Suboptimal |
| **Step decay** | CNNs | Predictable | Abrupt changes |
| **Exponential** | General | Smooth | Hard to tune Œª |
| **Cosine** | Most tasks | Smooth, proven | Fixed duration |
| **Warmup+decay** | Transformers | Stable training | Extra params |
| **One cycle** | Fast training | Fast convergence | Single epoch needed |

---

## üìö Resources

| Type | Title | Link |
|------|-------|------|
| üìÑ | Cyclical LR | [arXiv](https://arxiv.org/abs/1506.01186) |
| üìÑ | SGDR (Warm Restarts) | [arXiv](https://arxiv.org/abs/1608.03983) |
| üìÑ | 1cycle Policy | [arXiv](https://arxiv.org/abs/1803.09820) |
| üìñ | fast.ai Course | [fast.ai](https://www.fast.ai/) |
| üé• | Learning Rate Finder | [YouTube](https://www.youtube.com/watch?v=WW8TrbM3vLY) |
| üá®üá≥ | Â≠¶‰π†ÁéáË∞ÉÂ∫¶Á≠ñÁï• | [Áü•‰πé](https://zhuanlan.zhihu.com/p/32923584) |

---

‚¨ÖÔ∏è [Back: Constrained Optimization](../05_constrained_optimization/) | ‚û°Ô∏è [Next: Linear Programming](../06_linear_programming/)

> **Note:** This folder covers learning rate schedules used in optimization algorithms.

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
