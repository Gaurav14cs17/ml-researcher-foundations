<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Constrained%20Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“‚ Subtopics

| Folder | Topic | Key Concept |
|--------|-------|-------------|
| [01_kkt/](./01_kkt/) | KKT Conditions | Inequality constraints |
| [02_lagrange/](./02_lagrange/) | Lagrange Multipliers | Equality constraints |

---

## ğŸ¯ The Problem

```
Unconstrained:              Constrained:
                            
   Find x that               Find x that
   minimizes f(x)            minimizes f(x)
                             SUBJECT TO:
   â€¢ Go anywhere             â€¢ g(x) â‰¤ 0 (inequality)
                             â€¢ h(x) = 0 (equality)
                             
   â†“                         â†“
   
   Just set âˆ‡f = 0           Need Lagrange/KKT!

```

---

## ğŸ“ The Two Main Tools

```
+---------------------------------------------------------+

|                                                         |
|   LAGRANGE MULTIPLIERS              KKT CONDITIONS      |
|   --------------------              --------------      |
|                                                         |
|   For: h(x) = 0 only                For: g(x) â‰¤ 0 AND  |
|   (equality)                         h(x) = 0           |
|                                                         |
|   L = f(x) - Î»h(x)                  + Complementarity:  |
|                                     Î¼áµ¢gáµ¢(x) = 0         |
|   Solve: âˆ‡L = 0                     Î¼áµ¢ â‰¥ 0              |
|                                                         |
+---------------------------------------------------------+

```

---

## ğŸ“Š Visual Comparison

```
Unconstrained:            Constrained:

    âˆ‡f = 0                  âˆ‡f = Î»âˆ‡g
       |                        |
       â†“                        â†“
       â€¢                        â€¢ â† on boundary!
      â•± â•²                      -+-
     â•±   â•²                   â•±  |  â•²  feasible
    â•±     â•²                 â•±   |   â•² region
   
   Interior optimum        Boundary optimum

```

---

# Part 1: Lagrange Multipliers

## ğŸ“ Mathematical Formulation

### Problem Setup

```
minimize    f(x)
subject to  gáµ¢(x) = 0,  i = 1, ..., m

where x âˆˆ â„â¿, m < n (fewer constraints than variables)

```

### The Lagrangian Function

```
L(x, Î») = f(x) - Î£áµ¢ Î»áµ¢ gáµ¢(x)

where Î» = (Î»â‚, ..., Î»â‚˜) are Lagrange multipliers

Alternative form (with +):
L(x, Î») = f(x) + Î£áµ¢ Î»áµ¢ gáµ¢(x)
(sign convention varies by textbook)

```

### First-Order Necessary Conditions

```
At optimum (x*, Î»*):

Stationarity (gradient condition):
âˆ‡â‚“L = âˆ‡f(x*) - Î£áµ¢ Î»áµ¢*âˆ‡gáµ¢(x*) = 0

Feasibility:
gáµ¢(x*) = 0  for all i

Geometric interpretation:
âˆ‡f(x*) = Î£áµ¢ Î»áµ¢*âˆ‡gáµ¢(x*)

The objective gradient is a linear combination of constraint gradients!

```

---

## ğŸ“ Why Lagrange Multipliers Work: Complete Proof

**Theorem (First-Order Necessary Conditions):** Let x* be a local minimum of f(x) subject to g(x) = 0, where f and g are continuously differentiable, and âˆ‡g(x*) â‰  0. Then there exists Î»* such that:

```
âˆ‡f(x*) + Î»*âˆ‡g(x*) = 0

```

**Proof:**

```
Step 1: Define feasible directions
A direction d is feasible if there exists Î± > 0 such that:
x* + Î±d satisfies g(x* + Î±d) â‰ˆ 0 for small Î±

Taylor expansion:
g(x* + Î±d) â‰ˆ g(x*) + Î±âˆ‡g(x*)áµ€d
           = Î±âˆ‡g(x*)áµ€d           (since g(x*) = 0)

For feasibility: âˆ‡g(x*)áµ€d = 0

So: Feasible directions lie in tangent space T = {d : âˆ‡g(x*)áµ€d = 0}

Step 2: Optimality implies no descent direction
Since x* is a local minimum, f cannot decrease along any feasible direction:
âˆ‡f(x*)áµ€d â‰¥ 0  for all d âˆˆ T

Step 3: Characterize TâŠ¥ (orthogonal complement)
T = {d : âˆ‡g(x*)áµ€d = 0}
TâŠ¥ = span(âˆ‡g(x*))

Step 4: âˆ‡f must be in TâŠ¥
If âˆ‡f(x*) âˆ‰ TâŠ¥, then âˆ‡f has a component in T.
Let d = -projection of âˆ‡f onto T
Then âˆ‡f(x*)áµ€d < 0 (descent direction in T)
Contradiction! (x* wouldn't be optimal)

Therefore: âˆ‡f(x*) âˆˆ TâŠ¥ = span(âˆ‡g(x*))

Step 5: Conclusion
âˆ‡f(x*) = -Î»*âˆ‡g(x*) for some Î»* âˆˆ â„
âŸ¹ âˆ‡f(x*) + Î»*âˆ‡g(x*) = 0  âˆ

```

---

## ğŸ¯ Geometric Intuition

```
At optimum on constraint surface:

       âˆ‡f           âˆ‡g
        â†‘          â†—
         \        /
          \      /
           \    /
            \  /
        -----*----- constraint g(x) = 0

âˆ‡f must be perpendicular to constraint surface
(otherwise we could move along surface and improve)

This means âˆ‡f is parallel to âˆ‡g: âˆ‡f = Î»âˆ‡g

```

---

## ğŸ“ Second-Order Conditions

```
Bordered Hessian:
H_L = âˆ‡Â²â‚“â‚“L = âˆ‡Â²f - Î£áµ¢ Î»áµ¢âˆ‡Â²gáµ¢

For minimum: H_L positive definite on tangent space of constraints

Tangent space: {v : âˆ‡gáµ¢(x*)áµ€v = 0 for all i}

Check: váµ€ H_L v > 0 for all v in tangent space

```

---

## ğŸ’» Code Example

```python
import numpy as np
from scipy.optimize import minimize

def lagrange_example():
    """
    Minimize f(x,y) = xÂ² + yÂ²
    Subject to g(x,y) = x + y - 1 = 0
    
    Solution: x* = y* = 0.5, Î»* = 1
    """
    
    # Objective
    def f(xy):
        x, y = xy
        return x**2 + y**2
    
    # Constraint
    def g(xy):
        x, y = xy
        return x + y - 1
    
    # Solve using scipy (SLSQP handles equality constraints)
    result = minimize(
        f, 
        x0=[0, 0],
        constraints={'type': 'eq', 'fun': g}
    )
    
    print(f"Optimal x: {result.x}")
    print(f"Optimal f(x): {result.fun}")
    
    # Verify Lagrange conditions analytically
    x, y = result.x
    
    # âˆ‡f = (2x, 2y) = (1, 1)
    # âˆ‡g = (1, 1)
    # Î» = 2x = 2(0.5) = 1
    
    grad_f = np.array([2*x, 2*y])
    grad_g = np.array([1, 1])
    lambda_approx = grad_f[0] / grad_g[0]
    
    print(f"Î»* â‰ˆ {lambda_approx}")
    print(f"âˆ‡f = Î»âˆ‡g? {np.allclose(grad_f, lambda_approx * grad_g)}")
    
    return result

result = lagrange_example()

```

---

## ğŸ’¡ Shadow Prices Interpretation

```
The Lagrange multiplier Î» has economic meaning:

Î»* = âˆ‚f*/âˆ‚b

"How much would the optimal value improve
 if we relaxed the constraint by 1 unit?"

Example:
â€¢ Constraint: budget â‰¤ $100
â€¢ Î»* = 5
â€¢ Meaning: $1 more budget â†’ $5 more profit
â€¢ This is the "shadow price" of money!

```

---

# Part 2: KKT Conditions

## ğŸ¯ What are KKT Conditions?

KKT (Karush-Kuhn-Tucker) conditions are **necessary conditions** for optimality in constrained optimization.

```
+---------------------------------------------------------+

|                                                         |
|   The Problem:                                          |
|                                                         |
|   minimize   f(x)                                       |
|   subject to gáµ¢(x) â‰¤ 0,  i = 1,...,m  (inequality)     |
|              hâ±¼(x) = 0,  j = 1,...,p  (equality)       |
|                                                         |
|   The Lagrangian:                                       |
|                                                         |
|   L(x,Î¼,Î») = f(x) + Î£áµ¢ Î¼áµ¢gáµ¢(x) + Î£â±¼ Î»â±¼hâ±¼(x)           |
|                                                         |
+---------------------------------------------------------+

```

---

## ğŸ“ The 5 KKT Conditions

```
At the optimal point (x*, Î¼*, Î»*):

+---------------------------------------------------------+

|                                                         |
|   1. STATIONARITY                                       |
|      âˆ‡f(x*) + Î£áµ¢ Î¼áµ¢*âˆ‡gáµ¢(x*) + Î£â±¼ Î»â±¼*âˆ‡hâ±¼(x*) = 0       |
|      (Gradients balance at optimum)                     |
|                                                         |
|   2. PRIMAL FEASIBILITY                                 |
|      gáµ¢(x*) â‰¤ 0  for all i                             |
|      hâ±¼(x*) = 0  for all j                             |
|      (Solution satisfies constraints)                   |
|                                                         |
|   3. DUAL FEASIBILITY                                   |
|      Î¼áµ¢* â‰¥ 0  for all i                                |
|      (Multipliers non-negative for inequalities)        |
|                                                         |
|   4. COMPLEMENTARY SLACKNESS                            |
|      Î¼áµ¢* Â· gáµ¢(x*) = 0  for all i                       |
|      (Either constraint active OR multiplier = 0)       |
|                                                         |
|   5. (For convex) SUFFICIENCY                           |
|      If f, gáµ¢ convex and hâ±¼ affine â†’ KKT sufficient    |
|                                                         |
+---------------------------------------------------------+

```

---

## ğŸ¯ Visual Intuition: Complementary Slackness

```
Case 1: Constraint ACTIVE (gáµ¢ = 0)
+---------------------------------+

|                                 |
|   Optimal point ON boundary     |
|                                 |
|         â—--------------         |
|        â•±                        |
|       â•±  feasible               |
|      â•±   region                 |
|                                 |
|   Î¼áµ¢ > 0 (constraint matters)  |
|                                 |
+---------------------------------+

Case 2: Constraint INACTIVE (gáµ¢ < 0)
+---------------------------------+

|                                 |
|   Optimal point INSIDE          |
|                                 |
|         --------------          |
|        â•±    â—                   |
|       â•±  feasible               |
|      â•±   region                 |
|                                 |
|   Î¼áµ¢ = 0 (constraint irrelevant)|
|                                 |
+---------------------------------+

```

---

## ğŸ“ Example: Quadratic with Inequality

### Problem

```
minimize   f(x,y) = xÂ² + yÂ²
subject to g(x,y) = x + y - 1 â‰¤ 0

```

### Step 1: Lagrangian

```
L(x,y,Î¼) = xÂ² + yÂ² + Î¼(x + y - 1)

```

### Step 2: KKT Conditions

```
âˆ‚L/âˆ‚x = 2x + Î¼ = 0  â†’  x = -Î¼/2
âˆ‚L/âˆ‚y = 2y + Î¼ = 0  â†’  y = -Î¼/2

Complementarity: Î¼(x + y - 1) = 0

```

### Step 3: Solve Cases

**Case A: Î¼ = 0** (constraint inactive)

```
x = 0, y = 0
Check: g(0,0) = -1 â‰¤ 0 âœ“
Solution: (0, 0), f* = 0

```

**Case B: g = 0** (constraint active)

```
x + y = 1
x = y = -Î¼/2
â†’ -Î¼ = 1 â†’ Î¼ = -1 < 0 âœ— (violates dual feasibility)

```

**Answer: (0, 0) with f* = 0**

---

## ğŸ’» Code: Checking KKT

```python
import numpy as np
from scipy.optimize import minimize

def f(x):
    return x[0]**2 + x[1]**2

def g(x):
    return x[0] + x[1] - 1  # â‰¤ 0

# Solve with SLSQP (uses KKT internally)
result = minimize(
    f,
    x0=[0.5, 0.5],
    constraints={'type': 'ineq', 'fun': lambda x: -g(x)}
)

print(f"Optimal x: {result.x}")
print(f"Optimal f(x): {result.fun}")
print(f"Constraint g(x): {g(result.x)}")

# Check KKT manually
grad_f = 2 * result.x
print(f"âˆ‡f at optimum: {grad_f}")

```

---

## ğŸ“ KKT Proof (for Convex Problems)

**Theorem:** For a convex optimization problem with Slater condition, KKT conditions are both necessary and sufficient for optimality.

**Proof Sketch:**

```
Step 1: Define Lagrangian
  L(x, Î¼, Î») = f(x) + Î¼áµ€g(x) + Î»áµ€h(x)

Step 2: Strong duality (with Slater)
  min_x max_{Î¼â‰¥0,Î»} L = max_{Î¼â‰¥0,Î»} min_x L
  
  Optimal primal value = Optimal dual value (no gap)

Step 3: At optimum (x*, Î¼*, Î»*)
  x* minimizes L(x, Î¼*, Î»*)
  âŸ¹ âˆ‡_x L(x*, Î¼*, Î»*) = 0  (stationarity)

Step 4: Complementary slackness
  Strong duality âŸ¹ f(x*) = L(x*, Î¼*, Î»*) + Î¼*áµ€g(x*) + Î»*áµ€h(x*)
  
  Since h(x*) = 0 and the bound is tight:
  Î¼*áµ€g(x*) = 0
  
  With Î¼* â‰¥ 0 and g(x*) â‰¤ 0:
  Î¼áµ¢*gáµ¢(x*) = 0 for each i âˆ

```

---

## ğŸ“Š KKT vs Lagrange Multipliers

| Aspect | Lagrange | KKT |
|--------|----------|-----|
| **Constraints** | Equality only | Equality + Inequality |
| **Multipliers** | Î» âˆˆ â„ | Î¼ â‰¥ 0 for inequalities |
| **Extra condition** | None | Complementary slackness |
| **Applications** | Physics | ML, optimization |

---

## ğŸŒ Real-World Applications

| Application | Constraint Type | Example |
|-------------|-----------------|---------|
| **SVM** | Inequality | Margin â‰¥ 1 for all points |
| **Portfolio** | Equality + Inequality | Weights sum to 1, non-negative |
| **Physics** | Equality | Conserve energy/momentum |
| **RL (TRPO, PPO)** | KL constraint | Trust region |
| **Optimal Control** | Dynamics as equality | Trajectory optimization |

---

## ğŸ”— Dependencies

```
foundations/calculus
         |
         â†“
basic-methods/gradient-descent
         |
         â†“
+--------+--------+

| CONSTRAINED OPT |
+-----------------+
| â€¢ lagrange/     |--> Used in SVM!

| â€¢ kkt/          |--> Used in RLHF!
+--------+--------+
         |
         â†“
    Interior Point Methods
    (linear-programming/)

```

---

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Boyd CVX Ch.5 | [Free PDF](https://web.stanford.edu/~boyd/cvxbook/) |
| ğŸ“– | Nocedal Ch.12 | [Springer](https://link.springer.com/book/10.1007/978-0-387-40065-5) |
| ğŸ¥ | KKT Conditions | [YouTube](https://www.youtube.com/watch?v=uh1Dk68cfWs) |
| ğŸ“„ | Original KKT Paper | [1951](https://doi.org/10.1525/9780520347663-014) |
| ğŸ‡¨ğŸ‡³ | çŸ¥ä¹ KKTæ¡ä»¶ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/38163970) |
| ğŸ‡¨ğŸ‡³ | Bç«™ æ‹‰æ ¼æœ—æ—¥ | [Bç«™](https://www.bilibili.com/video/BV1HP4y1Y79e) |

---

## ğŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept for ML systems |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

---

â¬…ï¸ [Back: Convex Optimization](../04_convex_optimization/) | â¡ï¸ [Next: Linear Programming](../06_linear_programming/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
