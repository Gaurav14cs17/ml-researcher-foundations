<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Adam%20Optimizer&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“‚ Topics in This Folder

| File | Topic | Used In |
|------|-------|---------|

---

## ğŸ¯ What is Adam?

Adam = **Ada**ptive **M**oment Estimation

It combines:
1. **Momentum** (first moment) - Smooth out gradients
2. **RMSprop** (second moment) - Adaptive learning rate

```
+---------------------------------------------------------+

|                                                         |
|   SGD          + Momentum      + Adaptive LR   = Adam   |
|   ---             ---              ---            ---   |
|   Basic       Accelerate       Per-parameter    Best    |
|   update      through          learning rate    of all  |
|               ravines                                   |
|                                                         |
+---------------------------------------------------------+

```

---

## ğŸ“ The Algorithm

```
Initialize: mâ‚€ = 0, vâ‚€ = 0, t = 0

For each step:
+---------------------------------------------------------+

|                                                         |
|   1. t = t + 1                                          |
|                                                         |
|   2. g_t = âˆ‡L(Î¸_t)              # Get gradient         |
|                                                         |
|   3. m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t    # Momentum       |
|                                                         |
|   4. v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²   # Adaptive       |
|                                                         |
|   5. mÌ‚_t = m_t / (1 - Î²â‚áµ—)           # Bias correct   |
|      vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)                             |
|                                                         |
|   6. Î¸_t = Î¸_{t-1} - Î±Â·mÌ‚_t/(âˆšvÌ‚_t + Îµ)  # Update       |
|                                                         |
+---------------------------------------------------------+

```

---

## ğŸ”¢ Default Hyperparameters

| Parameter | Default | Meaning |
|-----------|---------|---------|
| **Î±** (lr) | 0.001 | Learning rate |
| **Î²â‚** | 0.9 | First moment decay (momentum) |
| **Î²â‚‚** | 0.999 | Second moment decay (adaptive) |
| **Îµ** | 10â»â¸ | Numerical stability |

---

## ğŸŒ Where Adam Is Used

| Model | Year | Notes |
|-------|------|-------|
| **GPT-2/3/4** | 2019+ | With warmup + decay |
| **BERT** | 2018 | AdamW variant |
| **Stable Diffusion** | 2022 | Default optimizer |
| **Transformers** | 2017+ | Almost universal |
| **Most deep learning** | - | Default choice |

---

## ğŸ’» Code Examples

### PyTorch

```python
import torch.optim as optim

# Standard Adam
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    eps=1e-8
)

# AdamW (for transformers)
optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=0.01
)

# Training loop
for epoch in range(epochs):
    for batch in dataloader:
        loss = model(batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

```

### With Learning Rate Schedule (GPT-style)

```python
from transformers import get_linear_schedule_with_warmup

optimizer = optim.AdamW(model.parameters(), lr=5e-5)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=1000,
    num_training_steps=100000
)

# In training loop:
optimizer.step()
scheduler.step()

```

---

## ğŸ“Š Adam vs Others

<img src="../sgd/images/momentum.svg" width="100%">

| Optimizer | Pros | Cons |
|-----------|------|------|
| **SGD** | Simple, good generalization | Slow, needs tuning |
| **Momentum** | Faster than SGD | Still needs LR tuning |
| **Adam** | Fast, little tuning | May generalize worse |
| **AdamW** | Best for transformers | Slightly more complex |

---

## âš ï¸ When NOT to Use Adam

| Situation | Better Choice | Why |
|-----------|---------------|-----|
| CNNs (ResNet) | SGD + Momentum | Better generalization |
| Very large batch | LAMB, LARS | Scales better |
| Simple convex | L-BFGS | Faster convergence |
| RL (some cases) | RMSprop | Historical reasons |

---

## ğŸ“ DETAILED MATHEMATICAL THEORY

### 1. Adam Algorithm: Complete Derivation

**Full Algorithm:**

```
Initialize:
  mâ‚€ = 0  (first moment vector)
  vâ‚€ = 0  (second moment vector)
  t = 0   (timestep)

Hyperparameters:
  Î± = 0.001    (learning rate)
  Î²â‚ = 0.9     (exponential decay for 1st moment)
  Î²â‚‚ = 0.999   (exponential decay for 2nd moment)
  Îµ = 10â»â¸     (numerical stability)

For each iteration:
+---------------------------------------------------------+

|                                                         |
|  1. t â† t + 1                                           |
|                                                         |
|  2. gâ‚œ â† âˆ‡L(Î¸â‚œâ‚‹â‚)              # Get gradient           |
|                                                         |
|  3. mâ‚œ â† Î²â‚Â·mâ‚œâ‚‹â‚ + (1-Î²â‚)Â·gâ‚œ   # Update 1st moment     |
|                                                         |
|  4. vâ‚œ â† Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)Â·gâ‚œÂ²  # Update 2nd moment     |
|                                                         |
|  5. mÌ‚â‚œ â† mâ‚œ/(1 - Î²â‚áµ—)          # Bias correction       |
|     vÌ‚â‚œ â† vâ‚œ/(1 - Î²â‚‚áµ—)                                  |
|                                                         |
|  6. Î¸â‚œ â† Î¸â‚œâ‚‹â‚ - Î±Â·mÌ‚â‚œ/(âˆšvÌ‚â‚œ + Îµ)  # Parameter update    |
|                                                         |
+---------------------------------------------------------+

```

---

### 2. Building Blocks: From SGD to Adam

**Component 1: Exponential Moving Average (Momentum)**

```
First moment: mâ‚œ = Î²â‚Â·mâ‚œâ‚‹â‚ + (1-Î²â‚)Â·gâ‚œ

Expansion:
  mâ‚œ = (1-Î²â‚) Î£áµ¢â‚Œâ‚€^{t-1} Î²â‚^i Â· gâ‚œâ‚‹áµ¢
  
  = (1-Î²â‚)[gâ‚œ + Î²â‚Â·gâ‚œâ‚‹â‚ + Î²â‚Â²Â·gâ‚œâ‚‹â‚‚ + ...]

Effective window:
  w = 1/(1-Î²â‚)
  
  Î²â‚ = 0.9  â†’ w = 10  gradients
  Î²â‚ = 0.99 â†’ w = 100 gradients

Effect: Smooths noisy gradients (like momentum)

```

**Component 2: Adaptive Learning Rate (RMSprop)**

```
Second moment: vâ‚œ = Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)Â·gâ‚œÂ²

Expansion:
  vâ‚œ = (1-Î²â‚‚) Î£áµ¢â‚Œâ‚€^{t-1} Î²â‚‚^i Â· gâ‚œâ‚‹áµ¢Â²
  
  â‰ˆ E[gâ‚œÂ²]  (exponential moving average of squared gradients)

Effective window:
  w = 1/(1-Î²â‚‚)
  
  Î²â‚‚ = 0.999 â†’ w = 1000 gradients (long memory!)

Update:
  Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·gâ‚œ/âˆš(vâ‚œ + Îµ)
  
  Effect: Larger steps for parameters with small gradients
          Smaller steps for parameters with large gradients

```

**Component 3: Bias Correction**

```
Problem: Initial estimates are biased toward 0

  mâ‚€ = 0, then mâ‚ = (1-Î²â‚)Â·gâ‚
  
  But we want: E[mâ‚] = E[gâ‚]
  
  Actually: E[mâ‚] = (1-Î²â‚)Â·E[gâ‚] â‰  E[gâ‚]  (biased!)

Solution: Divide by (1 - Î²â‚áµ—)
  
  mÌ‚â‚œ = mâ‚œ/(1 - Î²â‚áµ—)
  
  E[mÌ‚â‚œ] = E[mâ‚œ]/(1 - Î²â‚áµ—)
        = (1 - Î²â‚áµ—)Â·E[gâ‚œ]/(1 - Î²â‚áµ—)
        = E[gâ‚œ]  âœ“ (unbiased!)

As t â†’ âˆ: (1 - Î²â‚áµ—) â†’ 1, so correction vanishes

```

**Putting It Together: Adam Update**

```
Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î± Â· [mâ‚œ/(1-Î²â‚áµ—)] / âˆš[vâ‚œ/(1-Î²â‚‚áµ—) + Îµ]

   = Î¸â‚œâ‚‹â‚ - Î± Â· [mâ‚œÂ·âˆš(1-Î²â‚‚áµ—)] / [âˆšvâ‚œÂ·(1-Î²â‚áµ—) + Îµ']

where Îµ' = ÎµÂ·(1-Î²â‚áµ—)/âˆš(1-Î²â‚‚áµ—)

Interpretation:
  â€¢ Numerator: Bias-corrected momentum
  â€¢ Denominator: Bias-corrected RMS of gradients
  â€¢ Result: Adaptive per-parameter learning rates

```

---

### 3. Why Bias Correction Matters: Quantitative Analysis

**Without Bias Correction:**

```
Early iterations (t=1):
  mâ‚ = (1-Î²â‚)Â·gâ‚ = 0.1Â·gâ‚   (with Î²â‚=0.9)
  vâ‚ = (1-Î²â‚‚)Â·gâ‚Â² = 0.001Â·gâ‚Â²  (with Î²â‚‚=0.999)
  
  Update: Î¸â‚ = Î¸â‚€ - Î±Â·(0.1Â·gâ‚)/âˆš(0.001Â·gâ‚Â² + Îµ)
            â‰ˆ Î¸â‚€ - Î±Â·(0.1Â·gâ‚)/(0.032Â·|gâ‚|)
            = Î¸â‚€ - 3.16Â·Î±Â·sign(gâ‚)
  
  Problem: Step size 3Ã— too large in early iterations!

```

**With Bias Correction:**

```
Early iterations (t=1):
  mÌ‚â‚ = mâ‚/(1-Î²â‚Â¹) = 0.1Â·gâ‚/0.1 = gâ‚
  vÌ‚â‚ = vâ‚/(1-Î²â‚‚Â¹) = 0.001Â·gâ‚Â²/0.001 = gâ‚Â²
  
  Update: Î¸â‚ = Î¸â‚€ - Î±Â·gâ‚/âˆš(gâ‚Â² + Îµ)
            â‰ˆ Î¸â‚€ - Î±Â·sign(gâ‚)
  
  Correct behavior! Step size â‰ˆ Î± as expected

```

**Convergence of Bias:**

```
Bias factor: bâ‚œ = 1 - Î²â‚áµ—

  t=1:    bâ‚ = 0.1      (10% of true value without correction)
  t=10:   bâ‚â‚€ = 0.651   (65%)
  t=100:  bâ‚â‚€â‚€ = 0.9999 (essentially 1)

After ~20-30 iterations, bias correction has minimal effect

```

---

### 4. Convergence Analysis: Adam Theory

**Theorem (Reddi et al., 2018): Adam May Not Converge**

```
Counterexample: Simple online learning problem
  gâ‚œ = +C  if t mod 3 âˆˆ {1, 2}
  gâ‚œ = -2C otherwise
  
  Average gradient: E[gâ‚œ] = 0
  
  But Adam converges to wrong point!

Problem: Adaptive learning rate can cause non-convergence
         when gradient variance is high

```

**AMSGrad Fix:**

```
Modification: Use maximum of past vÌ‚â‚œ

  vâ‚œ = Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)Â·gâ‚œÂ²
  vÌ‚â‚œ = max(vÌ‚â‚œâ‚‹â‚, vâ‚œ/(1-Î²â‚‚áµ—))  # Max with previous!
  Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·mÌ‚â‚œ/âˆš(vÌ‚â‚œ + Îµ)

Effect: Ensures learning rate never increases
Result: Provable convergence (under standard assumptions)

Practical note: AMSGrad rarely better than Adam empirically

```

**Convergence Rate (Under Assumptions):**

```
For convex f with bounded gradients:
  E[f(Î¸â‚œ) - f(Î¸*)] = O(1/âˆšT)

For non-convex f:
  (1/T) Î£â‚œ E[||âˆ‡f(Î¸â‚œ)||Â²] = O(1/âˆšT)
  
  Interpretation: Finds stationary point in O(1/ÎµÂ²) iterations

Similar to SGD, but often faster in practice due to adaptivity

```

---

### 5. AdamW: Decoupling Weight Decay

**Problem with Adam + L2 Regularization:**

```
Standard L2 regularization:
  L_reg(Î¸) = L(Î¸) + (Î»/2)||Î¸||Â²
  
  Gradient: âˆ‡L_reg = âˆ‡L + Î»Â·Î¸
  
  Adam applies to total gradient:
    mâ‚œ = Î²â‚Â·mâ‚œâ‚‹â‚ + (1-Î²â‚)(âˆ‡L + Î»Â·Î¸)
    vâ‚œ = Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)(âˆ‡L + Î»Â·Î¸)Â²
    
  Problem: Weight decay gets scaled by adaptive learning rate!
           Not equivalent to L2 regularization

```

**AdamW Solution (Loshchilov & Hutter, 2019):**

```
Decouple weight decay from gradient:
  
  mâ‚œ = Î²â‚Â·mâ‚œâ‚‹â‚ + (1-Î²â‚)Â·âˆ‡L(Î¸â‚œ)  (no Î»Â·Î¸!)
  vâ‚œ = Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)Â·(âˆ‡L(Î¸â‚œ))Â²
  mÌ‚â‚œ = mâ‚œ/(1 - Î²â‚áµ—)
  vÌ‚â‚œ = vâ‚œ/(1 - Î²â‚‚áµ—)
  
  Î¸â‚œ = (1 - Î±Â·Î»)Â·Î¸â‚œâ‚‹â‚ - Î±Â·mÌ‚â‚œ/(âˆšvÌ‚â‚œ + Îµ)
       â†‘
       Decoupled weight decay!

Effect: Weight decay independent of learning rate and adaptivity
Result: Much better for transformer training

```

**Why AdamW Works Better:**

```
Adam + L2:
  Step in Î¸áµ¢ âˆ Î±/(âˆšváµ¢ + Îµ)
  
  Parameters with large gradients: Small adaptive LR
  â†’ Weight decay also reduced for these parameters
  â†’ Uneven regularization!

AdamW:
  Weight decay: (1 - Î±Â·Î») for ALL parameters
  â†’ Even regularization across all parameters
  â†’ Better generalization

Empirical result (BERT, GPT):
  AdamW consistently outperforms Adam + L2

```

---

### 6. Per-Parameter Learning Rates: The Key Insight

**Motivation: Different Parameters Need Different Learning Rates**

```
Consider neural network:
  Î¸ = [Wâ‚, bâ‚, Wâ‚‚, bâ‚‚, ...]
  
  Typical gradient magnitudes:
    ||âˆ‡Wâ‚|| = 0.1
    ||âˆ‡bâ‚|| = 0.001
    ||âˆ‡Wâ‚‚|| = 10.0
  
  With fixed LR Î± = 0.01:
    Î”bâ‚ = 0.00001  (too small!)
    Î”Wâ‚‚ = 0.1       (too large!)

```

**Adam's Adaptive Scaling:**

```
For parameter Î¸áµ¢:
  
  Effective learning rate: Î±_eff,i = Î±/âˆš(váµ¢ + Îµ)
  
  where váµ¢ â‰ˆ RMS of past gradients for Î¸áµ¢

Effect:
  Large typical gradients â†’ Large âˆšváµ¢ â†’ Small Î±_eff
  Small typical gradients â†’ Small âˆšváµ¢ â†’ Large Î±_eff
  
  Automatically balances learning rates!

```

**Mathematical Justification:**

```
Diagonal preconditioning interpretation:
  
  Adam update: Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·mâ‚œ/âˆšvâ‚œ
  
  Equivalent to: Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·Vâ‚œâ»Â¹/Â²Â·mâ‚œ
  
  where Vâ‚œ = diag(vâ‚œ) (diagonal preconditioner)

Compare to Newton's method:
  Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·Hâ»Â¹Â·âˆ‡f
  
  where H = Hessian (full matrix, expensive!)

Adam approximates Newton with diagonal Hessian!
  Vâ‚œ â‰ˆ diag(H)  (diagonal of Hessian)
  
  Cost: O(n) instead of O(nÂ³) for full Newton

```

---

### 7. Warmup: Why Transformers Need It

**Problem: Large Initial Updates**

```
At t=1 with random initialization:
  Gradients can be very large: ||gâ‚|| = O(100)
  
  Bias correction amplifies:
    mÌ‚â‚ = gâ‚/(1-Î²â‚) = 10Â·gâ‚  (with Î²â‚=0.9)
  
  Even with small Î± = 0.001:
    ||Î”Î¸â‚|| = Î±Â·||mÌ‚â‚||/âˆšvÌ‚â‚ can be huge
    
  Result: Training instability, NaN loss

```

**Linear Warmup Solution:**

```
For first T_warmup steps:
  
  Î±(t) = Î±_max Â· min(t/T_warmup, 1)
  
  Example (T_warmup = 1000):
    t=1:    Î± = 0.001Â·(1/1000)   = 0.000001
    t=500:  Î± = 0.001Â·(500/1000) = 0.0005
    t=1000: Î± = 0.001Â·(1000/1000) = 0.001
    t>1000: Î± = 0.001 (or decay schedule)

Effect: Start with tiny LR, gradually increase
Result: Stable training from start

```

**Why Transformers Especially Need Warmup:**

```
Transformers have:
  1. LayerNorm after embedding â†’ Normalized activations
  2. Random init with std = 0.02 â†’ Small weights
  3. But position embeddings added â†’ Large initial gradients
  
  Combination â†’ Very large gradients initially
  
  Without warmup: Embedding gradients destroy initialization
  With warmup: Gradual adaptation to data statistics

```

**Typical Schedule (BERT/GPT):**

```
Total steps: T = 100,000
Warmup: T_warmup = 10,000 (10% of training)

Schedule:
  t â‰¤ 10k:    Î±(t) = Î±_max Â· (t/10k)              (warmup)
  t > 10k:    Î±(t) = Î±_max Â· âˆš(10k/t)             (decay)
  
  or
  
  t > 10k:    Î±(t) = Î±_max Â· (T-t)/(T-T_warmup)   (linear decay)

```

---

### 8. Adam Variants: A Zoo of Optimizers

**RMSprop (Hinton, 2012):**

```
No momentum, just adaptive LR:
  
  vâ‚œ = Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)Â·gâ‚œÂ²
  Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·gâ‚œ/âˆš(vâ‚œ + Îµ)
  
  Used in: Early RNNs, some RL

```

**AdaGrad (Duchi et al., 2011):**

```
Sum all past gradients (no exponential decay):
  
  vâ‚œ = vâ‚œâ‚‹â‚ + gâ‚œÂ²
  Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·gâ‚œ/âˆš(vâ‚œ + Îµ)
  
  Problem: vâ‚œ grows unbounded â†’ learning rate â†’ 0
  Good for: Sparse gradients (NLP with count features)

```

**AdamW (Loshchilov & Hutter, 2019):**

```
Adam + decoupled weight decay (covered above)
  
  Default for: Transformers (BERT, GPT, etc.)

```

**Nadam (Dozat, 2016):**

```
Adam + Nesterov momentum:
  
  mâ‚œ = Î²â‚Â·mâ‚œâ‚‹â‚ + (1-Î²â‚)Â·gâ‚œ
  Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î±Â·(Î²â‚Â·mÌ‚â‚œ + (1-Î²â‚)Â·gâ‚œ/(1-Î²â‚áµ—))/âˆš(vÌ‚â‚œ + Îµ)
  
  Slightly better than Adam in some cases

```

**RAdam (Liu et al., 2020):**

```
Rectified Adam: Better bias correction
  
  Adapts learning rate based on variance estimate quality
  Claims to avoid need for warmup (debated)

```

**LAMB (You et al., 2020):**

```
Layer-wise Adaptive Moments:
  
  For each layer l:
    Î¸â‚œâ½Ë¡â¾ = Î¸â‚œâ‚‹â‚â½Ë¡â¾ - Î· Â· (||Î¸â‚œâ‚‹â‚â½Ë¡â¾||/||mâ‚œâ½Ë¡â¾/âˆšvâ‚œâ½Ë¡â¾||) Â· (mâ‚œâ½Ë¡â¾/âˆšvâ‚œâ½Ë¡â¾)
  
  Effect: Layer-wise learning rate adaptation
  Used for: Very large batch training (batch 64k+)
  Application: BERT trained in 76 minutes!

```

---

### 9. Hyperparameter Sensitivity Analysis

**Learning Rate Î±:**

```
Most important hyperparameter!

Typical ranges:
  â€¢ Transformers: 1e-4 to 5e-4
  â€¢ CNNs: 3e-4 to 1e-3
  â€¢ Small models: 1e-3 (default)

Sensitivity: HIGH
  â€¢ 2Ã— too large â†’ Training diverges
  â€¢ 2Ã— too small â†’ Very slow convergence

Tuning strategy:
  1. Try default (1e-3)
  2. If unstable, reduce by 3Ã— (3e-4)
  3. If too slow, increase by 3Ã— (3e-3)
  4. Fine-tune within Â±30%

```

**Î²â‚ (First Moment Decay):**

```
Default: 0.9
Range: [0.8, 0.99]

Sensitivity: LOW to MEDIUM
  â€¢ Larger Î²â‚ â†’ Smoother updates, more momentum
  â€¢ Smaller Î²â‚ â†’ More responsive to recent gradients

Typical adjustments:
  â€¢ Noisy gradients: Î²â‚ = 0.95 (more smoothing)
  â€¢ Need responsiveness: Î²â‚ = 0.85 (less smoothing)
  
Most people never change this!

```

**Î²â‚‚ (Second Moment Decay):**

```
Default: 0.999
Range: [0.99, 0.9999]

Sensitivity: VERY LOW
  â€¢ Î²â‚‚ controls how long to remember gradient magnitudes
  â€¢ 0.999 â†’ average over ~1000 steps
  â€¢ 0.99 â†’ average over ~100 steps

When to change:
  â€¢ Short training (<1000 steps): Î²â‚‚ = 0.99
  â€¢ Very long training (>100k steps): Î²â‚‚ = 0.9999
  â€¢ Sparse gradients: Î²â‚‚ = 0.98
  
99% of users: Keep default 0.999

```

**Îµ (Numerical Stability):**

```
Default: 1e-8
Range: [1e-7, 1e-8]

Sensitivity: VERY LOW (almost never matters)
  
Only matters when:
  â€¢ Some parameters have zero gradient for long time
  â€¢ Mixed precision training (fp16): Use Îµ = 1e-7

Advice: Forget this parameter exists

```

---

### 10. Adam vs SGD: The Great Debate

**Empirical Observations:**

```
Adam wins:
  âœ“ Transformers (BERT, GPT, T5)
  âœ“ GANs (StyleGAN, DCGAN)
  âœ“ Diffusion models
  âœ“ RNNs, LSTMs
  âœ“ Reinforcement learning
  âœ“ Less hyperparameter tuning needed

SGD+Momentum wins:
  âœ“ ResNet on ImageNet
  âœ“ Some CNNs (better generalization)
  âœ“ When batch size is small
  âœ“ Transfer learning (fine-tuning)

Tie:
  â‰ˆ Small fully connected networks
  â‰ˆ Logistic regression
  â‰ˆ Simple problems

```

**Why the Difference?**

```
Theory (Wilson et al., 2017):
  Adam finds sharper minima â†’ worse generalization
  SGD finds flatter minima â†’ better generalization
  
  But: For transformers, Adam's adaptivity crucial
       Position/token embeddings need different LRs

Practice:
  â€¢ Adam: "Works out of the box"
  â€¢ SGD: Needs careful LR tuning, but can be better

Recommendation:
  1. Try Adam first (default Î±=1e-3)
  2. If not good enough, try SGD+Momentum
  3. Tune LR carefully for SGD
  4. For transformers: Always AdamW

```

---

### 11. Common Pitfalls and Solutions

**1. Forgetting Weight Decay:**

```
Problem: Adam without weight decay â†’ overfitting

Solution:
  optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=0.01)
  
  NOT:
  optimizer = torch.optim.Adam(params, lr=1e-4, weight_decay=0.01)

```

**2. No Learning Rate Schedule:**

```
Problem: Constant LR â†’ suboptimal final performance

Solution: Add cosine annealing or linear decay
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(...)

```

**3. Wrong Warmup:**

```
Problem: Start with full LR â†’ NaN loss

Solution: Always warmup for transformers
  for step in range(warmup_steps):
      lr = max_lr * step / warmup_steps
      for param_group in optimizer.param_groups:
          param_group['lr'] = lr

```

**4. Batch Size Scaling:**

```
Problem: Increase batch size, keep same LR â†’ worse results

Linear scaling rule (Goyal et al.):
  batch=256, lr=1e-4  â†’  batch=1024, lr=4e-4
  
  But: Only works up to certain batch size (âˆ¼2048)

```

---

### 12. Code: Manual Implementation

```python
import numpy as np

class Adam:
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        self.params = list(params)
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.weight_decay = weight_decay
        
        # Initialize moments
        self.m = [np.zeros_like(p) for p in self.params]
        self.v = [np.zeros_like(p) for p in self.params]
        self.t = 0
    
    def step(self, gradients):
        self.t += 1
        
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            # Add weight decay to gradient (L2 regularization)
            if self.weight_decay != 0:
                grad = grad + self.weight_decay * param
            
            # Update biased first moment estimate
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            
            # Update biased second raw moment estimate
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)
            
            # Compute bias-corrected first moment estimate
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            
            # Compute bias-corrected second raw moment estimate
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            # Update parameters
            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)

# Usage example
params = [np.random.randn(100, 50), np.random.randn(50)]
optimizer = Adam(params, lr=1e-3, betas=(0.9, 0.999))

for epoch in range(1000):
    grads = compute_gradients(params)  # Your gradient computation
    optimizer.step(grads)

```

---

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“„ | Original Adam Paper | [arXiv](https://arxiv.org/abs/1412.6980) |
| ğŸ“„ | AdamW Paper | [arXiv](https://arxiv.org/abs/1711.05101) |
| ğŸ¥ | Adam Explained | [YouTube](https://www.youtube.com/watch?v=JXQT_vxqwIs) |
| ğŸ‡¨ğŸ‡³ | çŸ¥ä¹ Adamè¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/32230623) |
| ğŸ‡¨ğŸ‡³ | CSDN AdamåŸç† | [CSDN](https://blog.csdn.net/willduan1/article/details/78070086) |

---

â¬…ï¸ [Back: Machine Learning](../) | â¡ï¸ [Next: SGD](../02_sgd/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
