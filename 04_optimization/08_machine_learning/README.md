<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Machine%20Learning%20Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## üìÇ Subtopics

| Folder | Topic | Used In |
|--------|-------|---------|
| [01_adam/](./01_adam/) | Adam Optimizer | GPT, Stable Diffusion |
| [02_sgd/](./02_sgd/) | SGD & Variants | ResNet, BERT |

---

## üéØ Why ML Optimization is Special

```
+---------------------------------------------------------+
|                                                         |
|   Classical Optimization:                               |
|   ‚Ä¢ Full gradient ‚àáf(x) available                       |
|   ‚Ä¢ Single objective                                    |
|   ‚Ä¢ Compute-bound                                       |
|                                                         |
|   Machine Learning:                                     |
|   ‚Ä¢ Stochastic gradients (mini-batch)                   |
|   ‚Ä¢ Non-convex (neural networks)                        |
|   ‚Ä¢ Billions of parameters                              |
|   ‚Ä¢ Memory-bound                                        |
|                                                         |
+---------------------------------------------------------+
```

---

# Part 1: Stochastic Gradient Descent (SGD)

## üìê Mathematical Formulation

### The Problem

```
minimize  f(Œ∏) = (1/n) Œ£·µ¢ f·µ¢(Œ∏)

where f·µ¢ = loss on data point i
      n = dataset size (millions/billions)
      Œ∏ = model parameters (millions/billions)

Computing full gradient is expensive: O(n)
```

### SGD Update Rule

```
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑ ‚àáf·µ¢‚Çú(Œ∏‚Çú)

where i‚Çú is randomly sampled from {1,...,n}

Key insight: E[‚àáf·µ¢(Œ∏)] = ‚àáf(Œ∏)  (unbiased!)
```

### Mini-batch SGD

```
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑ ¬∑ (1/|B|) Œ£·µ¢‚ààB ‚àáf·µ¢(Œ∏‚Çú)

where B = mini-batch of size b

Properties:
‚Ä¢ Variance ‚àù 1/b
‚Ä¢ Parallelizable (GPU-friendly)
‚Ä¢ Typical b: 32, 64, 128, 256, 512
```

---

## üìê Convergence Analysis

### Assumptions

```
1. L-smoothness: ||‚àáf(x) - ‚àáf(y)|| ‚â§ L||x - y||
2. Bounded variance: E[||‚àáf·µ¢(Œ∏) - ‚àáf(Œ∏)||¬≤] ‚â§ œÉ¬≤
3. (Optional) Œº-strong convexity: f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y-x) + (Œº/2)||y-x||¬≤
```

### Convex Case: Convergence Theorem

**Theorem:** For L-smooth convex f with bounded variance œÉ¬≤, SGD with Œ∑ = 1/(L‚àöT) achieves:

```
E[f(Œ∏ÃÑ‚Çú)] - f* ‚â§ O(||Œ∏‚ÇÄ - Œ∏*||¬≤L/T + œÉ||Œ∏‚ÇÄ - Œ∏*||/‚àöT)

= O(1/‚àöT)
```

**Proof:**

```
Step 1: Smoothness bound
f(Œ∏‚Çú‚Çä‚ÇÅ) ‚â§ f(Œ∏‚Çú) + ‚àáf(Œ∏‚Çú)·µÄ(Œ∏‚Çú‚Çä‚ÇÅ - Œ∏‚Çú) + (L/2)||Œ∏‚Çú‚Çä‚ÇÅ - Œ∏‚Çú||¬≤
       = f(Œ∏‚Çú) - Œ∑‚àáf(Œ∏‚Çú)·µÄg‚Çú + (LŒ∑¬≤/2)||g‚Çú||¬≤

where g‚Çú = ‚àáf·µ¢‚Çú(Œ∏‚Çú) is stochastic gradient

Step 2: Take expectation
E[f(Œ∏‚Çú‚Çä‚ÇÅ)] ‚â§ E[f(Œ∏‚Çú)] - Œ∑E[‚àáf(Œ∏‚Çú)·µÄg‚Çú] + (LŒ∑¬≤/2)E[||g‚Çú||¬≤]

Since E[g‚Çú|Œ∏‚Çú] = ‚àáf(Œ∏‚Çú):
E[‚àáf(Œ∏‚Çú)·µÄg‚Çú] = E[||‚àáf(Œ∏‚Çú)||¬≤]

And E[||g‚Çú||¬≤] = ||‚àáf(Œ∏‚Çú)||¬≤ + œÉ¬≤ (variance decomposition)

Step 3: Simplify
E[f(Œ∏‚Çú‚Çä‚ÇÅ)] ‚â§ E[f(Œ∏‚Çú)] - Œ∑(1 - LŒ∑/2)E[||‚àáf(Œ∏‚Çú)||¬≤] + (LŒ∑¬≤œÉ¬≤/2)

Step 4: With Œ∑ ‚â§ 1/L and convexity
||‚àáf(Œ∏‚Çú)||¬≤ ‚â• 2Œº(f(Œ∏‚Çú) - f*)  (PL condition for strongly convex)

Step 5: Telescope sum over T iterations
For Œ∑ = 1/(L‚àöT), summing and using convexity of f(Œ∏ÃÑ):

E[f(Œ∏ÃÑ)] - f* ‚â§ O(1/‚àöT)  ‚àé
```

### Strongly Convex Case

```
With Œº-strong convexity and Œ∑ = 1/(Œºt):

E[f(Œ∏‚Çú)] - f* ‚â§ O(œÉ¬≤/(ŒºT))

Linear convergence to neighborhood of optimum!
```

### Non-Convex Case

```
For L-smooth non-convex f:

(1/T) Œ£‚Çú E[||‚àáf(Œ∏‚Çú)||¬≤] ‚â§ O((f(Œ∏‚ÇÄ) - f*)/(Œ∑T) + LŒ∑œÉ¬≤)

SGD finds approximate stationary point!
(but may be saddle or local min)
```

---

## üìê SGD with Momentum

### Update Rule

```
v‚Çú = Œ≥v‚Çú‚Çã‚ÇÅ + ‚àáf(Œ∏‚Çú)
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑v‚Çú

Or equivalently:
v‚Çú = Œ≥v‚Çú‚Çã‚ÇÅ + Œ∑‚àáf(Œ∏‚Çú)
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - v‚Çú

where Œ≥ ‚àà [0.9, 0.99] typically
```

### Why Momentum Helps

```
Physical intuition: Ball rolling down hill

Without momentum:          With momentum:
        ‚Üì                       ‚Üì‚Üì‚Üì
       ‚ï± ‚ï≤                     ‚ï±   ‚ï≤
      ‚ï±   ‚ï≤                   ‚ï±     ‚ï≤
     ‚ï±  ‚Üì  ‚ï≤                 ‚ï± ‚Üì‚Üì‚Üì   ‚ï≤
    ‚ï±       ‚ï≤               ‚ï±         ‚ï≤
   
   Oscillates in            Accelerates in
   narrow valleys           consistent direction
```

### Convergence Improvement

```
For quadratic f(x) = (1/2)x·µÄAx with eigenvalues Œª_min ‚â§ ... ‚â§ Œª_max:

Without momentum: Œ∫ = Œª_max/Œª_min iterations
With momentum:    ‚àöŒ∫ iterations

Optimal Œ≥ = (‚àöŒ∫ - 1)/(‚àöŒ∫ + 1) ‚âà 1 - 2/‚àöŒ∫
```

---

## üíª SGD Implementation

```python
import numpy as np

class SGD:
    def __init__(self, params, lr=0.01, momentum=0.0, weight_decay=0.0):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.velocity = [np.zeros_like(p) for p in params]
    
    def step(self, grads):
        for i, (param, grad) in enumerate(zip(self.params, grads)):
            # Weight decay (L2 regularization)
            if self.weight_decay > 0:
                grad = grad + self.weight_decay * param
            
            # Momentum
            self.velocity[i] = self.momentum * self.velocity[i] + grad
            
            # Update
            param -= self.lr * self.velocity[i]

# PyTorch equivalent
import torch

optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.01, 
    momentum=0.9,
    weight_decay=1e-4,
    nesterov=True  # Nesterov momentum
)

# Training loop
for x, y in dataloader:
    optimizer.zero_grad()
    loss = criterion(model(x), y)
    loss.backward()
    optimizer.step()
```

### Nesterov Momentum

```
"Look ahead" before computing gradient:

Œ∏_lookahead = Œ∏‚Çú - Œ≥v‚Çú‚Çã‚ÇÅ
v‚Çú = Œ≥v‚Çú‚Çã‚ÇÅ + Œ∑‚àáf(Œ∏_lookahead)
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - v‚Çú

Intuition: Correct momentum direction using future position
Achieves optimal O(1/T¬≤) rate for convex problems!
```

---

# Part 2: Adam Optimizer

## üìê The Adam Algorithm

```
Adam = Adaptive Moment Estimation

Combines:
‚Ä¢ Momentum (first moment)
‚Ä¢ RMSprop (second moment)
‚Ä¢ Bias correction

Update rule:
m‚Çú = Œ≤‚ÇÅm‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)g‚Çú           (first moment estimate)
v‚Çú = Œ≤‚ÇÇv‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ)g‚Çú¬≤          (second moment estimate)
mÃÇ‚Çú = m‚Çú/(1-Œ≤‚ÇÅ·µó)                   (bias-corrected first)
vÃÇ‚Çú = v‚Çú/(1-Œ≤‚ÇÇ·µó)                   (bias-corrected second)
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑ mÃÇ‚Çú/(‚àövÃÇ‚Çú + Œµ)

Default values:
Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999, Œµ = 10‚Åª‚Å∏
```

---

## üìê Why Bias Correction?

```
Problem: Initial moments are biased toward zero

m‚ÇÄ = 0, v‚ÇÄ = 0

After t steps:
E[m‚Çú] = (1-Œ≤‚ÇÅ·µó) E[g]  ‚â† E[g]  (biased!)
E[v‚Çú] = (1-Œ≤‚ÇÇ·µó) E[g¬≤] ‚â† E[g¬≤] (biased!)

Bias correction fixes this:
E[mÃÇ‚Çú] = E[m‚Çú]/(1-Œ≤‚ÇÅ·µó) = E[g]    ‚úì
E[vÃÇ‚Çú] = E[v‚Çú]/(1-Œ≤‚ÇÇ·µó) = E[g¬≤]   ‚úì
```

### Derivation of Bias Correction

```
m‚Çú = (1-Œ≤‚ÇÅ) Œ£·µ¢‚Çå‚ÇÅ·µó Œ≤‚ÇÅ·µó‚Åª‚Å± g·µ¢

E[m‚Çú] = (1-Œ≤‚ÇÅ) Œ£·µ¢‚Çå‚ÇÅ·µó Œ≤‚ÇÅ·µó‚Åª‚Å± E[g]
      = E[g] (1-Œ≤‚ÇÅ) (1-Œ≤‚ÇÅ·µó)/(1-Œ≤‚ÇÅ)
      = E[g] (1-Œ≤‚ÇÅ·µó)

Therefore:
E[m‚Çú/(1-Œ≤‚ÇÅ·µó)] = E[g]  ‚úì
```

---

## üìê Adam Convergence

**Theorem (Kingma & Ba, 2015):** For convex f with bounded gradients, Adam achieves:

```
Regret ‚â§ O(d‚àöT)

where d = dimension, T = iterations

Equivalent to O(1/‚àöT) convergence rate
```

**Proof Sketch:**

```
Step 1: Define regret
R‚Çú = Œ£‚Çõ‚Çå‚ÇÅ·µó (f(Œ∏‚Çõ) - f(Œ∏*))

Step 2: Per-step bound (using online learning analysis)
f(Œ∏‚Çú) - f(Œ∏*) ‚â§ ‚ü®g‚Çú, Œ∏‚Çú - Œ∏*‚ü©

Step 3: Adaptive learning rate helps
With vÃÇ‚Çú tracking gradient magnitudes:
‚Ä¢ Large gradients ‚Üí smaller effective lr ‚Üí stability
‚Ä¢ Small gradients ‚Üí larger effective lr ‚Üí faster progress

Step 4: Bound via potential function
Careful analysis of ||Œ∏‚Çú - Œ∏*||¬≤_diag(‚àövÃÇ‚Çú)
yields O(d‚àöT) regret ‚àé
```

---

## üìê Adam vs SGD: The Great Debate

```
+---------------------------------------------------------+
|                                                         |
|   Adam wins:                     SGD wins:              |
|   ----------                     ---------              |
|   ‚Ä¢ Faster initial progress      ‚Ä¢ Better final acc     |
|   ‚Ä¢ Less lr tuning               ‚Ä¢ Simpler analysis     |
|   ‚Ä¢ Sparse gradients             ‚Ä¢ Better generalization|
|   ‚Ä¢ NLP, transformers            ‚Ä¢ Vision (sometimes)   |
|                                                         |
+---------------------------------------------------------+

Why SGD generalizes better (conjecture):
‚Ä¢ Adam finds "sharp" minima (poor generalization)
‚Ä¢ SGD's noise helps find "flat" minima (good generalization)
```

---

## üìê AdamW: Weight Decay Done Right

```
Problem with Adam + L2 regularization:

Standard Adam:
g' = g + ŒªŒ∏
m‚Çú = Œ≤‚ÇÅm‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)g'

The regularization gets scaled by ‚àövÃÇ‚Çú, weakening it!

AdamW (decoupled weight decay):
m‚Çú = Œ≤‚ÇÅm‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)g‚Çú  (no ŒªŒ∏ here)
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑(mÃÇ‚Çú/(‚àövÃÇ‚Çú + Œµ) + ŒªŒ∏‚Çú)  (add separately)

This is the standard for modern transformers!
```

---

## üíª Adam Implementation

```python
import numpy as np

class Adam:
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), 
                 eps=1e-8, weight_decay=0.0):
        self.params = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.weight_decay = weight_decay
        
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        self.t = 0
    
    def step(self, grads):
        self.t += 1
        
        for i, (param, grad) in enumerate(zip(self.params, grads)):
            # Update biased first moment
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            
            # Update biased second moment
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2
            
            # Bias correction
            m_hat = self.m[i] / (1 - self.beta1**self.t)
            v_hat = self.v[i] / (1 - self.beta2**self.t)
            
            # Update with AdamW-style weight decay
            param -= self.lr * (m_hat / (np.sqrt(v_hat) + self.eps) + 
                               self.weight_decay * param)

# PyTorch
import torch

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    betas=(0.9, 0.999),
    weight_decay=0.01
)

# With learning rate scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=num_epochs
)

for epoch in range(num_epochs):
    for x, y in dataloader:
        optimizer.zero_grad()
        loss = criterion(model(x), y)
        loss.backward()
        optimizer.step()
    scheduler.step()
```

---

## üìê Other Adaptive Methods

### RMSprop

```
v‚Çú = Œ≥v‚Çú‚Çã‚ÇÅ + (1-Œ≥)g‚Çú¬≤
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑ g‚Çú/‚àö(v‚Çú + Œµ)

Adam's predecessor, no momentum or bias correction
```

### AdaGrad

```
v‚Çú = v‚Çú‚Çã‚ÇÅ + g‚Çú¬≤
Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑ g‚Çú/‚àö(v‚Çú + Œµ)

Problem: Learning rate decays to zero
Good for sparse gradients
```

### Comparison

| Optimizer | Momentum | Adaptive LR | Bias Correct | Best For |
|-----------|----------|-------------|--------------|----------|
| SGD | Optional | No | N/A | Vision |
| AdaGrad | No | Yes | No | Sparse |
| RMSprop | No | Yes | No | RNNs |
| Adam | Yes | Yes | Yes | Transformers |
| AdamW | Yes | Yes | Yes | Modern DL |

---

## üìê Learning Rate Schedules

```
Warmup + Decay (standard for transformers):

Œ∑(t) = { Œ∑_max ¬∑ t/T_warmup           if t < T_warmup
       { Œ∑_max ¬∑ cos(œÄ(t-T_warmup)/(2T_total))  otherwise

Why warmup?
‚Ä¢ Adam's m, v need time to initialize
‚Ä¢ Large initial gradients can destabilize

Why decay?
‚Ä¢ Helps convergence to better minima
‚Ä¢ Reduces final oscillation
```

---

## üìä Convergence Bounds Summary

| Algorithm | Convex | Strongly Convex | Non-Convex |
|-----------|--------|-----------------|------------|
| **GD** | O(1/T) | O(exp(-T)) | O(1/T) |
| **SGD** | O(1/‚àöT) | O(1/T) | O(1/‚àöT) |
| **SGD+Momentum** | O(1/T¬≤) | O(exp(-‚àöT)) | O(1/‚àöT) |
| **Adam** | O(1/‚àöT) | O(1/‚àöT) | O(1/‚àöT) |

---

## üìö References

| Type | Title | Link |
|------|-------|------|
| üìÑ | Adam Paper | [arXiv](https://arxiv.org/abs/1412.6980) |
| üìÑ | AdamW Paper | [arXiv](https://arxiv.org/abs/1711.05101) |
| üìÑ | On the Convergence of Adam | [arXiv](https://arxiv.org/abs/1904.09237) |
| üìñ | Deep Learning Book Ch 8 | [Book](https://www.deeplearningbook.org/) |
| üé• | Stanford CS231n | [Optimization Lecture](http://cs231n.stanford.edu/) |
| üá®üá≥ | ‰ºòÂåñÂô®ËØ¶Ëß£ | [Áü•‰πé](https://zhuanlan.zhihu.com/p/32230623) |
| üá®üá≥ | AdamÂéüÁêÜ‰∏éÂÆûÁé∞ | [Áü•‰πé](https://zhuanlan.zhihu.com/p/32626442) |

---

## üîó Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Computer Vision** | ResNet, ViT training |
| **NLP** | BERT, GPT, LLaMA |
| **Generative AI** | Diffusion models, GANs |
| **Reinforcement Learning** | Policy optimization |

---

‚¨ÖÔ∏è [Back: Integer Programming](../07_integer_programming/) | ‚û°Ô∏è [Next: Metaheuristics](../09_metaheuristics/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
