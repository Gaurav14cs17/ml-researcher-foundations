<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Stochastic%20Gradient%20Descent&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“ Mathematical Foundations

### Vanilla SGD
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· âˆ‡L_B(Î¸â‚œ)

Where B is a mini-batch (random subset)
E[âˆ‡L_B] = âˆ‡L (unbiased estimator)
```

### SGD with Momentum
```
vâ‚œâ‚Šâ‚ = Î² vâ‚œ + âˆ‡L_B(Î¸â‚œ)
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· vâ‚œâ‚Šâ‚

Î² typically 0.9 (exponential moving average of gradients)
```

### Nesterov Accelerated Gradient
```
Î¸_lookahead = Î¸â‚œ - Î² vâ‚œ
vâ‚œâ‚Šâ‚ = Î² vâ‚œ + âˆ‡L_B(Î¸_lookahead)
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· vâ‚œâ‚Šâ‚

Evaluates gradient at "lookahead" position
```

### Convergence Rate
```
For convex functions with ÏƒÂ² gradient variance:
E[f(Î¸â‚œ) - f*] â‰¤ O(1/âˆšt) + O(ÏƒÂ²/Î·)

Learning rate schedule:
Î·â‚œ = Î·â‚€ / âˆšt or step decay
```

---

## ğŸŒ Where SGD is Used

| Application | How | Paper/Reference |
|-------------|-----|-----------------|
| **GPT/LLM Training** | Mini-batch SGD on billions of tokens | [GPT-3 Paper](https://arxiv.org/abs/2005.14165) |
| **Diffusion Models** | Denoising score matching with SGD | [DDPM](https://arxiv.org/abs/2006.11239) |
| **ResNet/ImageNet** | SGD with momentum, batch norm | [ResNet](https://arxiv.org/abs/1512.03385) |
| **Recommendation Systems** | Matrix factorization with SGD | Netflix Prize |
| **Reinforcement Learning** | Policy gradient (a form of SGD) | [PPO](https://arxiv.org/abs/1707.06347) |

---

## ğŸ”— Dependency Graph

```
foundations/linear-algebra
         |
         v
    basic-methods/gradient-descent
         |
         v
+--------+--------+
|   SGD Variants  |
+-----------------+
| â€¢ vanilla-sgd   |
| â€¢ momentum      |
| â€¢ nesterov      |
| â€¢ learning-rates|
+--------+--------+
         |
         v
    machine-learning/adam
```

---

# Part 1: Vanilla SGD

## ğŸ“ Formula

```
+-------------------------------------------------+
|                                                 |
|   Î¸_{t+1} = Î¸_t - Î± Â· âˆ‡L(Î¸_t; x_i, y_i)        |
|                                                 |
|   where:                                        |
|   â€¢ Î¸ = parameters                              |
|   â€¢ Î± = learning rate                           |
|   â€¢ (x_i, y_i) = random sample from dataset    |
|                                                 |
+-------------------------------------------------+
```

---

## ğŸ¯ Key Insight

| Full Batch GD | Mini-Batch SGD |
|---------------|----------------|
| âˆ‡L = (1/N) Î£ âˆ‡L_i | âˆ‡L â‰ˆ (1/B) Î£ âˆ‡L_i |
| N = all data | B = batch size (32-512) |
| Exact gradient | Noisy estimate |
| Slow per step | Fast per step |
| Smooth path | Noisy path |

---

## ğŸŒ Real-World Applications

### 1. **Language Model Training (GPT, BERT)**
```
Dataset: Trillions of tokens
Batch size: 512 - 4096
Why SGD: Impossible to fit full dataset in memory
```

### 2. **Image Classification (ResNet on ImageNet)**
```
Dataset: 1.2M images
Batch size: 256
Why SGD: Memory efficient, good generalization
Paper: "Deep Residual Learning" (2015)
```

### 3. **Diffusion Models (Stable Diffusion)**
```
Training: Predict noise at each timestep
Loss: ||Îµ - Îµ_Î¸(x_t, t)||Â²
SGD variant: Adam (covered later)
Paper: "Denoising Diffusion Probabilistic Models"
```

---

## âš ï¸ Noise is a Feature, Not a Bug

```
Why noise helps:

1. Escapes local minima
   -----â€¢-----     With noise:    -----â€¢â†’â†’â†’-----
        â•²_â•±   -------------->          \_â†’â€¢_/
   Stuck here!                    Escapes!

2. Finds flatter minima (better generalization)
   
   Sharp minimum:     Flat minimum:
       |â•²             --------
       | â•²            â•²      â•±
       |  â€¢           â•²â€¢----â•±
   Overfits!          Generalizes!
```

---

## ğŸ’» PyTorch Example

```python
import torch
import torch.nn as nn

# Model and data
model = nn.Linear(784, 10)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(100):
    for x_batch, y_batch in dataloader:
        # Forward pass
        output = model(x_batch)
        loss = criterion(output, y_batch)
        
        # Backward pass (compute âˆ‡L)
        optimizer.zero_grad()
        loss.backward()
        
        # SGD update: Î¸ = Î¸ - Î±âˆ‡L
        optimizer.step()
```

---

## ğŸ“Š Convergence Analysis

| Assumption | Rate | Notes |
|------------|------|-------|
| Convex, L-smooth | O(1/âˆšT) | Sublinear |
| Strongly convex | O(1/T) | Linear |
| Non-convex | O(1/âˆšT) to stationary | Finds local min |

---

## ğŸ“ DETAILED MATHEMATICAL THEORY

### 1. SGD Algorithm: From Full Batch to Stochastic

**Full Batch Gradient Descent:**
```
Loss: L(Î¸) = (1/N) Î£áµ¢â‚Œâ‚â¿ â„“(Î¸; xáµ¢, yáµ¢)

Update:
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·âˆ‡L(Î¸â‚œ)
       = Î¸â‚œ - Î±Â·(1/N) Î£áµ¢â‚Œâ‚â¿ âˆ‡â„“(Î¸â‚œ; xáµ¢, yáµ¢)

Cost per iteration: O(N) gradient computations
```

**Stochastic Gradient Descent (SGD):**
```
Sample: Pick i uniformly at random from {1,...,N}

Stochastic gradient:
  gÌƒâ‚œ = âˆ‡â„“(Î¸â‚œ; xáµ¢, yáµ¢)  (single sample!)

Update:
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·gÌƒâ‚œ

Cost per iteration: O(1) gradient computation

Key property: E[gÌƒâ‚œ | Î¸â‚œ] = âˆ‡L(Î¸â‚œ)  (unbiased!)
```

**Mini-Batch SGD (Practical):**
```
Sample: Pick batch B âŠ‚ {1,...,N} of size b

Mini-batch gradient:
  gÌƒâ‚œ = (1/b) Î£áµ¢âˆˆB âˆ‡â„“(Î¸â‚œ; xáµ¢, yáµ¢)

Update:
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·gÌƒâ‚œ

Cost per iteration: O(b) gradient computations

Variance reduction: Var[gÌƒâ‚œ] âˆ 1/b
```

---

### 2. Convergence Theory: Convex Case

**Theorem 1: Sublinear Convergence (Convex + Smooth)**

```
Assumptions:
  1. L is convex: L(y) â‰¥ L(x) + âˆ‡L(x)áµ€(y-x)
  2. L is L-smooth: ||âˆ‡L(x) - âˆ‡L(y)|| â‰¤ L||x-y||
  3. Bounded gradients: E[||gÌƒâ‚œ||Â²] â‰¤ GÂ²
  4. Unbiased: E[gÌƒâ‚œ] = âˆ‡L(Î¸â‚œ)
  5. Constant step size: Î±â‚œ = Î± = 1/(2L)

Then:
  E[L(Î¸Ì„_T)] - L(Î¸*) â‰¤ (2L||Î¸â‚€ - Î¸*||Â² + Î±GÂ²T)/(2T)
                     = O(1/âˆšT)  when Î± = O(1/âˆšT)

where Î¸Ì„_T = (1/T) Î£â‚œâ‚Œâ‚áµ€ Î¸â‚œ (average iterate)
```

**Proof:**

```
Step 1: Descent lemma for stochastic update
  E[||Î¸â‚œâ‚Šâ‚ - Î¸*||Â²]
    = E[||Î¸â‚œ - Î±Â·gÌƒâ‚œ - Î¸*||Â²]
    = E[||Î¸â‚œ - Î¸*||Â²] - 2Î±Â·E[gÌƒâ‚œáµ€(Î¸â‚œ - Î¸*)] + Î±Â²E[||gÌƒâ‚œ||Â²]
    
  By unbiasedness: E[gÌƒâ‚œáµ€(Î¸â‚œ - Î¸*)] = âˆ‡L(Î¸â‚œ)áµ€(Î¸â‚œ - Î¸*)
  
  By convexity: âˆ‡L(Î¸â‚œ)áµ€(Î¸â‚œ - Î¸*) â‰¥ L(Î¸â‚œ) - L(Î¸*)
  
  Therefore:
  E[||Î¸â‚œâ‚Šâ‚ - Î¸*||Â²] â‰¤ ||Î¸â‚œ - Î¸*||Â² - 2Î±(L(Î¸â‚œ) - L(Î¸*)) + Î±Â²GÂ²

Step 2: Rearrange
  2Î±(L(Î¸â‚œ) - L(Î¸*)) â‰¤ ||Î¸â‚œ - Î¸*||Â² - E[||Î¸â‚œâ‚Šâ‚ - Î¸*||Â²] + Î±Â²GÂ²

Step 3: Sum from t=0 to T-1
  2Î± Î£â‚œ(L(Î¸â‚œ) - L(Î¸*)) â‰¤ ||Î¸â‚€ - Î¸*||Â² + TÎ±Â²GÂ²

Step 4: Average and apply Jensen's inequality
  By convexity: L(Î¸Ì„_T) â‰¤ (1/T) Î£â‚œ L(Î¸â‚œ)
  
  Therefore:
  2Î±T(L(Î¸Ì„_T) - L(Î¸*)) â‰¤ ||Î¸â‚€ - Î¸*||Â² + TÎ±Â²GÂ²
  
  L(Î¸Ì„_T) - L(Î¸*) â‰¤ (||Î¸â‚€ - Î¸*||Â²)/(2Î±T) + (Î±GÂ²)/2

Step 5: Optimize step size
  Set Î± = ||Î¸â‚€ - Î¸*||/(GâˆšT) to balance terms
  
  L(Î¸Ì„_T) - L(Î¸*) â‰¤ (G||Î¸â‚€ - Î¸*||)/âˆšT = O(1/âˆšT) âœ“  QED
```

**Key Insight:**
```
SGD converges O(1/âˆšT) vs GD's O(1/T)
  
BUT: SGD's cost per iteration is O(1) vs GD's O(N)

Total cost to reach Îµ-accuracy:
  GD:  O(N/Îµ) gradient evaluations
  SGD: O(1/ÎµÂ²) gradient evaluations

SGD wins when N > 1/Îµ (almost always in ML!)
```

---

### 3. Strongly Convex Case: Faster Convergence

**Theorem 2: Linear Convergence (Strongly Convex)**

```
Additional assumption:
  L is Î¼-strongly convex: L(y) â‰¥ L(x) + âˆ‡L(x)áµ€(y-x) + (Î¼/2)||y-x||Â²

With decreasing step size Î±â‚œ = Î±â‚€/(1 + Î¼Î±â‚€t):

  E[L(Î¸â‚œ) - L(Î¸*)] â‰¤ C/(Î¼Î±â‚€t) = O(1/t)

Much faster than O(1/âˆšt)!
```

---

### 4. Non-Convex Case: Stationary Points

**Theorem 3: Finding Stationary Points**

```
For non-convex L (no convexity assumption!):

With constant step size Î± = 1/(2L):
  (1/T) Î£â‚œâ‚Œâ‚€áµ€â»Â¹ E[||âˆ‡L(Î¸â‚œ)||Â²] â‰¤ (2L(L(Î¸â‚€) - L*))/T + LÏƒGÂ²/T

where ÏƒGÂ² = E[||gÌƒâ‚œ - âˆ‡L(Î¸â‚œ)||Â²] (gradient variance)

To find Îµ-stationary point (||âˆ‡L|| â‰¤ Îµ):
  T = O(1/ÎµÂ²) iterations
```

**Why This Matters for Deep Learning:**

```
Neural networks are non-convex, yet SGD works!

Empirical observations:
  1. Local minima are nearly as good as global minima
  2. High dimensionality â†’ saddle points, not local mins
  3. SGD noise helps escape saddle points
  4. Wide networks â†’ loss landscape becomes "nicer"
```

---

### 5. Variance Reduction: The Mini-Batch Effect

**Gradient Variance:**

```
Single sample (b=1):
  Var[gÌƒ] = E[||gÌƒ - âˆ‡L||Â²] = ÏƒGÂ²

Mini-batch (size b):
  Var[á¸¡] = E[||(1/b)Î£áµ¢â‚Œâ‚áµ‡ gÌƒáµ¢ - âˆ‡L||Â²]
         = ÏƒGÂ²/b  (variance decreases!)

Trade-off:
  â€¢ Larger b â†’ Less variance, smoother convergence
  â€¢ Smaller b â†’ More noise, better exploration
  â€¢ Optimal b depends on problem (typically 32-512)
```

---

### 6. Learning Rate Schedules: Theory

**Robbins-Monro Conditions (Theoretical):**

```
For convergence, need:
  1. Î£â‚œ Î±â‚œ = âˆ         (go far enough)
  2. Î£â‚œ Î±â‚œÂ² < âˆ        (noise decreases)

Examples:
  â€¢ Î±â‚œ = Î±â‚€/t          âœ“ (satisfies both)
  â€¢ Î±â‚œ = Î±â‚€/âˆšt         âœ“
  â€¢ Î±â‚œ = constant      âœ— (violates condition 2)
```

**Practical Schedules:**

```
1. Constant:
   Î±(t) = Î±â‚€
   
   Pros: Simple, works if Î±â‚€ well-tuned
   Cons: Never fully converges

2. Step decay:
   Î±(t) = Î±â‚€ Â· Î³^âŒŠt/sâŒ‹
   
   Example: Divide by 10 every 30 epochs
   Used in: ResNet ImageNet training

3. Cosine annealing:
   Î±(t) = Î±_min + (Î±_max - Î±_min)(1 + cos(Ï€t/T))/2
   
   Popular for transformers

4. Warmup + decay:
   Î±(t) = Î±_max Â· min(t/t_warmup, (t/t_warmup)^{-0.5})
   
   Critical for transformer training
```

---

### 7. Noise and Generalization: The Implicit Bias

**Why SGD Generalizes Better Than GD:**

```
Theory (Simplified):
  SGD introduces noise â†’ implicit regularization
  
  SGD tends to find flatter minima (better generalization)

Mathematical intuition:
  SDE approximation of SGD:
    dÎ¸_t = -âˆ‡L(Î¸_t)dt + âˆš(2Î±B)Â·dW_t
    
  where:
    B = covariance of gradient noise
    dW_t = Brownian motion
  
  Effect: SGD explores around minimum
  â†’ Finds wider valleys (flatter minima)
  â†’ Better generalization!
```

---

# Part 2: SGD with Momentum

## ğŸ“ Formula

```
+-------------------------------------------------+
|                                                 |
|   v_t = Î² Â· v_{t-1} + âˆ‡L(Î¸_t)                  |
|   Î¸_{t+1} = Î¸_t - Î± Â· v_t                      |
|                                                 |
|   where:                                        |
|   â€¢ v = velocity (accumulated gradient)         |
|   â€¢ Î² = momentum coefficient (typically 0.9)    |
|   â€¢ Î± = learning rate                           |
|                                                 |
+-------------------------------------------------+
```

---

## ğŸ¯ Visual Intuition

```
Without Momentum:              With Momentum:
                               
    â€¢                              â€¢
    |â•²                             â•²
    | â•²                             â•²
    |  â•²                             â•²
    |   â€¢                             â•²
    |  â•±                               â€¢
    | â•±                              (faster!)
    â€¢
  Oscillates                   Smooth path
```

---

## ğŸ”¬ Physics Analogy

```
Ball rolling down a hill:

              Start
                â€¢
               â•± velocity builds up
              â•±
             â€¢   
            â•±
           â•±
          â•±
         â€¢  
        â•±     
       â•±
      â€¢------â€¢  overshoots slightly
             â•±  then settles
            â€¢   
         Minimum

Î² = 0.9 means: "Remember 90% of previous velocity"
```

---

## ğŸ“Š Why Momentum Helps

| Problem | Without Momentum | With Momentum |
|---------|------------------|---------------|
| Ravines | Oscillates | Accelerates through |
| Saddle points | Stuck | Escapes faster |
| Noise | Noisy path | Smoothed |
| Convergence | Slow | 2-10x faster |

---

## ğŸ“ DETAILED MATHEMATICAL THEORY

### 1. Momentum: Mathematical Derivation

**Standard (Polyak) Momentum:**

```
Algorithm:
  vâ‚€ = 0
  For t = 0, 1, 2, ...:
    vâ‚œâ‚Šâ‚ = Î²Â·vâ‚œ + gâ‚œ
    Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·vâ‚œâ‚Šâ‚

where:
  vâ‚œ = velocity (accumulated gradient)
  Î² âˆˆ [0,1) = momentum coefficient (typically 0.9)
  gâ‚œ = âˆ‡L(Î¸â‚œ) or stochastic gradient
  Î± = learning rate
```

**Exponential Moving Average Interpretation:**

```
Expand vâ‚œ recursively:
  vâ‚œ = Î²Â·vâ‚œâ‚‹â‚ + gâ‚œ
     = Î²(Î²Â·vâ‚œâ‚‹â‚‚ + gâ‚œâ‚‹â‚) + gâ‚œ
     = Î²Â²Â·vâ‚œâ‚‹â‚‚ + Î²Â·gâ‚œâ‚‹â‚ + gâ‚œ
     = ...
     = Î£áµ¢â‚Œâ‚€^âˆ Î²^i Â· gâ‚œâ‚‹áµ¢

Effective averaging window:
  w_eff = Î£áµ¢â‚Œâ‚€^âˆ Î²^i = 1/(1-Î²)
  
  Î² = 0.9 â†’ w_eff = 10 gradients
  Î² = 0.99 â†’ w_eff = 100 gradients
  Î² = 0.999 â†’ w_eff = 1000 gradients
```

---

### 2. Acceleration: Why Momentum Converges Faster

**Without Momentum (GD on Strongly Convex):**

```
Convergence rate for condition number Îº = L/Î¼:
  ||Î¸â‚œ - Î¸*|| â‰¤ ((Îº-1)/(Îº+1))^t Â· ||Î¸â‚€ - Î¸*||
  
  Rate: Ï_GD = (Îº-1)/(Îº+1) â‰ˆ 1 - 2/Îº  (when Îº large)

Number of iterations to reach Îµ-accuracy:
  T_GD = O(Îº log(1/Îµ))
```

**With Optimal Momentum:**

```
Optimal Î²: Î²* = (âˆšÎº - 1)/(âˆšÎº + 1)

Convergence rate:
  ||Î¸â‚œ - Î¸*|| â‰¤ ((âˆšÎº-1)/(âˆšÎº+1))^t Â· ||Î¸â‚€ - Î¸*||
  
  Rate: Ï_Mom = (âˆšÎº-1)/(âˆšÎº+1) â‰ˆ 1 - 2/âˆšÎº  (when Îº large)

Number of iterations:
  T_Mom = O(âˆšÎº log(1/Îµ))

Speedup factor:
  T_GD/T_Mom = âˆšÎº
  
  Îº = 100 â†’ 10Ã— fewer iterations!
  Îº = 10000 â†’ 100Ã— fewer iterations!
```

**Proof Sketch (Strongly Convex Quadratics):**

```
Consider quadratic: f(Î¸) = (1/2)Î¸áµ€AÎ¸ - báµ€Î¸
  where A is positive definite with eigenvalues Î»â‚,...,Î»â‚™
  
  Condition number: Îº = Î»_max/Î»_min

Step 1: Momentum update in matrix form
  [Î¸â‚œâ‚Šâ‚]   [I - Î±A    Î²I] [Î¸â‚œ]     [0]
  [vâ‚œâ‚Šâ‚] = [  -Î±A   Î²I] [vâ‚œ] + [Î±b]

Step 2: Spectral analysis
  Convergence determined by spectral radius Ï(M) of update matrix M
  
  Ï_GD = max|(1 - Î±Â·Î»áµ¢)| = (Îº-1)/(Îº+1)  (for Î± = 2/(Î»_max+Î»_min))
  
  Ï_Mom = ((âˆšÎº-1)/(âˆšÎº+1))  (with optimal Î²)

Step 3: General convex case
  For general smooth strongly convex f:
    Similar analysis via Polyak-Lojasiewicz condition
    Result: O(âˆšÎº) acceleration holds âœ“  QED
```

---

### 3. Nesterov Accelerated Gradient (NAG)

**Nesterov Momentum (1983):**

```
Algorithm:
  vâ‚€ = 0
  For t = 0, 1, 2, ...:
    Î¸_lookahead = Î¸â‚œ - Î±Â·Î²Â·vâ‚œ       (lookahead!)
    vâ‚œâ‚Šâ‚ = Î²Â·vâ‚œ + âˆ‡f(Î¸_lookahead)   (gradient at lookahead)
    Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·vâ‚œâ‚Šâ‚

Key difference: Evaluate gradient at lookahead position!
```

**Why Lookahead Helps:**

```
Standard momentum: "Blind momentum"
  1. Apply momentum: Î¸_new = Î¸ - Î±Â·v
  2. Compute gradient at Î¸_new
  3. Update velocity
  
  Problem: May overshoot, then have to correct

Nesterov momentum: "Informed momentum"
  1. Look ahead: Î¸_look = Î¸ - Î±Â·Î²Â·v
  2. Compute gradient at Î¸_look (future position!)
  3. Correct velocity based on future gradient
  
  Benefit: Better anticipation of future gradient
```

**Convergence Guarantee:**

```
For smooth convex f:
  f(Î¸â‚œ) - f(Î¸*) â‰¤ (2L||Î¸â‚€ - Î¸*||Â²)/(t+1)Â²
  
  Rate: O(1/tÂ²) vs O(1/t) for GD

For smooth strongly convex f:
  Same O(âˆšÎº) as standard momentum, but better constants
```

---

### 4. Momentum in Non-Convex Optimization

**Escaping Saddle Points:**

```
Saddle point: âˆ‡f = 0, but Hessian has negative eigenvalues

Problem for GD:
  â€¢ Attracted to saddle from most directions
  â€¢ Can get stuck for many iterations
  â€¢ Escape time: exponential in dimension!

Momentum helps:
  â€¢ Kinetic energy carries through flat region
  â€¢ Escapes faster than GD
  â€¢ Escape time: polynomial in dimension

Mathematical intuition:
  At saddle with Hessian eigenvalue Î» < 0:
    Momentum amplifies motion in negative curvature direction
    Escape time: O(log(1/|Î»|)) vs O(1/|Î»|) for GD
```

---

### 5. Practical Hyperparameter Selection

**Momentum Coefficient Î²:**

```
Common values:
  Î² = 0.9   (default, works well most cases)
  Î² = 0.99  (slower but steadier convergence)
  Î² = 0.999 (very smooth, for noisy objectives)

Heuristic rule:
  Î² â‰ˆ 1 - 1/âˆšÎº  (where Îº = condition number)
  
  Well-conditioned (Îº â‰ˆ 10): Î² = 0.68
  Ill-conditioned (Îº â‰ˆ 100): Î² = 0.90
  Very ill-conditioned (Îº â‰ˆ 10000): Î² = 0.99

In practice: Just use Î² = 0.9 as starting point
```

**Learning Rate with Momentum:**

```
Rule of thumb:
  Î±_momentum â‰ˆ (1-Î²)Â·Î±_no_momentum
  
  Reason: Velocity accumulates, so effective step is larger

Example:
  No momentum: Î± = 0.1
  With Î² = 0.9: Î± = 0.01
  
  Effective step: (1/(1-Î²))Â·Î± = 10Â·0.01 = 0.1 (same!)
```

---

### 6. Code Implementation

```python
import numpy as np

class MomentumSGD:
    def __init__(self, params, lr=0.01, momentum=0.9, nesterov=False):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.nesterov = nesterov
        self.velocities = [np.zeros_like(p) for p in params]
    
    def step(self, gradients):
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            v = self.velocities[i]
            
            if self.nesterov:
                # Nesterov momentum
                v = self.momentum * v + grad
                param -= self.lr * (grad + self.momentum * v)
            else:
                # Standard (Polyak) momentum
                v = self.momentum * v + grad
                param -= self.lr * v
            
            self.velocities[i] = v

# PyTorch equivalent
import torch

optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    nesterov=True  # Nesterov momentum
)

for epoch in range(100):
    for x, y in dataloader:
        optimizer.zero_grad()
        loss = criterion(model(x), y)
        loss.backward()
        optimizer.step()
```

---

### 7. SGD Variants Summary

```
Vanilla SGD:
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·gÌƒâ‚œ
  
  Pros: Simple, unbiased
  Cons: High variance, slow

SGD with Momentum:
  vâ‚œ = Î²Â·vâ‚œâ‚‹â‚ + gÌƒâ‚œ
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·vâ‚œ
  
  Effect: Smooths gradients, accelerates
  Convergence: O(1/t) with Î² = 1-O(1/âˆšÎº)

Nesterov Momentum:
  vâ‚œ = Î²Â·vâ‚œâ‚‹â‚ + âˆ‡L(Î¸â‚œ - Î±Â·Î²Â·vâ‚œâ‚‹â‚)
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·vâ‚œ
  
  Better: Looks ahead before stepping

RMSprop (adaptive):
  vâ‚œ = Î²Â·vâ‚œâ‚‹â‚ + (1-Î²)Â·gÌƒâ‚œÂ²
  Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±Â·gÌƒâ‚œ/âˆš(vâ‚œ + Îµ)
  
  Effect: Per-parameter learning rates
```

---

## ğŸ“š References

| Type | Resource | Link |
|------|----------|------|
| ğŸ“„ | Robbins & Monro (1951) | Original stochastic approximation |
| ğŸ“„ | SGD Convergence | [Bottou et al., 2018](https://arxiv.org/abs/1606.04838) |
| ğŸ“„ | Polyak Momentum (1964) | Classic |
| ğŸ“„ | Sutskever - Momentum Importance | [Paper](https://www.cs.toronto.edu/~hinton/absps/momentum.pdf) |
| ğŸ“– | Goodfellow Ch. 8 | [Deep Learning Book](https://www.deeplearningbook.org/) |
| ğŸ¥ | Stanford CS231n | [Optimization Lecture](http://cs231n.stanford.edu/) |
| ğŸ‡¨ğŸ‡³ | SGDä¼˜åŒ–è¯¦è§£ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/22252270) |
| ğŸ‡¨ğŸ‡³ | åŠ¨é‡æ³•åŸç† | [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2017-07-12-8) |

---

â¬…ï¸ [Back: Adam](../01_adam/) | â¬†ï¸ [Up: Machine Learning](../)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
