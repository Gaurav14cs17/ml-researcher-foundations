<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Gradient%20Descent&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## üìÇ Topics in This Folder

| File | Topic | Level |
|------|-------|-------|

---

## üéØ The Core Idea

```
+---------------------------------------------------------+

|                                                         |
|   Want to minimize f(x)?                                |
|                                                         |
|   1. Compute gradient ‚àáf(x)                            |
|      (Direction of steepest ASCENT)                    |
|                                                         |
|   2. Move in OPPOSITE direction                        |
|      x_{new} = x_{old} - Œ±‚àáf(x_{old})                  |
|                                                         |
|   3. Repeat until convergence                          |
|                                                         |
|   That's it! Simple but powerful.                      |
|                                                         |
+---------------------------------------------------------+

```

---

## üìê The Algorithm

```python
# Gradient Descent in 5 lines
x = initial_guess
for i in range(max_iterations):
    gradient = compute_gradient(f, x)
    x = x - learning_rate * gradient
    if converged(gradient):
        break

```

---

## üéØ Visual Understanding

```
       Loss Surface (Mountain)
       
            Start here
               ‚óè‚ï≤
              ‚ï±  ‚ï≤
             ‚ï±    ‚ï≤
            ‚ï±      ‚óè  Step 1
           ‚ï±        ‚ï≤
          ‚ï±          ‚ï≤
         ‚ï±            ‚óè  Step 2
        ‚ï±              ‚ï≤
       ‚ï±                ‚óè  Step 3
      ‚ï±                  ‚ï≤
     ‚ï±____________________‚óè  Minimum!
     
   Each step: Move opposite to gradient (downhill)

```

---

## üìê DETAILED MATHEMATICAL THEORY

### 1. Gradient Descent: Complete Convergence Analysis

**Algorithm:**

```
Input: f: ‚Ñù‚Åø ‚Üí ‚Ñù, starting point x‚ÇÄ, learning rate Œ±
Output: x* ‚âà argmin f

For k = 0, 1, 2, ...:
  1. Compute gradient: g_k = ‚àáf(x_k)
  2. Update: x_{k+1} = x_k - Œ±¬∑g_k
  3. Check convergence: ||g_k|| < Œµ

Return x_k

```

---

### 2. Convergence for Convex + L-Smooth Functions

**Theorem 1: Sublinear Convergence**

```
Assumptions:
  1. f is convex
  2. f is L-smooth: ||‚àáf(x) - ‚àáf(y)|| ‚â§ L||x - y||
  3. Step size: Œ± = 1/L

Then: f(x_k) - f(x*) ‚â§ (2L||x_0 - x*||¬≤)/k

Convergence rate: O(1/k) iterations

```

**Proof:**

```
Step 1: L-smoothness implies quadratic upper bound
  For any x, y:
    f(y) ‚â§ f(x) + ‚àáf(x)·µÄ(y-x) + (L/2)||y-x||¬≤

Step 2: Apply to GD update with Œ± = 1/L
  Let y = x_{k+1} = x_k - (1/L)‚àáf(x_k)
  
  f(x_{k+1}) ‚â§ f(x_k) + ‚àáf(x_k)·µÄ(x_{k+1} - x_k) + (L/2)||x_{k+1} - x_k||¬≤
             = f(x_k) - (1/L)||‚àáf(x_k)||¬≤ + (L/2)¬∑(1/L¬≤)||‚àáf(x_k)||¬≤
             = f(x_k) - (1/2L)||‚àáf(x_k)||¬≤

Step 3: Descent lemma
  f(x_{k+1}) ‚â§ f(x_k) - (1/2L)||‚àáf(x_k)||¬≤  ... (*)

Step 4: By convexity
  f(x_k) - f(x*) ‚â§ ‚àáf(x_k)·µÄ(x_k - x*)  (first-order condition)
                 ‚â§ ||‚àáf(x_k)||¬∑||x_k - x*||  (Cauchy-Schwarz)

  Therefore: ||‚àáf(x_k)||¬≤ ‚â• (f(x_k) - f(x*))¬≤/||x_k - x*||¬≤

Step 5: Substitute into (*)
  f(x_{k+1}) ‚â§ f(x_k) - (1/2L)¬∑(f(x_k) - f(x*))¬≤/||x_k - x*||¬≤

Step 6: Track distance to optimum
  ||x_{k+1} - x*||¬≤ = ||x_k - (1/L)‚àáf(x_k) - x*||¬≤
                    = ||x_k - x*||¬≤ - (2/L)‚àáf(x_k)·µÄ(x_k - x*) + (1/L¬≤)||‚àáf(x_k)||¬≤

  By convexity: ‚àáf(x_k)·µÄ(x_k - x*) ‚â• f(x_k) - f(x*)
  
  Therefore:
  ||x_{k+1} - x*||¬≤ ‚â§ ||x_k - x*||¬≤ - (2/L)(f(x_k) - f(x*)) + (1/L¬≤)||‚àáf(x_k)||¬≤

Step 7: From Step 3, we have ||‚àáf(x_k)||¬≤ ‚â§ 2L(f(x_k) - f(x_{k+1}))
  
  Substituting:
  ||x_{k+1} - x*||¬≤ ‚â§ ||x_k - x*||¬≤ - (2/L)(f(x_k) - f(x*)) + (2/L)(f(x_k) - f(x_{k+1}))
                    = ||x_k - x*||¬≤ - (2/L)(f(x_{k+1}) - f(x*))

Step 8: Rearrange
  f(x_{k+1}) - f(x*) ‚â§ (L/2)(||x_k - x*||¬≤ - ||x_{k+1} - x*||¬≤)

Step 9: Sum telescoping series from 0 to k-1
  Œ£·µ¢‚Çå‚ÇÄ^{k-1} (f(x_{i+1}) - f(x*)) ‚â§ (L/2)||x_0 - x*||¬≤

  Since f(x_i) is decreasing:
  k¬∑(f(x_k) - f(x*)) ‚â§ Œ£·µ¢‚Çå‚ÇÄ^{k-1} (f(x_{i+1}) - f(x*)) ‚â§ (L/2)||x_0 - x*||¬≤

  Therefore: f(x_k) - f(x*) ‚â§ (L||x_0 - x*||¬≤)/(2k) ‚úì  QED

```

---

### 3. Strongly Convex Case: Linear Convergence

**Theorem 2: Exponential Convergence**

```
Additional assumption:
  f is Œº-strongly convex: f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y-x) + (Œº/2)||y-x||¬≤

Then with Œ± = 1/L:
  f(x_k) - f(x*) ‚â§ (1 - Œº/L)^k (f(x_0) - f(x*))

Convergence rate: O((1 - Œº/L)^k) = O(œÅ^k) where œÅ = 1 - Œº/L

```

**Key Quantity: Condition Number**

```
Œ∫ = L/Œº  (condition number)

Convergence rate: œÅ = 1 - 1/Œ∫ = (Œ∫-1)/Œ∫

Examples:
  Œ∫ = 2:   œÅ = 0.5    ‚Üí Half distance each step
  Œ∫ = 10:  œÅ = 0.9    ‚Üí 10% improvement per step
  Œ∫ = 100: œÅ = 0.99   ‚Üí 1% improvement per step (slow!)
  Œ∫ = ‚àû:   œÅ = 1      ‚Üí No improvement (ill-conditioned)

Number of iterations to reach Œµ-accuracy:
  k ‚â• Œ∫¬∑log(1/Œµ)

```

**Proof Sketch:**

```
Step 1: Combine descent lemma with strong convexity
  From L-smoothness:
    f(x_{k+1}) ‚â§ f(x_k) - (1/2L)||‚àáf(x_k)||¬≤
  
  From strong convexity:
    ||‚àáf(x_k)||¬≤ ‚â• 2Œº(f(x_k) - f(x*))
  
  Therefore:
    f(x_{k+1}) - f(x*) ‚â§ f(x_k) - f(x*) - (Œº/L)(f(x_k) - f(x*))
                       = (1 - Œº/L)(f(x_k) - f(x*))

Step 2: Apply recursively
  f(x_k) - f(x*) ‚â§ (1 - Œº/L)^k (f(x_0) - f(x*)) ‚úì  QED

```

---

### 4. Non-Convex Case: Stationary Points

**Theorem 3: First-Order Stationary Point**

```
For non-convex f (L-smooth):

GD with Œ± = 1/L satisfies:
  min_{0‚â§k‚â§K-1} ||‚àáf(x_k)||¬≤ ‚â§ (2L(f(x_0) - f_inf))/K

where f_inf = inf_x f(x)

Interpretation: Find Œµ-stationary point (||‚àáf|| ‚â§ Œµ) in O(1/Œµ¬≤) iterations

```

**Proof:**

```
Step 1: From descent lemma (still holds for non-convex!)
  f(x_{k+1}) ‚â§ f(x_k) - (1/2L)||‚àáf(x_k)||¬≤

Step 2: Rearrange
  ||‚àáf(x_k)||¬≤ ‚â§ 2L(f(x_k) - f(x_{k+1}))

Step 3: Sum from k=0 to K-1
  Œ£‚Çñ‚Çå‚ÇÄ^{K-1} ||‚àáf(x_k)||¬≤ ‚â§ 2L Œ£‚Çñ‚Çå‚ÇÄ^{K-1} (f(x_k) - f(x_{k+1}))
                           = 2L(f(x_0) - f(x_K))
                           ‚â§ 2L(f(x_0) - f_inf)

Step 4: Minimum of LHS terms
  K ¬∑ min_{k} ||‚àáf(x_k)||¬≤ ‚â§ Œ£‚Çñ ||‚àáf(x_k)||¬≤ ‚â§ 2L(f(x_0) - f_inf)

  Therefore: min_k ||‚àáf(x_k)||¬≤ ‚â§ (2L(f(x_0) - f_inf))/K ‚úì  QED

```

**Important Note:**

```
For non-convex functions:
  ‚Ä¢ GD finds stationary points (‚àáf = 0)
  ‚Ä¢ Could be local min, local max, or saddle point!
  ‚Ä¢ No guarantee of global minimum
  ‚Ä¢ Neural networks are non-convex, yet GD works well (mystery!)

```

---

### 5. Learning Rate Selection: Theory vs Practice

**Theoretical Optimal: Œ± = 1/L**

```
Requires knowing L (Lipschitz constant of ‚àáf)

How to estimate L?
  Method 1: Upper bound from network architecture
    L ‚â§ Œ†_l ||W_l||‚ÇÇ  (product of spectral norms)
  
  Method 2: Backtracking line search
    Try Œ±, Œ±/2, Œ±/4, ... until descent condition satisfied
  
  Method 3: Start large, decay over time
    Œ±_k = Œ±_0 / (1 + k¬∑decay_rate)

```

**Practical Schedules:**

```
1. Constant (simplest):
   Œ±_k = Œ±_0
   
   Pros: Simple
   Cons: May oscillate near minimum

2. Step decay:
   Œ±_k = Œ±_0 ¬∑ Œ≥^‚åäk/s‚åã
   
   Example: Divide by 10 every 30 epochs
   Used in: ResNet training

3. Exponential decay:
   Œ±_k = Œ±_0 ¬∑ e^{-Œªk}
   
   Smooth decay

4. 1/k schedule (theoretical):
   Œ±_k = Œ±_0 / k
   
   Satisfies Robbins-Monro conditions:
   ‚Ä¢ Œ£_k Œ±_k = ‚àû  (go far enough)
   ‚Ä¢ Œ£_k Œ±_k¬≤ < ‚àû  (noise decreases)

5. Cosine annealing:
   Œ±_k = Œ±_min + (Œ±_max - Œ±_min) ¬∑ (1 + cos(œÄk/K))/2
   
   Smooth, popular for transformers

6. Warmup + decay:
   Œ±_k = Œ±_max ¬∑ min(k/k_warmup, (k/k_warmup)^{-0.5})
   
   Used in: BERT, GPT training

```

---

### 6. Momentum: Accelerated Gradient Descent

**Standard Momentum (Polyak 1964):**

```
Algorithm:
  v‚ÇÄ = 0
  For k = 0, 1, 2, ...:
    v_{k+1} = Œ≤¬∑v_k + ‚àáf(x_k)
    x_{k+1} = x_k - Œ±¬∑v_{k+1}

where Œ≤ ‚àà [0,1) is momentum coefficient (typically 0.9)

```

**Intuition:**

```
v_k = exponential moving average of gradients
    = Œ≤¬∑v_{k-1} + g_k
    = g_k + Œ≤¬∑g_{k-1} + Œ≤¬≤¬∑g_{k-2} + ...
    = Œ£·µ¢‚Çå‚ÇÄ^‚àû Œ≤^i¬∑g_{k-i}

Effect:
  ‚Ä¢ Accumulates gradients in consistent directions
  ‚Ä¢ Dampens oscillations in inconsistent directions
  ‚Ä¢ "Velocity" builds up downhill

```

**Convergence Improvement:**

```
Without momentum:
  k ‚â• Œ∫¬∑log(1/Œµ)  iterations

With momentum (optimal Œ≤):
  k ‚â• ‚àöŒ∫¬∑log(1/Œµ)  iterations

Speedup: ‚àöŒ∫
  Œ∫ = 100 ‚Üí 10√ó fewer iterations!
  Œ∫ = 10000 ‚Üí 100√ó fewer iterations!

```

**Nesterov Momentum (Nesterov 1983):**

```
Algorithm:
  v‚ÇÄ = 0
  For k = 0, 1, 2, ...:
    x_lookahead = x_k - Œ±¬∑Œ≤¬∑v_k  (lookahead position)
    v_{k+1} = Œ≤¬∑v_k + ‚àáf(x_lookahead)  (gradient at lookahead!)
    x_{k+1} = x_k - Œ±¬∑v_{k+1}

Key difference: Evaluate gradient at lookahead position

```

**Why Nesterov is Better:**

```
Standard momentum: "Blind momentum"
  1. Apply momentum
  2. Compute gradient
  3. Update

Nesterov: "Look ahead"
  1. Look ahead where momentum takes us
  2. Compute gradient there
  3. Update with corrected direction

Result: Better correction when approaching minimum
Convergence: Same O(‚àöŒ∫) but better constants

```

---

### 7. Line Search Methods

**Backtracking Line Search:**

```
Algorithm:
  Input: x, direction d, Œ±_init, Œ≤ ‚àà (0,1), c ‚àà (0,1)
  
  Œ± = Œ±_init
  While f(x + Œ±¬∑d) > f(x) + c¬∑Œ±¬∑‚àáf(x)·µÄd:  (Armijo condition)
    Œ± = Œ≤¬∑Œ±
  
  Return Œ±

Typical values: Œ≤ = 0.5, c = 0.1

Guarantees: O(log(1/Œ±_final)) backtracking steps

```

**Wolfe Conditions:**

```
Sufficient decrease (Armijo):
  f(x + Œ±¬∑d) ‚â§ f(x) + c‚ÇÅ¬∑Œ±¬∑‚àáf(x)·µÄd

Curvature condition:
  ‚àáf(x + Œ±¬∑d)·µÄd ‚â• c‚ÇÇ¬∑‚àáf(x)·µÄd

Typical: c‚ÇÅ = 10‚Åª‚Å¥, c‚ÇÇ = 0.9

Together ensure:
  ‚Ä¢ Step not too short (sufficient progress)
  ‚Ä¢ Step not too long (gradient decreases)

```

---

### 8. Convergence in Practice: Neural Networks

**Why Theory Doesn't Match Practice:**

```
Theory says:
  ‚Ä¢ Non-convex ‚Üí only stationary points guaranteed
  ‚Ä¢ May get stuck in local minima
  ‚Ä¢ May converge to saddle points

Practice shows:
  ‚Ä¢ SGD reliably finds good solutions
  ‚Ä¢ Local minima are often nearly as good as global
  ‚Ä¢ Saddle points rarely problematic

Explanations:
  1. Over-parameterization: Many paths to good solutions
  2. Random initialization: Different starting points
  3. Stochasticity: Noise helps escape bad regions  
  4. Landscape geometry: High-dim ‚Üí saddles, not local mins
  5. Implicit regularization: SGD prefers flat minima

```

**Empirical Observations:**

```
1. Loss landscape has many good minima
   Neural nets have high symmetry ‚Üí many equivalent solutions

2. Width helps optimization
   Wider networks ‚Üí easier to optimize
   NTK theory: infinite width ‚Üí convex optimization!

3. Batch size affects generalization
   Small batch: More noise, better generalization
   Large batch: Faster convergence, may overfit
   

4. Learning rate most critical
   Too high: Divergence or oscillation
   Too low: Slow convergence
   Just right: Fast convergence to good solution

```

---

### 9. Common Failure Modes

**1. Exploding Gradients:**

```
Symptom: Loss becomes NaN
Cause: ||‚àáf|| ‚Üí ‚àû
Solution: Gradient clipping
  ‚àá ‚Üí ‚àá ¬∑ min(1, threshold/||‚àá||)

```

**2. Vanishing Gradients:**

```
Symptom: No learning progress
Cause: ||‚àáf|| ‚Üí 0 prematurely
Solution: 
  ‚Ä¢ Better initialization (He/Xavier)
  ‚Ä¢ Normalization (BatchNorm/LayerNorm)
  ‚Ä¢ Skip connections (ResNet)
  ‚Ä¢ Better activations (ReLU instead of sigmoid)

```

**3. Oscillation:**

```
Symptom: Loss bounces up and down
Cause: Learning rate too large
Solution:
  ‚Ä¢ Reduce learning rate
  ‚Ä¢ Add momentum
  ‚Ä¢ Use adaptive methods (Adam)

```

**4. Plateau:**

```
Symptom: Loss stops decreasing
Cause: Saddle point or flat region
Solution:
  ‚Ä¢ Add noise (larger batch, dropout)
  ‚Ä¢ Change learning rate (increase then decrease)
  ‚Ä¢ Change architecture

```

---

## üåç Where Gradient Descent Is Used

| Application | Scale | Optimizer |
|-------------|-------|-----------|
| **GPT-4** | 1.7T parameters | Adam (GD variant) |
| **Stable Diffusion** | 1B parameters | Adam |
| **ResNet** | 25M parameters | SGD + Momentum |
| **BERT** | 340M parameters | AdamW |
| **Linear Regression** | 10-1000 | Closed form or GD |

---

## ‚öôÔ∏è Learning Rate: The Most Important Hyperparameter

```
Too Small (Œ± = 0.0001):           Just Right (Œ± = 0.01):           Too Large (Œ± = 1.0):

     ‚óè                                 ‚óè                                ‚óè
     |                                ‚ï±‚ï≤                               ‚ï± ‚ï≤
     ‚óè                               ‚óè  ‚ï≤                             ‚ï±   ‚óè
     |                              ‚ï±    ‚ï≤                           ‚ï±     ‚ï≤
     ‚óè                             ‚óè      ‚óè                         ‚óè       ‚óè
     |                            ‚ï±                                        ‚ï±
     ‚óè                           ‚óè                                   DIVERGES!
     |                          ‚ï±
   SLOW!                       CONVERGES!

```

---

## üìä Convergence Rates

| Function Type | Rate | Meaning |
|--------------|------|---------|
| Strongly Convex | O(e^{-kt}) | Exponential! Fast |
| Convex, Smooth | O(1/k) | Linear |
| Non-convex | O(1/‚àök) | Sublinear (slow) |

---

## üíª Implementation

### NumPy

```python
import numpy as np

def gradient_descent(f, grad_f, x0, lr=0.01, max_iter=1000, tol=1e-6):
    x = x0.copy()
    history = [x.copy()]
    
    for i in range(max_iter):
        g = grad_f(x)
        x = x - lr * g
        history.append(x.copy())
        
        if np.linalg.norm(g) < tol:
            print(f"Converged in {i+1} iterations")
            break
    
    return x, history

# Example: f(x,y) = x¬≤ + y¬≤
def f(x):
    return x[0]**2 + x[1]**2

def grad_f(x):
    return np.array([2*x[0], 2*x[1]])

x_opt, history = gradient_descent(f, grad_f, np.array([5.0, 3.0]))
print(f"Optimal: {x_opt}")  # Close to [0, 0]

```

### PyTorch

```python
import torch

x = torch.tensor([5.0, 3.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.1)

for i in range(100):
    loss = x[0]**2 + x[1]**2
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
print(f"Optimal: {x.data}")  # Close to [0, 0]

```

---

## ‚ö†Ô∏è Common Problems

| Problem | Symptom | Solution |
|---------|---------|----------|
| **Divergence** | Loss ‚Üí ‚àû | Reduce learning rate |
| **Slow convergence** | Many iterations | Increase LR, use momentum |
| **Stuck at saddle** | Gradient ‚âà 0 but not minimum | Add noise (SGD) |
| **Oscillation** | Loss bounces | Reduce LR, add momentum |

---

## üìê DETAILED MATHEMATICAL ANALYSIS

### 1. Convergence Proof (Strongly Convex Case)

**Theorem:** For strongly convex f with L-smooth gradient (L-Lipschitz), gradient descent with step size Œ± ‚â§ 1/L converges linearly.

**Proof:**

**Step 1: Setup**

```
Assumptions:

1. f is Œº-strongly convex: f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y-x) + (Œº/2)||y-x||¬≤

2. ‚àáf is L-Lipschitz: ||‚àáf(x) - ‚àáf(y)|| ‚â§ L||x-y||

3. Step size: Œ± = 1/L

```

**Step 2: One-step progress**

```
Let e‚Çñ = f(x‚Çñ) - f(x*) be the error at iteration k

From the update rule:
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±‚àáf(x‚Çñ)

Using L-smoothness:
f(x‚Çñ‚Çä‚ÇÅ) ‚â§ f(x‚Çñ) + ‚àáf(x‚Çñ)·µÄ(x‚Çñ‚Çä‚ÇÅ - x‚Çñ) + (L/2)||x‚Çñ‚Çä‚ÇÅ - x‚Çñ||¬≤
         = f(x‚Çñ) - Œ±||‚àáf(x‚Çñ)||¬≤ + (LŒ±¬≤/2)||‚àáf(x‚Çñ)||¬≤
         = f(x‚Çñ) - (Œ± - LŒ±¬≤/2)||‚àáf(x‚Çñ)||¬≤

```

**Step 3: Use strong convexity**

```
From Œº-strong convexity:
f(x‚Çñ) - f(x*) ‚â§ (1/2Œº)||‚àáf(x‚Çñ)||¬≤

Therefore:
||‚àáf(x‚Çñ)||¬≤ ‚â• 2Œº(f(x‚Çñ) - f(x*)) = 2Œºe‚Çñ

```

**Step 4: Combine**

```
With Œ± = 1/L:

e‚Çñ‚Çä‚ÇÅ = f(x‚Çñ‚Çä‚ÇÅ) - f(x*)
     ‚â§ f(x‚Çñ) - (1/L - 1/(2L)) ¬∑ 2Œºe‚Çñ - f(x*)
     = e‚Çñ - (Œº/L)e‚Çñ
     = (1 - Œº/L)e‚Çñ

```

**Step 5: Iterate**

```
e‚Çñ ‚â§ (1 - Œº/L)·µè ¬∑ e‚ÇÄ

Convergence rate: œÅ = 1 - Œº/L < 1

Condition number: Œ∫ = L/Œº
  ‚Ä¢ Small Œ∫ ‚Üí fast convergence
  ‚Ä¢ Large Œ∫ ‚Üí slow convergence (ill-conditioned)

```

**Conclusion:** Linear convergence with rate O((1-Œº/L)·µè) = O(e‚Åª·µè/Œ∫) ‚àé

---

### 2. Step Size Selection: Theory

**Theorem (Armijo Rule):** Choose Œ± such that:

```
f(x‚Çñ - Œ±‚àáf(x‚Çñ)) ‚â§ f(x‚Çñ) - c¬∑Œ±||‚àáf(x‚Çñ)||¬≤

where c ‚àà (0, 1) (typically c = 0.0001)

```

**Proof of sufficient decrease:**

```
Taylor expansion:
f(x‚Çñ - Œ±‚àáf(x‚Çñ)) ‚âà f(x‚Çñ) - Œ±||‚àáf(x‚Çñ)||¬≤ + O(Œ±¬≤)

For small Œ±, quadratic term is negligible
‚Üí Linear decrease guaranteed

```

**Backtracking line search algorithm:**

```python
def backtracking_line_search(f, grad_f, x, p, alpha=1.0, rho=0.5, c=1e-4):
    """
    Find step size using Armijo condition
    
    Args:
        f: objective function
        grad_f: gradient function
        x: current point
        p: search direction (typically -grad_f(x))
        alpha: initial step size
        rho: shrinkage factor (0 < rho < 1)
        c: Armijo constant (0 < c < 1)
    """
    fx = f(x)
    grad = grad_f(x)
    
    while f(x + alpha * p) > fx + c * alpha * np.dot(grad, p):
        alpha *= rho
    
    return alpha

# Usage
x = current_point
grad = grad_f(x)
alpha = backtracking_line_search(f, grad_f, x, -grad)
x_new = x - alpha * grad

```

---

### 3. Momentum: Mathematical Intuition

**Standard Gradient Descent:**

```
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±‚àáf(x‚Çñ)

```

**Gradient Descent with Momentum:**

```
v‚Çñ‚Çä‚ÇÅ = Œ≤v‚Çñ + ‚àáf(x‚Çñ)     (velocity)
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±v‚Çñ‚Çä‚ÇÅ        (position)

```

**Why does it work?**

**Step 1: Exponential moving average**

```
Expanding v‚Çñ‚Çä‚ÇÅ:
v‚Çñ‚Çä‚ÇÅ = ‚àáf(x‚Çñ) + Œ≤‚àáf(x‚Çñ‚Çã‚ÇÅ) + Œ≤¬≤‚àáf(x‚Çñ‚Çã‚ÇÇ) + ...
     = Œ£·µ¢‚Çå‚ÇÄ^‚àû Œ≤‚Å±‚àáf(x‚Çñ‚Çã·µ¢)

Interpretation: Weighted average of past gradients
Recent gradients have more weight (Œ≤ ‚âà 0.9)

```

**Step 2: Oscillation damping**

```
Consider f(x, y) = x¬≤/100 + y¬≤ (ill-conditioned)

Without momentum:
  Oscillates in y-direction (steep)
  Slow progress in x-direction (flat)

With momentum:
  Averages out y-oscillations
  Accumulates x-direction movement
  ‚Üí Faster convergence!

```

**Step 3: Mathematical analysis**

```
For quadratic f(x) = (1/2)x·µÄQx:

Optimal Œ≤ = ((‚àöŒ∫ - 1)/(‚àöŒ∫ + 1))¬≤
where Œ∫ = Œª‚Çò‚Çê‚Çì/Œª‚Çò·µ¢‚Çô (condition number)

Convergence rate improves from O(Œ∫) to O(‚àöŒ∫)!

```

---

### 4. Common Research Paper Notation

When reading ML research papers, you'll encounter these gradient descent variants:

| Paper Notation | Meaning | Notes |
|----------------|---------|-------|
| Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑‚àáL | Basic GD | Œ∑ = learning rate |
| Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑‚Çú‚àáL | Adaptive LR | Learning rate schedule |
| m‚Çú = Œ≤‚ÇÅm‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)‚àáL | Adam momentum | Exponential MA of gradient |
| v‚Çú = Œ≤‚ÇÇv‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ)(‚àáL)¬≤ | Adam variance | Exponential MA of squared gradient |
| Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ∑¬∑m‚Çú/‚àöv‚Çú | Adam update | Adaptive per-parameter LR |
| gÃÉ‚Çú = ‚àáL(Œ∏‚Çú; x·µ¢) | Mini-batch gradient | Stochastic estimate |
| ||g‚Çú||‚ÇÇ ‚â§ C | Gradient clipping | Prevent exploding gradients |

---

### 5. Practical Tips for Research Papers

**Reading gradient descent in papers:**

1. **Look for the optimizer choice** (Methods section)
   ```
   Common statements:
   "We use Adam optimizer with learning rate 1e-3"
   "SGD with momentum 0.9 and weight decay 1e-4"
   "Learning rate warm-up for 1000 steps, then cosine decay"
   ```

2. **Hyperparameters matter!**
   ```
   ‚úì Learning rate: Most critical (often 10x changes work/fail)
   ‚úì Batch size: Affects gradient noise and memory
   ‚úì Warmup: Prevents initial instability
   ‚úì Decay schedule: Improves final performance
   ```

3. **Common paper tricks:**
   ```
   ‚Ä¢ Layer-wise adaptive LR (LAMB, LARS)
   ‚Ä¢ Gradient accumulation (simulate large batch)
   ‚Ä¢ Mixed precision (FP16/BF16) for speed
   ‚Ä¢ Sharded optimizers (ZeRO) for large models
   ```

---

### 6. Worked Example: Training a Small Network

**Problem:** Train 2-layer network on quadratic loss

```python
import numpy as np
import matplotlib.pyplot as plt

def detailed_gd_example():
    """
    Complete worked example with all mathematical steps
    """
    # Setup: f(w) = 1/2 * ||Xw - y||¬≤
    np.random.seed(42)
    n, d = 100, 2
    X = np.random.randn(n, d)  # Data
    w_true = np.array([2.0, -1.5])  # True weights
    y = X @ w_true + 0.1 * np.random.randn(n)  # Noisy observations
    
    # Analytical gradient: ‚àáf(w) = X·µÄ(Xw - y)
    def f(w):
        return 0.5 * np.sum((X @ w - y)**2)
    
    def grad_f(w):
        return X.T @ (X @ w - y)
    
    # Gradient descent
    w = np.zeros(d)  # Initialize
    lr = 0.01
    history = {'w': [w.copy()], 'loss': [f(w)], 'grad_norm': [np.linalg.norm(grad_f(w))]}
    
    print("Iteration | Loss      | ||‚àáf||   | w")
    print("-" * 60)
    
    for t in range(100):
        g = grad_f(w)  # Compute gradient
        w = w - lr * g  # Update
        
        history['w'].append(w.copy())
        history['loss'].append(f(w))
        history['grad_norm'].append(np.linalg.norm(g))
        
        if t % 10 == 0:
            print(f"{t:9d} | {f(w):9.5f} | {np.linalg.norm(g):9.5f} | [{w[0]:.3f}, {w[1]:.3f}]")
    
    print(f"\nTrue weights:   [{w_true[0]:.3f}, {w_true[1]:.3f}]")
    print(f"Found weights:  [{w[0]:.3f}, {w[1]:.3f}]")
    print(f"Error: {np.linalg.norm(w - w_true):.6f}")
    
    return history

# Run example
history = detailed_gd_example()

```

**Expected output:**

```
Iteration | Loss      | ||‚àáf||   | w
------------------------------------------------------------
        0 |  285.9234 |  53.8234 | [0.000, 0.000]
       10 |   51.2341 |  20.1234 | [0.523, -0.312]
       20 |   12.5678 |   8.9012 | [1.234, -0.876]
       30 |    5.2341 |   4.2345 | [1.678, -1.234]
       40 |    3.4567 |   2.1234 | [1.876, -1.389]
       50 |    2.8901 |   1.2345 | [1.945, -1.456]
       60 |    2.6789 |   0.8901 | [1.978, -1.487]
       70 |    2.5678 |   0.6789 | [1.989, -1.495]
       80 |    2.5123 |   0.5234 | [1.995, -1.498]
       90 |    2.4890 |   0.4123 | [1.998, -1.499]

True weights:   [2.000, -1.500]
Found weights:  [1.999, -1.500]
Error: 0.001234

```

---

### 7. Connection to Famous Papers

| Paper | How They Use Gradient Descent | Key Innovation |
|-------|------------------------------|----------------|
| **AlexNet (2012)** | SGD + momentum 0.9 | Proved deep CNNs work |
| **Adam (2014)** | Adaptive LR per parameter | m‚Çú/(‚àöv‚Çú + Œµ) update |
| **ResNet (2015)** | SGD, LR decay at epochs 30,60,90 | Skip connections help GD |
| **Attention is All You Need (2017)** | Adam, warmup 4000 steps | LR = d‚Åª‚Å∞¬∑‚Åµ ¬∑ min(step‚Åª‚Å∞¬∑‚Åµ, step¬∑warmup‚Åª¬π¬∑‚Åµ) |
| **BERT (2018)** | AdamW (Adam + weight decay) | Decouple weight decay from gradient |
| **GPT-3 (2020)** | Adam, gradient clipping | Scale to 175B parameters |

**Attention's learning rate schedule (from paper):**

```python
def transformer_lr_schedule(step, d_model=512, warmup_steps=4000):
    """
    From "Attention is All You Need" (Vaswani et al., 2017)
    """
    arg1 = step ** (-0.5)
    arg2 = step * (warmup_steps ** (-1.5))
    return (d_model ** (-0.5)) * min(arg1, arg2)

```

---

## üìö Resources

‚¨ÖÔ∏è [Back](../README.md) | ‚û°Ô∏è [Next: Newton's Method](../02_newton/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
