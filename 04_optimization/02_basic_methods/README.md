<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Basic%20Optimization%20Methods&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“‚ Subtopics

| Folder | Topic | Order | Used In |
|--------|-------|-------|---------|
| [01_gradient_descent/](./01_gradient_descent/) | Gradient Descent | First-order | All DL |
| [02_newton/](./02_newton/) | Newton's Method | Second-order | Fast optimization |

---

## ğŸ¯ First-Order vs Second-Order

```
+---------------------------------------------------------+
|                                                         |
|   FIRST-ORDER (uses gradient)                           |
|   -------------------------                             |
|   x_{k+1} = x_k - Î±âˆ‡f(x_k)                              |
|                                                         |
|   Pros: Cheap, scalable to billions of params           |
|   Cons: Can be slow, needs LR tuning                    |
|   Used: Neural networks (SGD, Adam, etc.)               |
|                                                         |
+---------------------------------------------------------+
|                                                         |
|   SECOND-ORDER (uses Hessian)                           |
|   --------------------------                            |
|   x_{k+1} = x_k - Hâ»Â¹âˆ‡f(x_k)                            |
|                                                         |
|   Pros: Very fast convergence                           |
|   Cons: O(nÂ³) per step, memory O(nÂ²)                    |
|   Used: Small problems, L-BFGS                          |
|                                                         |
+---------------------------------------------------------+
```

---

# Part 1: Gradient Descent

## ğŸ¯ The Core Idea

```
+---------------------------------------------------------+
|                                                         |
|   Want to minimize f(x)?                                |
|                                                         |
|   1. Compute gradient âˆ‡f(x)                            |
|      (Direction of steepest ASCENT)                    |
|                                                         |
|   2. Move in OPPOSITE direction                        |
|      x_{new} = x_{old} - Î±âˆ‡f(x_{old})                  |
|                                                         |
|   3. Repeat until convergence                          |
|                                                         |
|   That's it! Simple but powerful.                      |
|                                                         |
+---------------------------------------------------------+
```

---

## ğŸ“ The Algorithm

```python

# Gradient Descent in 5 lines
x = initial_guess
for i in range(max_iterations):
    gradient = compute_gradient(f, x)
    x = x - learning_rate * gradient
    if converged(gradient):
        break
```

---

## ğŸ¯ Visual Understanding

```
       Loss Surface (Mountain)
       
            Start here
               â—â•²
              â•±  â•²
             â•±    â•²
            â•±      â—  Step 1
           â•±        â•²
          â•±          â•²
         â•±            â—  Step 2
        â•±              â•²
       â•±                â—  Step 3
      â•±                  â•²
     â•±____________________â—  Minimum!
     
   Each step: Move opposite to gradient (downhill)
```

---

## ğŸ“ DETAILED MATHEMATICAL THEORY

### 1. Gradient Descent: Complete Convergence Analysis

**Algorithm:**
```
Input: f: â„â¿ â†’ â„, starting point xâ‚€, learning rate Î±
Output: x* â‰ˆ argmin f

For k = 0, 1, 2, ...:
  1. Compute gradient: g_k = âˆ‡f(x_k)
  2. Update: x_{k+1} = x_k - Î±Â·g_k
  3. Check convergence: ||g_k|| < Îµ

Return x_k
```

---

### 2. Convergence for Convex + L-Smooth Functions

**Theorem 1: Sublinear Convergence**

**Assumptions:**
1. f is convex
2. f is L-smooth: \(\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|\)
3. Step size: Î± = 1/L

**Conclusion:** \(f(x_k) - f(x^*) \leq \frac{2L\|x_0 - x^*\|^2}{k}\)

**Convergence rate:** O(1/k) iterations

**Proof:**

```
Step 1: L-smoothness implies quadratic upper bound
  For any x, y:
    f(y) â‰¤ f(x) + âˆ‡f(x)áµ€(y-x) + (L/2)||y-x||Â²

Step 2: Apply to GD update with Î± = 1/L
  Let y = x_{k+1} = x_k - (1/L)âˆ‡f(x_k)
  
  f(x_{k+1}) â‰¤ f(x_k) + âˆ‡f(x_k)áµ€(x_{k+1} - x_k) + (L/2)||x_{k+1} - x_k||Â²
             = f(x_k) - (1/L)||âˆ‡f(x_k)||Â² + (L/2)Â·(1/LÂ²)||âˆ‡f(x_k)||Â²
             = f(x_k) - (1/2L)||âˆ‡f(x_k)||Â²

Step 3: Descent lemma
  f(x_{k+1}) â‰¤ f(x_k) - (1/2L)||âˆ‡f(x_k)||Â²  ... (*)

Step 4: By convexity
  f(x_k) - f(x*) â‰¤ âˆ‡f(x_k)áµ€(x_k - x*)  (first-order condition)
                 â‰¤ ||âˆ‡f(x_k)||Â·||x_k - x*||  (Cauchy-Schwarz)

  Therefore: ||âˆ‡f(x_k)||Â² â‰¥ (f(x_k) - f(x*))Â²/||x_k - x*||Â²

Step 5: Substitute into (*)
  f(x_{k+1}) â‰¤ f(x_k) - (1/2L)Â·(f(x_k) - f(x*))Â²/||x_k - x*||Â²

Step 6: Track distance to optimum
  ||x_{k+1} - x*||Â² = ||x_k - (1/L)âˆ‡f(x_k) - x*||Â²
                    = ||x_k - x*||Â² - (2/L)âˆ‡f(x_k)áµ€(x_k - x*) + (1/LÂ²)||âˆ‡f(x_k)||Â²

  By convexity: âˆ‡f(x_k)áµ€(x_k - x*) â‰¥ f(x_k) - f(x*)
  
  Therefore:
  ||x_{k+1} - x*||Â² â‰¤ ||x_k - x*||Â² - (2/L)(f(x_k) - f(x*)) + (1/LÂ²)||âˆ‡f(x_k)||Â²

Step 7: From Step 3, we have ||âˆ‡f(x_k)||Â² â‰¤ 2L(f(x_k) - f(x_{k+1}))
  
  Substituting:
  ||x_{k+1} - x*||Â² â‰¤ ||x_k - x*||Â² - (2/L)(f(x_k) - f(x*)) + (2/L)(f(x_k) - f(x_{k+1}))
                    = ||x_k - x*||Â² - (2/L)(f(x_{k+1}) - f(x*))

Step 8: Rearrange
  f(x_{k+1}) - f(x*) â‰¤ (L/2)(||x_k - x*||Â² - ||x_{k+1} - x*||Â²)

Step 9: Sum telescoping series from 0 to k-1
  Î£áµ¢â‚Œâ‚€^{k-1} (f(x_{i+1}) - f(x*)) â‰¤ (L/2)||x_0 - x*||Â²

  Since f(x_i) is decreasing:
  kÂ·(f(x_k) - f(x*)) â‰¤ Î£áµ¢â‚Œâ‚€^{k-1} (f(x_{i+1}) - f(x*)) â‰¤ (L/2)||x_0 - x*||Â²

  Therefore: f(x_k) - f(x*) â‰¤ (L||x_0 - x*||Â²)/(2k) âœ“  QED
```

---

### 3. Strongly Convex Case: Linear Convergence

**Theorem 2: Exponential Convergence**

**Additional assumption:** f is Î¼-strongly convex: \(f(y) \geq f(x) + \nabla f(x)^T(y-x) + \frac{\mu}{2}\|y-x\|^2\)

**Conclusion with Î± = 1/L:**
\[f(x_k) - f(x^*) \leq (1 - \mu/L)^k (f(x_0) - f(x^*))\]

**Key Quantity: Condition Number**
```
Îº = L/Î¼  (condition number)

Convergence rate: Ï = 1 - 1/Îº = (Îº-1)/Îº

Examples:
  Îº = 2:   Ï = 0.5    â†’ Half distance each step
  Îº = 10:  Ï = 0.9    â†’ 10% improvement per step
  Îº = 100: Ï = 0.99   â†’ 1% improvement per step (slow!)
  Îº = âˆ:   Ï = 1      â†’ No improvement (ill-conditioned)

Number of iterations to reach Îµ-accuracy:
  k â‰¥ ÎºÂ·log(1/Îµ)
```

**Proof Sketch:**

```
Step 1: Combine descent lemma with strong convexity
  From L-smoothness:
    f(x_{k+1}) â‰¤ f(x_k) - (1/2L)||âˆ‡f(x_k)||Â²
  
  From strong convexity:
    ||âˆ‡f(x_k)||Â² â‰¥ 2Î¼(f(x_k) - f(x*))
  
  Therefore:
    f(x_{k+1}) - f(x*) â‰¤ f(x_k) - f(x*) - (Î¼/L)(f(x_k) - f(x*))
                       = (1 - Î¼/L)(f(x_k) - f(x*))

Step 2: Apply recursively
  f(x_k) - f(x*) â‰¤ (1 - Î¼/L)^k (f(x_0) - f(x*)) âœ“  QED
```

---

### 4. Non-Convex Case: Stationary Points

**Theorem 3: First-Order Stationary Point**

For non-convex f (L-smooth):

GD with Î± = 1/L satisfies:
\[\min_{0 \leq k \leq K-1} \|\nabla f(x_k)\|^2 \leq \frac{2L(f(x_0) - f_{inf})}{K}\]

where \(f_{inf} = \inf_x f(x)\)

**Interpretation:** Find Îµ-stationary point ($\|\nabla f\| \leq \epsilon$) in O(1/ÎµÂ²) iterations

---

### 5. Learning Rate Selection: Theory vs Practice

**Practical Schedules:**

```
1. Constant (simplest):
   Î±_k = Î±_0
   
   Pros: Simple
   Cons: May oscillate near minimum

2. Step decay:
   Î±_k = Î±_0 Â· Î³^âŒŠk/sâŒ‹
   
   Example: Divide by 10 every 30 epochs
   Used in: ResNet training

3. Exponential decay:
   Î±_k = Î±_0 Â· e^{-Î»k}
   
   Smooth decay

4. 1/k schedule (theoretical):
   Î±_k = Î±_0 / k
   
   Satisfies Robbins-Monro conditions:
   â€¢ Î£_k Î±_k = âˆ  (go far enough)
   â€¢ Î£_k Î±_kÂ² < âˆ  (noise decreases)

5. Cosine annealing:
   Î±_k = Î±_min + (Î±_max - Î±_min) Â· (1 + cos(Ï€k/K))/2
   
   Smooth, popular for transformers

6. Warmup + decay:
   Î±_k = Î±_max Â· min(k/k_warmup, (k/k_warmup)^{-0.5})
   
   Used in: BERT, GPT training
```

---

### 6. Momentum: Accelerated Gradient Descent

**Standard Momentum (Polyak 1964):**

```
Algorithm:
  vâ‚€ = 0
  For k = 0, 1, 2, ...:
    v_{k+1} = Î²Â·v_k + âˆ‡f(x_k)
    x_{k+1} = x_k - Î±Â·v_{k+1}

where Î² âˆˆ [0,1) is momentum coefficient (typically 0.9)
```

**Convergence Improvement:**

```
Without momentum:
  k â‰¥ ÎºÂ·log(1/Îµ)  iterations

With momentum (optimal Î²):
  k â‰¥ âˆšÎºÂ·log(1/Îµ)  iterations

Speedup: âˆšÎº
  Îº = 100 â†’ 10Ã— fewer iterations!
  Îº = 10000 â†’ 100Ã— fewer iterations!
```

**Nesterov Momentum (Nesterov 1983):**

```
Algorithm:
  vâ‚€ = 0
  For k = 0, 1, 2, ...:
    x_lookahead = x_k - Î±Â·Î²Â·v_k  (lookahead position)
    v_{k+1} = Î²Â·v_k + âˆ‡f(x_lookahead)  (gradient at lookahead!)
    x_{k+1} = x_k - Î±Â·v_{k+1}

Key difference: Evaluate gradient at lookahead position
```

---

## ğŸ“Š Convergence Rates Summary

| Function Type | Rate | Meaning |
|--------------|------|---------|
| Strongly Convex | O(e^{-kt}) | Exponential! Fast |
| Convex, Smooth | O(1/k) | Linear |
| Non-convex | O(1/âˆšk) | Sublinear (slow) |

---

## ğŸ’» Implementation

### NumPy
```python
import numpy as np

def gradient_descent(f, grad_f, x0, lr=0.01, max_iter=1000, tol=1e-6):
    x = x0.copy()
    history = [x.copy()]
    
    for i in range(max_iter):
        g = grad_f(x)
        x = x - lr * g
        history.append(x.copy())
        
        if np.linalg.norm(g) < tol:
            print(f"Converged in {i+1} iterations")
            break
    
    return x, history

# Example: f(x,y) = xÂ² + yÂ²
def f(x):
    return x[0]**2 + x[1]**2

def grad_f(x):
    return np.array([2*x[0], 2*x[1]])

x_opt, history = gradient_descent(f, grad_f, np.array([5.0, 3.0]))
print(f"Optimal: {x_opt}")  # Close to [0, 0]
```

### PyTorch
```python
import torch

x = torch.tensor([5.0, 3.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.1)

for i in range(100):
    loss = x[0]**2 + x[1]**2
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
print(f"Optimal: {x.data}")  # Close to [0, 0]
```

---

# Part 2: Newton's Method

## ğŸ¯ The Core Idea

```
+---------------------------------------------------------+
|                                                         |
|   Gradient Descent: Linear approximation               |
|   f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Î”x                          |
|                                                         |
|   Newton's Method: Quadratic approximation             |
|   f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Î”x + Â½Î”xáµ€HÎ”x               |
|                                                         |
|   Why better? Captures curvature!                       |
|                                                         |
+---------------------------------------------------------+
```

---

## ğŸ“ The Algorithm

```
Newton Step:

+---------------------------------------------------------+
|                                                         |
|   x_{k+1} = x_k - H(x_k)â»Â¹ âˆ‡f(x_k)                     |
|                                                         |
|   where:                                                |
|   â€¢ H = Hessian (matrix of second derivatives)          |
|   â€¢ âˆ‡f = gradient (vector of first derivatives)        |
|                                                         |
|   No learning rate needed!                              |
|   (The Hessian provides natural step size)              |
|                                                         |
+---------------------------------------------------------+
```

---

## ğŸ¯ Visual: Why It's Faster

```
Gradient Descent:              Newton's Method:
(Linear approx)                (Quadratic approx)

    â•²                               â•²
     â•²   function                    â•²   function
      â•²_____â€¢_____                    â•²__â€¢__â•±
           â•±â•²                            |
          â•±  â•² tangent line              | perfect step!
         â•±    â•²                          â†“
                                         â€¢ minimum

Takes many small steps           Takes one big accurate step
```

---

## ğŸ“ Mathematical Foundations

### Newton Update Rule

```
Standard Newton Step:
x_{k+1} = x_k - H(x_k)â»Â¹âˆ‡f(x_k)

Where:
â€¢ H(x) = âˆ‡Â²f(x) is the Hessian matrix
â€¢ âˆ‡f(x) is the gradient
â€¢ Hâ»Â¹âˆ‡f is the Newton direction
```

### Derivation from Taylor Expansion

```
Second-order Taylor approximation:
f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Î”x + Â½Î”xáµ€ H Î”x

Setting derivative to zero:
âˆ‡f(x) + HÂ·Î”x = 0

Solving for optimal step:
Î”x* = -Hâ»Â¹âˆ‡f(x)
```

### Newton Decrement

```
Î»Â² = âˆ‡f(x)áµ€ H(x)â»Â¹ âˆ‡f(x)

Interpretation:
â€¢ Î»Â² â‰ˆ f(x) - f(x*)  (approximate suboptimality)
â€¢ Stopping criterion: Î»Â² < Îµ
```

### Convergence Analysis

```
Near optimum (local convergence):
â€–x_{k+1} - x*â€– â‰¤ C Â· â€–x_k - x*â€–Â²

Quadratic convergence means:
â€¢ Error squares each iteration
â€¢ 10â»Â² â†’ 10â»â´ â†’ 10â»â¸ â†’ 10â»Â¹â¶
â€¢ Very fast once "close enough"

Global convergence (with damping):
x_{k+1} = x_k - Î±_k Â· Hâ»Â¹âˆ‡f

where Î±_k found by line search
```

---

## ğŸ“Š Convergence Comparison

| Method | Convergence | Per-Step Cost | Memory |
|--------|-------------|---------------|--------|
| **Gradient Descent** | O(1/k) | O(n) | O(n) |
| **Newton** | O(log log(1/Îµ)) | O(nÂ³) | O(nÂ²) |
| **L-BFGS** | Superlinear | O(n) | O(mn) |

```
Newton converges QUADRATICALLY:

If error at step k is Îµ,
error at step k+1 is ÎµÂ²!

Example:
Step 1: error = 0.1
Step 2: error = 0.01
Step 3: error = 0.0001
Step 4: error = 0.00000001

4 steps to machine precision!
```

---

## ğŸ’» Algorithm Implementation

```python
def newton_method(f, grad_f, hess_f, x0, tol=1e-8, max_iter=100):
    x = x0
    for k in range(max_iter):
        g = grad_f(x)        # Gradient
        H = hess_f(x)        # Hessian
        
        # Newton direction: solve HÂ·d = -g
        d = np.linalg.solve(H, -g)
        
        # Newton decrement (stopping criterion)
        lambda_sq = -g @ d
        if lambda_sq / 2 < tol:
            break
        
        # Damped Newton with backtracking
        alpha = backtracking_line_search(f, x, g, d)
        x = x + alpha * d
    
    return x

def backtracking_line_search(f, x, g, d, alpha=1.0, beta=0.5, c=0.1):
    """Armijo backtracking line search"""
    while f(x + alpha * d) > f(x) + c * alpha * (g @ d):
        alpha *= beta
    return alpha
```

---

## âš ï¸ Challenges and Solutions

```
1. Hessian computation: O(nÂ²) storage, O(nÂ³) solve
2. Non-positive definite H: Direction may not be descent
3. Far from optimum: May diverge without damping
4. Saddle points: H singular or indefinite

Solutions:
â€¢ Regularization: H + Î»I (Levenberg-Marquardt)
â€¢ Modified Newton: Use |H| eigenvalues
â€¢ Trust region: Constrain step size
â€¢ Line search: Ensure descent
```

---

## ğŸ“ Proof: Quadratic Convergence

**Theorem:** Under suitable conditions (f twice continuously differentiable, H positive definite near x*, H Lipschitz continuous), Newton's method converges quadratically:

\[\|x_{k+1} - x^*\| \leq C \|x_k - x^*\|^2\]

**Proof Sketch:**

```
Step 1: Taylor expand gradient at x*
  âˆ‡f(x_k) = âˆ‡f(x*) + H(x*)(x_k - x*) + O(||x_k - x*||Â²)
          = H(x*)(x_k - x*) + O(||x_k - x*||Â²)  (since âˆ‡f(x*) = 0)

Step 2: Newton step
  x_{k+1} = x_k - H(x_k)â»Â¹âˆ‡f(x_k)

Step 3: Error analysis
  x_{k+1} - x* = x_k - x* - H(x_k)â»Â¹âˆ‡f(x_k)
               = x_k - x* - H(x_k)â»Â¹[H(x*)(x_k - x*) + O(||x_k - x*||Â²)]
               = [I - H(x_k)â»Â¹H(x*)](x_k - x*) + O(||x_k - x*||Â²)

Step 4: Since H is continuous, H(x_k) â†’ H(x*) as x_k â†’ x*
  H(x_k)â»Â¹H(x*) â†’ I
  
  Therefore: ||x_{k+1} - x*|| = O(||x_k - x*||Â²)  âˆ
```

---

## ğŸŒ Where Newton's Method Is Used

| Application | Why Newton? | Details |
|-------------|-------------|---------|
| **L-BFGS** | Approximates Newton | Scipy default |
| **Logistic Regression** | Small scale, convex | Sklearn uses Newton |
| **Scientific Computing** | Need precision | Physics simulations |
| **Trust Region (RL)** | TRPO uses Newton | Constrained optimization |
| **Interior Point** | LP/QP solvers | Gurobi, MOSEK |

---

## ğŸ“Š When to Use What

| Scenario | Method | Why |
|----------|--------|-----|
| n < 1000 | Newton | Fast, accurate |
| n < 100000 | L-BFGS | Practical Newton |
| n > 100000 | SGD/Adam | Only option |
| Non-convex DL | Adam | Handles saddles |
| Convex ML | L-BFGS | Guaranteed optimal |

---

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Nocedal & Wright | [Springer](https://link.springer.com/book/10.1007/978-0-387-40065-5) |
| ğŸ“„ | L-BFGS Paper | [ACM](https://dl.acm.org/doi/10.1145/279232.279236) |
| ğŸ¥ | Newton's Method | [YouTube](https://www.youtube.com/watch?v=sDv4f4s2SB8) |
| ğŸ¥ | GD Visualization | [YouTube](https://www.youtube.com/watch?v=IHZwWFHWa-w) |
| ğŸ‡¨ğŸ‡³ | çŸ¥ä¹ æ¢¯åº¦ä¸‹é™ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25202034) |
| ğŸ‡¨ğŸ‡³ | çŸ¥ä¹ ç‰›é¡¿æ³• | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/37588590) |

---

## ğŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept for ML systems |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

---

â¬…ï¸ [Back: Foundations](../01_foundations/) | â¡ï¸ [Next: Advanced Methods](../03_advanced_methods/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
