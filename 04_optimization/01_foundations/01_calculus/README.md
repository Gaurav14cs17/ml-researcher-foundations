<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Calculus%20for%20Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ðŸŽ¯ The Big Picture

```
Function f(x)
     |
     v
+-----------------------------------------------------+
|                                                     |
|   First Derivative        Second Derivative         |
|   âˆ‡f (Gradient)           H (Hessian)              |
|                                                     |
|   "Which direction        "How curved is           |
|    to move?"              the surface?"            |
|                                                     |
|        |                        |                   |
|        v                        v                   |
|   Gradient Descent         Newton's Method          |
|   (first-order)            (second-order)           |
|                                                     |
+-----------------------------------------------------+

```

---

# Part 1: Gradients & Partial Derivatives

## ðŸ“– What is a Gradient?

The gradient is a vector of all partial derivatives. It points in the direction of **steepest ascent**.

```
+---------------------------------------------------------+
|                                                         |
|   For f(xâ‚, xâ‚‚, ..., xâ‚™):                              |
|                                                         |
|                + âˆ‚f/âˆ‚xâ‚ +                               |
|                | âˆ‚f/âˆ‚xâ‚‚ |                               |
|   âˆ‡f(x) =      |   â‹®    |                               |
|                | âˆ‚f/âˆ‚xâ‚™ |                               |
|                +        +                               |
|                                                         |
|   Size: n Ã— 1 vector                                    |
|                                                         |
+---------------------------------------------------------+

```

---

## ðŸŽ¯ Visual Intuition

```
         Mountain Surface (Loss Landscape)
         
              â†— âˆ‡f (gradient points UP)
             /
            â€¢  You are here
           /|\
          / | \
         /  |  \
        ----+----  Valley (minimum)
        
   To minimize: Go OPPOSITE to gradient!
   
   x_new = x_old - Î±âˆ‡f(x_old)
           -----------------
           Gradient Descent!

```

---

## ðŸ“ Mathematical Definition and Properties

### Formal Definition

For a scalar-valued function \( f: \mathbb{R}^n \to \mathbb{R} \), the gradient is defined as:

\[
\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
\]

### Directional Derivative

The directional derivative of \( f \) at \( \mathbf{x} \) in direction \( \mathbf{d} \) (unit vector):

\[
D_\mathbf{d} f(\mathbf{x}) = \nabla f(\mathbf{x})^\top \mathbf{d} = \|\nabla f(\mathbf{x})\| \cos\theta
\]

where \( \theta \) is the angle between \( \nabla f \) and \( \mathbf{d} \).

**Key Insight:** Maximum increase occurs when \( \theta = 0 \), i.e., moving in the direction of the gradient.

### Gradient Properties

| Property | Formula | Meaning |
|----------|---------|---------|
| **Linearity** | âˆ‡(af + bg) = aâˆ‡f + bâˆ‡g | Gradients add |
| **Product Rule** | âˆ‡(fg) = fâˆ‡g + gâˆ‡f | Chain rule for products |
| **Chain Rule** | âˆ‡(fâˆ˜g) = (âˆ‡f)Â·(âˆ‡g) | Backpropagation! |
| **Zero at extrema** | âˆ‡f(x*) = 0 | How we find optima |

---

## ðŸ“ Step-by-Step Example

### Function: f(x, y) = xÂ² + 2yÂ²

**Step 1: Partial Derivatives**

```
âˆ‚f/âˆ‚x = 2x    (derivative treating y as constant)
âˆ‚f/âˆ‚y = 4y    (derivative treating x as constant)

```

**Step 2: Gradient Vector**

```
âˆ‡f(x,y) = [2x, 4y]áµ€

```

**Step 3: Evaluate at Point (3, 2)**

```
âˆ‡f(3,2) = [2(3), 4(2)]áµ€ = [6, 8]áµ€

```

**Step 4: Gradient Descent Step**

```
Î± = 0.1  (learning rate)

[x_new]   [3]         [6]   [3 - 0.6]   [2.4]
[y_new] = [2] - 0.1 Ã— [8] = [2 - 0.8] = [1.2]

```

---

## ðŸ“ Proof: Gradient Points in Direction of Steepest Ascent

**Theorem:** For a differentiable function \( f \), the gradient \( \nabla f(\mathbf{x}) \) points in the direction of steepest increase at \( \mathbf{x} \).

**Proof:**

```
Step 1: Consider the directional derivative in direction d (||d|| = 1):
  D_d f(x) = âˆ‡f(x)áµ€ Â· d = ||âˆ‡f(x)|| Â· ||d|| Â· cos(Î¸) = ||âˆ‡f(x)|| Â· cos(Î¸)

Step 2: Maximize the directional derivative
  max_d D_d f(x) = max_Î¸ ||âˆ‡f(x)|| Â· cos(Î¸)

Step 3: Since cos(Î¸) â‰¤ 1 with equality at Î¸ = 0:
  Maximum occurs when d is parallel to âˆ‡f(x)
  
Step 4: Maximum value is:
  ||âˆ‡f(x)|| Â· cos(0) = ||âˆ‡f(x)||

Therefore: âˆ‡f points in direction of steepest ascent with magnitude ||âˆ‡f||. âˆŽ

```

---

## ðŸŒ Where Gradients Are Used

| Application | How | Example |
|-------------|-----|---------|
| **Neural Networks** | Backpropagation computes âˆ‡L | Training GPT |
| **Diffusion Models** | Score âˆ‡log p(x) | Stable Diffusion |
| **Physics** | Force = -âˆ‡V (potential) | Molecular dynamics |
| **Economics** | Marginal utility = âˆ‚U/âˆ‚x | Optimization |
| **Computer Graphics** | Surface normals | Shading |

---

## ðŸ’» Code Examples

### PyTorch (Autograd)

```python
import torch

# Define parameters
x = torch.tensor([3.0, 2.0], requires_grad=True)

# Define function
def f(x):
    return x[0]**2 + 2*x[1]**2

# Compute gradient
loss = f(x)
loss.backward()

print(f"âˆ‡f = {x.grad}")  # tensor([6., 8.])

```

### NumPy (Manual)

```python
import numpy as np

def gradient_f(x, y):
    """Gradient of f(x,y) = xÂ² + 2yÂ²"""
    df_dx = 2 * x
    df_dy = 4 * y
    return np.array([df_dx, df_dy])

grad = gradient_f(3, 2)
print(f"âˆ‡f(3,2) = {grad}")  # [6, 8]

```

### JAX (Automatic)

```python
import jax
import jax.numpy as jnp

def f(x):
    return x[0]**2 + 2*x[1]**2

# Auto-compute gradient function
grad_f = jax.grad(f)

x = jnp.array([3.0, 2.0])
print(f"âˆ‡f = {grad_f(x)}")  # [6., 8.]

```

---

## âš ï¸ Common Mistakes

| Mistake | Problem | Fix |
|---------|---------|-----|
| Confusing âˆ‡f direction | Gradient is ASCENT, not descent | Use **-**âˆ‡f |
| Wrong partial derivative | Forgot to treat others as constant | Check each variable |
| Not normalizing | Gradient can be huge | Clip or normalize |
| Ignoring numerical issues | Gradient vanishing/exploding | Use techniques like BatchNorm |

---

# Part 2: Hessian Matrix

## ðŸ“– What is the Hessian?

The Hessian is a matrix of **second partial derivatives**. It tells us about the **curvature** of the function.

```
+---------------------------------------------------------+
|                                                         |
|   For f(xâ‚, xâ‚‚, ..., xâ‚™):                              |
|                                                         |
|         + âˆ‚Â²f/âˆ‚xâ‚Â²    âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  ...  âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚™ +   |
|         | âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚‚Â²    ...  âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚™ |   |
|   H =   |     â‹®           â‹®        â‹±       â‹®      |   |
|         | âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚‚  ...  âˆ‚Â²f/âˆ‚xâ‚™Â²   |   |
|         +                                          +   |
|                                                         |
|   Size: n Ã— n matrix (symmetric!)                       |
|                                                         |
+---------------------------------------------------------+

```

---

## ðŸŽ¯ Visual Intuition

```
Hessian tells us the SHAPE of the bowl:

    Positive Definite         Negative Definite        Indefinite
    (H â‰» 0, all Î» > 0)        (H â‰º 0, all Î» < 0)       (mixed Î»)
    
         â•²___â•±                    â•±â€¾â€¾â€¾â•²                  â•²__â•±â€¾â•²
          \â€¢/                     \â€¢/                      â€¢
        MINIMUM                 MAXIMUM               SADDLE POINT
        
    "Bowl up"                "Bowl down"             "Saddle"

```

---

## ðŸ“ Mathematical Definition

For \( f: \mathbb{R}^n \to \mathbb{R} \), the Hessian matrix is:

\[
H(f) = \nabla^2 f = \begin{bmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\]

**Schwarz's Theorem:** If \( f \) has continuous second derivatives, then \( H \) is symmetric:
\[
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}
\]

---

## ðŸ“ Example: f(x, y) = xÂ² + 3yÂ²

**Step 1: First Derivatives**

```
âˆ‚f/âˆ‚x = 2x
âˆ‚f/âˆ‚y = 6y

```

**Step 2: Second Derivatives**

```
âˆ‚Â²f/âˆ‚xÂ² = 2       (how âˆ‚f/âˆ‚x changes with x)
âˆ‚Â²f/âˆ‚yÂ² = 6       (how âˆ‚f/âˆ‚y changes with y)
âˆ‚Â²f/âˆ‚xâˆ‚y = 0      (how âˆ‚f/âˆ‚x changes with y)
âˆ‚Â²f/âˆ‚yâˆ‚x = 0      (how âˆ‚f/âˆ‚y changes with x)

```

**Step 3: Hessian Matrix**

```
      +     +
H =   | 2  0 |
      | 0  6 |
      +     +

```

**Step 4: Analyze Eigenvalues**

```
Î»â‚ = 2 > 0
Î»â‚‚ = 6 > 0

Both positive â†’ MINIMUM at (0,0) âœ“

```

---

## ðŸ“ Classifying Critical Points Using the Hessian

### Second Derivative Test (Proof)

**Theorem:** Let \( \mathbf{x}^* \) be a critical point where \( \nabla f(\mathbf{x}^*) = \mathbf{0} \). Then:
- If \( H(\mathbf{x}^*) \succ 0 \) (positive definite), \( \mathbf{x}^* \) is a local minimum
- If \( H(\mathbf{x}^*) \prec 0 \) (negative definite), \( \mathbf{x}^* \) is a local maximum  
- If \( H(\mathbf{x}^*) \) has both positive and negative eigenvalues, \( \mathbf{x}^* \) is a saddle point

**Proof:**

```
Step 1: Taylor expansion around x*
  f(x* + Î”x) = f(x*) + âˆ‡f(x*)áµ€Î”x + Â½Î”xáµ€H(x*)Î”x + O(||Î”x||Â³)

Step 2: Since x* is critical, âˆ‡f(x*) = 0
  f(x* + Î”x) = f(x*) + Â½Î”xáµ€H(x*)Î”x + O(||Î”x||Â³)

Step 3: For small Î”x, the quadratic term dominates
  f(x* + Î”x) - f(x*) â‰ˆ Â½Î”xáµ€H(x*)Î”x

Step 4: Positive definite H means Î”xáµ€HÎ”x > 0 for all Î”x â‰  0
  Therefore f(x* + Î”x) > f(x*) for all small perturbations
  â†’ x* is a local minimum âˆŽ

```

| Hessian Eigenvalues | Type | Example |
|---------------------|------|---------|
| All Î» > 0 | Local minimum | Bottom of bowl |
| All Î» < 0 | Local maximum | Top of hill |
| Mixed signs | Saddle point | Horse saddle |
| Some Î» = 0 | Degenerate | Needs more analysis |

---

## ðŸ”— Connection to Optimization: Taylor Expansion

```
Taylor Expansion (2nd order):

f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Î”x + Â½Î”xáµ€HÎ”x
            -----  ----------  --------
            value   linear      quadratic
                    term        term (curvature)

Newton's method minimizes this quadratic approximation!

```

### Newton's Method Derivation

```
Step 1: Approximate f with Taylor expansion
  f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Î”x + Â½Î”xáµ€HÎ”x

Step 2: Minimize the approximation over Î”x
  âˆ‚/âˆ‚Î”x [f(x) + âˆ‡f(x)áµ€Î”x + Â½Î”xáµ€HÎ”x] = 0
  âˆ‡f(x) + HÎ”x = 0

Step 3: Solve for optimal Î”x
  Î”x* = -Hâ»Â¹âˆ‡f(x)

Step 4: Newton update
  x_{k+1} = x_k - H(x_k)â»Â¹âˆ‡f(x_k)

```

---

## ðŸŒ Where Hessian Is Used

| Application | How | Why |
|-------------|-----|-----|
| **Newton's Method** | x_{k+1} = x_k - Hâ»Â¹âˆ‡f | Faster convergence |
| **Loss Landscape Analysis** | Eigenvalues of H | Sharp vs flat minima |
| **Fisher Information** | Expected Hessian | Natural gradient |
| **Laplacian** | Trace of H | Image processing |
| **Mode Connectivity** | Hessian along path | Understanding DL |

---

## ðŸ’» Computing Hessian in Code

### PyTorch

```python
import torch
from torch.autograd.functional import hessian

def f(x):
    return x[0]**2 + 3*x[1]**2

x = torch.tensor([1.0, 1.0])
H = hessian(f, x)
print(f"Hessian:\n{H}")
# [[2., 0.],
#  [0., 6.]]

```

### JAX

```python
import jax
import jax.numpy as jnp

def f(x):
    return x[0]**2 + 3*x[1]**2

hess_f = jax.hessian(f)
x = jnp.array([1.0, 1.0])
print(f"Hessian:\n{hess_f(x)}")

```

---

## âš ï¸ Why We Often Avoid Hessian

| Problem | Details | Solution |
|---------|---------|----------|
| **Storage** | O(nÂ²) memory | Too big for neural nets |
| **Computation** | O(nÂ²) to compute | Too slow |
| **Inversion** | O(nÂ³) to invert | Even slower |

**Solution: Quasi-Newton methods (BFGS, L-BFGS)** approximate H using only gradient info!

---

## ðŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ðŸ“– | Khan Academy Multivariable | [Link](https://www.khanacademy.org/math/multivariable-calculus) |
| ðŸŽ¥ | 3Blue1Brown Calculus | [YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) |
| ðŸ“– | Numerical Optimization Ch.2 | [Springer](https://link.springer.com/book/10.1007/978-0-387-40065-5) |
| ðŸŽ¥ | Hessian Visualized | [YouTube](https://www.youtube.com/watch?v=LbBcuZukCAw) |
| ðŸ‡¨ðŸ‡³ | çŸ¥ä¹Žæ¢¯åº¦è¯¦è§£ | [çŸ¥ä¹Ž](https://zhuanlan.zhihu.com/p/25202034) |
| ðŸ‡¨ðŸ‡³ | çŸ¥ä¹Ž - HessiançŸ©é˜µ | [çŸ¥ä¹Ž](https://zhuanlan.zhihu.com/p/37688632) |

---

## ðŸ”— Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept for ML systems |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

---

â¬…ï¸ [Back: Foundations](../) | âž¡ï¸ [Next: Linear Algebra](../02_linear_algebra/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
