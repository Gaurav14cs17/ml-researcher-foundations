<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Linear%20Algebra%20for%20Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“ Key Concepts for Optimization

Linear algebra provides the mathematical foundation for understanding optimization algorithms. These concepts appear in every optimization paper and algorithm.

---

## ğŸ¯ Gradients and Directional Derivatives

### Gradient Vector

```
For f: â„â¿ â†’ â„, the gradient is:

âˆ‡f(x) = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€

Properties:
â€¢ Points in direction of steepest ASCENT
â€¢ Magnitude = rate of change in that direction
â€¢ Perpendicular to level sets of f
```

### Directional Derivative

```
The rate of change of f in direction d:

D_d f(x) = âˆ‡f(x)áµ€ d / ||d||

Key insight:
â€¢ Maximum when d = âˆ‡f(x)  (steepest ascent)
â€¢ Minimum when d = -âˆ‡f(x) (steepest descent)
â€¢ Zero when d âŠ¥ âˆ‡f(x)     (level set direction)
```

---

## ğŸ“ Hessian Matrix

### Definition

```
The Hessian H is the matrix of second derivatives:

H(x) = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼] = 
       +                           +
       | âˆ‚Â²f/âˆ‚xâ‚Â² ... âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚™  |
       |    â‹®      â‹±      â‹®        |
       | âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚ ... âˆ‚Â²f/âˆ‚xâ‚™Â² |
       +                           +

Properties:
â€¢ Symmetric (if f is twice continuously differentiable)
â€¢ Size: n Ã— n for f: â„â¿ â†’ â„
â€¢ Captures curvature information
```

### Second-Order Taylor Expansion

```
f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Î”x + (1/2)Î”xáµ€H(x)Î”x

This approximation is crucial for:
â€¢ Newton's method (uses full Hessian)
â€¢ Quasi-Newton methods (approximates Hessian)
â€¢ Trust region methods (constrained quadratic)
```

---

## ğŸ“ Positive Definiteness

### Definition

```
A symmetric matrix A is:

Positive definite (PD):      xáµ€Ax > 0  for all x â‰  0
Positive semi-definite (PSD): xáµ€Ax â‰¥ 0  for all x
Negative definite:           xáµ€Ax < 0  for all x â‰  0
Indefinite:                  xáµ€Ax can be positive or negative
```

### Importance for Optimization

```
At a stationary point x* where âˆ‡f(x*) = 0:

â€¢ H(x*) â‰» 0 (positive definite)  âŸ¹ x* is a LOCAL MINIMUM
â€¢ H(x*) â‰º 0 (negative definite)  âŸ¹ x* is a LOCAL MAXIMUM
â€¢ H(x*) indefinite              âŸ¹ x* is a SADDLE POINT
```

### How to Check Positive Definiteness

```
Method 1: Eigenvalues
  A â‰» 0  âŸº  all eigenvalues Î»áµ¢ > 0

Method 2: Cholesky Decomposition
  A â‰» 0  âŸº  Cholesky decomposition A = LLáµ€ exists

Method 3: Leading Principal Minors (Sylvester's criterion)
  A â‰» 0  âŸº  all leading principal minors are positive
  
  det([aâ‚â‚]) > 0
  det([aâ‚â‚ aâ‚â‚‚; aâ‚‚â‚ aâ‚‚â‚‚]) > 0
  det([aâ‚â‚ aâ‚â‚‚ aâ‚â‚ƒ; aâ‚‚â‚ aâ‚‚â‚‚ aâ‚‚â‚ƒ; aâ‚ƒâ‚ aâ‚ƒâ‚‚ aâ‚ƒâ‚ƒ]) > 0
  ...
```

---

## ğŸ“ Eigenvalues and Eigenvectors

### Definition

```
For matrix A, scalar Î» and vector v â‰  0:

Av = Î»v

â€¢ Î» is an eigenvalue
â€¢ v is the corresponding eigenvector
```

### Properties for Symmetric Matrices

```
For symmetric A = Aáµ€:

1. All eigenvalues are real
2. Eigenvectors are orthogonal
3. A = QÎ›Qáµ€ (spectral decomposition)
   where:
   Q = [vâ‚, vâ‚‚, ..., vâ‚™]  (orthonormal eigenvectors)
   Î› = diag(Î»â‚, Î»â‚‚, ..., Î»â‚™)  (eigenvalues)
```

### Role in Optimization

```
For quadratic f(x) = (1/2)xáµ€Ax - báµ€x:

â€¢ Gradient: âˆ‡f(x) = Ax - b
â€¢ Hessian: H = A
â€¢ Eigenvalues of A determine convergence rate:

  Condition number Îº = Î»_max / Î»_min
  
  Îº â‰ˆ 1:  Well-conditioned, fast convergence
  Îº >> 1: Ill-conditioned, slow convergence
```

---

## ğŸ“ Condition Number

### Definition

```
For a matrix A, the condition number is:

Îº(A) = ||A|| Â· ||Aâ»Â¹|| = Ïƒ_max / Ïƒ_min  (for 2-norm)

For symmetric positive definite A:
Îº(A) = Î»_max / Î»_min
```

### Interpretation

```
Îº measures how sensitive Ax = b is to perturbations:

||Î”x||/||x|| â‰¤ Îº(A) Â· ||Î”b||/||b||

In optimization:
â€¢ Small Îº â†’ Fast convergence
â€¢ Large Îº â†’ Slow convergence (ill-conditioned)

GD convergence rate for quadratic:
  Error decreases by factor (Îº-1)/(Îº+1) per iteration
  
  Îº = 2:    factor = 1/3    (fast)
  Îº = 100:  factor = 0.98   (slow)
  Îº = 10000: factor = 0.9998 (very slow!)
```

### Improving Conditioning: Preconditioning

```
Instead of solving Ax = b, solve:
  Mâ»Â¹Ax = Mâ»Â¹b

If M â‰ˆ A, then Îº(Mâ»Â¹A) â‰ˆ 1 â†’ fast convergence

Common preconditioners:
â€¢ Diagonal: M = diag(A)
â€¢ Jacobi: M = diag(A)
â€¢ Incomplete Cholesky: M = LÌƒLÌƒáµ€
```

---

## ğŸ“ Matrix Norms

### Common Norms

```
Frobenius norm:
||A||_F = âˆš(Î£áµ¢â±¼ aáµ¢â±¼Â²) = âˆš(trace(Aáµ€A))

Spectral norm (operator norm):
||A||â‚‚ = Ïƒ_max(A) = max||Ax||â‚‚/||x||â‚‚

Nuclear norm (trace norm):
||A||_* = Î£áµ¢ Ïƒáµ¢(A)  (sum of singular values)
```

### Applications in ML

```
â€¢ ||W||â‚‚: Lipschitz constant of linear layer
â€¢ ||W||_F: Weight magnitude (L2 regularization)
â€¢ ||W||_*: Low-rank regularization
```

---

## ğŸ“ Singular Value Decomposition (SVD)

### Definition

```
Any m Ã— n matrix A can be decomposed as:

A = UÎ£Váµ€

where:
â€¢ U: m Ã— m orthogonal (left singular vectors)
â€¢ Î£: m Ã— n diagonal (singular values Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ 0)
â€¢ V: n Ã— n orthogonal (right singular vectors)
```

### Key Properties

```
â€¢ rank(A) = number of nonzero Ïƒáµ¢
â€¢ ||A||â‚‚ = Ïƒ_max
â€¢ ||A||_F = âˆš(Î£áµ¢ Ïƒáµ¢Â²)
â€¢ Aâº = VÎ£âºUáµ€ (pseudoinverse)
```

### Applications

```
1. Low-rank approximation:
   A_k = Î£áµ¢â‚Œâ‚áµ Ïƒáµ¢uáµ¢váµ¢áµ€  (best rank-k approximation)

2. PCA: Principal components are right singular vectors
   
3. Least squares: x = Aâºb when A is rank-deficient

4. Conditioning: Îº(A) = Ïƒ_max/Ïƒ_min
```

---

## ğŸ’» Code Examples

### Computing Hessian

```python
import torch

def compute_hessian(f, x):
    """Compute Hessian matrix"""
    return torch.autograd.functional.hessian(f, x)

# Example
def f(x):
    return x[0]**2 + 3*x[1]**2 + x[0]*x[1]

x = torch.tensor([1.0, 1.0])
H = compute_hessian(f, x)
print(f"Hessian:\n{H}")
# [[2., 1.],
#  [1., 6.]]
```

### Check Positive Definiteness

```python
import numpy as np

def is_positive_definite(H):
    """Check if matrix is positive definite"""
    eigenvalues = np.linalg.eigvalsh(H)
    return np.all(eigenvalues > 0)

def check_with_cholesky(H):
    """Check using Cholesky decomposition"""
    try:
        np.linalg.cholesky(H)
        return True
    except np.linalg.LinAlgError:
        return False

# Example
H = np.array([[2, 1], [1, 6]])
print(f"Eigenvalues: {np.linalg.eigvalsh(H)}")
print(f"Is PD: {is_positive_definite(H)}")
print(f"Condition number: {np.linalg.cond(H)}")
```

### Condition Number and Convergence

```python
import numpy as np

def analyze_conditioning(A):
    """Analyze conditioning of optimization problem"""
    eigenvalues = np.linalg.eigvalsh(A)
    lambda_max = np.max(eigenvalues)
    lambda_min = np.min(eigenvalues)
    kappa = lambda_max / lambda_min
    
    # GD convergence rate
    rate = (kappa - 1) / (kappa + 1)
    
    # Iterations to reduce error by 10^-6
    iterations = np.log(1e-6) / np.log(rate)
    
    print(f"Î»_max = {lambda_max:.4f}")
    print(f"Î»_min = {lambda_min:.4f}")
    print(f"Condition number Îº = {kappa:.4f}")
    print(f"GD convergence rate = {rate:.4f}")
    print(f"Iterations for 10â»â¶ accuracy â‰ˆ {iterations:.0f}")

# Well-conditioned example
A_good = np.array([[2, 0], [0, 3]])
print("Well-conditioned problem:")
analyze_conditioning(A_good)

print("\nIll-conditioned problem:")
A_bad = np.array([[100, 0], [0, 1]])
analyze_conditioning(A_bad)
```

---

## ğŸ“ Key Theorems

### Spectral Theorem

```
For symmetric A âˆˆ â„â¿Ë£â¿:

A = QÎ›Qáµ€ = Î£áµ¢ Î»áµ¢qáµ¢qáµ¢áµ€

where:
â€¢ Q orthogonal: Qáµ€Q = I
â€¢ Î› diagonal with real eigenvalues
â€¢ qáµ¢ orthonormal eigenvectors

Applications:
â€¢ Matrix powers: Aáµ = QÎ›áµQáµ€
â€¢ Matrix functions: f(A) = Qf(Î›)Qáµ€
â€¢ Analysis of quadratics: xáµ€Ax = Î£áµ¢ Î»áµ¢(qáµ¢áµ€x)Â²
```

### Courant-Fischer Theorem (Min-Max)

```
Î»â‚– = min_{dim(V)=k} max_{xâˆˆV, ||x||=1} xáµ€Ax
   = max_{dim(V)=n-k+1} min_{xâˆˆV, ||x||=1} xáµ€Ax

Applications:
â€¢ Understanding eigenvalue sensitivity
â€¢ Bounds on condition number
â€¢ Analysis of Rayleigh quotient
```

### Weyl's Inequality

```
For symmetric A, B:

Î»â‚–(A+B) â‰¤ Î»â±¼(A) + Î»â‚–â‚‹â±¼â‚Šâ‚(B)

Useful for:
â€¢ Perturbation analysis
â€¢ Understanding Hessian changes during optimization
```

---

## ğŸ“Š Summary Table

| Concept | Formula | Role in Optimization |
|---------|---------|---------------------|
| **Gradient** | âˆ‡f = [âˆ‚f/âˆ‚xáµ¢] | Direction of steepest descent |
| **Hessian** | H = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼] | Curvature, Newton's method |
| **Eigenvalues** | Av = Î»v | Convergence rate |
| **Condition number** | Îº = Î»_max/Î»_min | Problem difficulty |
| **Positive definite** | xáµ€Ax > 0 | Local minimum test |
| **SVD** | A = UÎ£Váµ€ | Low-rank, pseudoinverse |

---

## ğŸ“š Resources

| Type | Title | Link |
|------|-------|------|
| ğŸ“– | Linear Algebra Done Right | Axler |
| ğŸ“– | Matrix Computations | Golub & Van Loan |
| ğŸ“– | Numerical Linear Algebra | Trefethen & Bau |
| ğŸ¥ | 3Blue1Brown Linear Algebra | [YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) |
| ğŸ¥ | MIT 18.06 Linear Algebra | [MIT OCW](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/) |
| ğŸ‡¨ğŸ‡³ | çº¿æ€§ä»£æ•°åŸºç¡€ | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/25385801) |

---

â¬…ï¸ [Back: Calculus](../01_calculus/) | â¬†ï¸ [Up: Foundations](../)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
