<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Linear%20Programming&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## üìÇ Subtopics

| Folder | Topic | Applications |
|--------|-------|--------------|
| [01_simplex/](./01_simplex/) | Simplex Method | Classic LP solver |

---

## üéØ What is Linear Programming?

```
+---------------------------------------------------------+

|                                                         |
|   STANDARD FORM:                                        |
|                                                         |
|   minimize    c·µÄx         (linear objective)            |
|   subject to  Ax = b      (linear constraints)          |
|               x ‚â• 0       (non-negativity)              |
|                                                         |
|   where:                                                |
|   ‚Ä¢ x ‚àà ‚Ñù‚Åø = decision variables                        |
|   ‚Ä¢ c ‚àà ‚Ñù‚Åø = cost coefficients                         |
|   ‚Ä¢ A ‚àà ‚Ñù·µêÀ£‚Åø = constraint matrix                       |
|   ‚Ä¢ b ‚àà ‚Ñù·µê = right-hand side                           |
|                                                         |
+---------------------------------------------------------+

```

---

## üåç Real-World Applications

| Industry | Problem | LP Formulation |
|----------|---------|----------------|
| **Airlines** | Crew scheduling | Minimize cost, cover all flights |
| **Logistics** | Route optimization | Minimize distance, capacity constraints |
| **Manufacturing** | Production planning | Maximize profit, resource limits |
| **Finance** | Portfolio optimization | Minimize risk, return constraints |
| **Energy** | Power grid dispatch | Minimize cost, meet demand |
| **Telecom** | Network flow | Maximize throughput, capacity |

---

## üìê Simple Example

### Problem: Factory Production

```
A factory makes chairs (x‚ÇÅ) and tables (x‚ÇÇ)

Profit: $20 per chair, $30 per table

Constraints:
‚Ä¢ Wood: 4 units/chair, 3 units/table, 120 units available
‚Ä¢ Labor: 2 hours/chair, 4 hours/table, 80 hours available

Formulation:
maximize    20x‚ÇÅ + 30x‚ÇÇ           (profit)
subject to  4x‚ÇÅ + 3x‚ÇÇ ‚â§ 120       (wood)
            2x‚ÇÅ + 4x‚ÇÇ ‚â§ 80        (labor)
            x‚ÇÅ, x‚ÇÇ ‚â• 0

```

---

## üîó Three Methods

```
+---------------------------------------------------------+

|                                                         |
|                    LP SOLVERS                           |
|                                                         |
+-------------+-----------------+-------------------------+

|   SIMPLEX   |   DUAL SIMPLEX  |   INTERIOR POINT        |
+-------------+-----------------+-------------------------+

| Walk on     | Dual space      | Walk through            |
| vertices    | vertices        | interior                |
|             |                 |                         |
| Fast in     | Good for        | Polynomial              |
| practice    | re-optimization | worst-case              |
|             |                 |                         |
| Exponential | Exponential     | O(n¬≥¬∑‚Åµ)                 |
| worst-case  | worst-case      | guaranteed              |
+-------------+-----------------+-------------------------+

```

---

## üíª Code Examples

### Python (PuLP)

```python
from pulp import *

# Create problem
prob = LpProblem("Factory", LpMaximize)

# Decision variables
x1 = LpVariable("chairs", lowBound=0)
x2 = LpVariable("tables", lowBound=0)

# Objective
prob += 20*x1 + 30*x2, "Profit"

# Constraints
prob += 4*x1 + 3*x2 <= 120, "Wood"
prob += 2*x1 + 4*x2 <= 80, "Labor"

# Solve
prob.solve()

print(f"Chairs: {x1.value()}")
print(f"Tables: {x2.value()}")
print(f"Profit: ${value(prob.objective)}")

```

### Python (SciPy)

```python
from scipy.optimize import linprog

# minimize -c'x (negate for maximization)
c = [-20, -30]  
A = [[4, 3], [2, 4]]  
b = [120, 80]

result = linprog(c, A_ub=A, b_ub=b)
print(f"Optimal x: {result.x}")
print(f"Profit: ${-result.fun}")

```

### Gurobi (Commercial)

```python
import gurobipy as gp

m = gp.Model()
x1 = m.addVar(name="chairs")
x2 = m.addVar(name="tables")

m.setObjective(20*x1 + 30*x2, gp.GRB.MAXIMIZE)
m.addConstr(4*x1 + 3*x2 <= 120, "Wood")
m.addConstr(2*x1 + 4*x2 <= 80, "Labor")

m.optimize()

```

---

## üìä Comparison of Methods

| Method | Best For | Complexity |
|--------|----------|------------|
| **Simplex** | Most practical problems | Exp worst, fast avg |
| **Dual Simplex** | Re-optimization, warm start | Same as simplex |
| **Interior Point** | Very large problems | O(n^3.5 log(1/Œµ)) |

---

# Part 1: Simplex Method

## üéØ Visual Overview

*The Simplex method moves along edges of the feasible polytope, improving the objective at each vertex until reaching the optimal corner.*

---

## üìê Mathematical Foundations

### Standard Form LP

```
Minimize:    c·µÄx
Subject to:  Ax = b
             x ‚â• 0

Where:
‚Ä¢ x ‚àà ‚Ñù‚Åø (decision variables)
‚Ä¢ c ‚àà ‚Ñù‚Åø (cost vector)
‚Ä¢ A ‚àà ‚Ñù·µêÀ£‚Åø (constraint matrix, m < n)
‚Ä¢ b ‚àà ‚Ñù·µê (right-hand side, b ‚â• 0)

```

### Basic Feasible Solutions

```
Partition: x = [x_B, x_N]  (basic and non-basic)
           A = [B, N]       (basis matrix and non-basic)

Basic solution:
x_B = B‚Åª¬πb,  x_N = 0

Feasible if: x_B ‚â• 0

Number of basic solutions: C(n, m) = n!/(m!(n-m)!)

```

### Reduced Costs

```
Reduced cost for non-basic variable j:

cÃÑ_j = c_j - c_B·µÄ B‚Åª¬π A_j
    = c_j - œÄ·µÄ A_j

where œÄ = B‚Åª·µÄ c_B (simplex multipliers / dual variables)

Optimality condition: cÃÑ_j ‚â• 0 for all j ‚àà N

```

### Simplex Tableau

```
Initial tableau:
+------------------------------------+

|   | x_B | x_N |  b  |
|---|-----|-----|-----|
| A |  I  | B‚Åª¬πN| B‚Åª¬πb|
|---|-----|-----|-----|
| cÃÑ |  0  | cÃÑ_N | -z  |
+------------------------------------+

After row operations:
‚Ä¢ Basic variables have identity columns
‚Ä¢ cÃÑ_B = 0 (reduced costs of basic vars)
‚Ä¢ Last entry = negative of objective value

```

---

## üîÑ Simplex Algorithm

### Step-by-Step

```
1. Initialize: Find basic feasible solution (BFS)

2. Optimality test: 
   If cÃÑ_j ‚â• 0 for all j, STOP (optimal)

3. Select entering variable (pricing):
   Choose j with cÃÑ_j < 0 (most negative or Dantzig's rule)

4. Compute direction:
   d = B‚Åª¬π A_j (column of entering variable)

5. Ratio test (select leaving variable):
   Œ∏* = min { (B‚Åª¬πb)_i / d_i : d_i > 0 }
   Leaving variable = argmin of above
   
   If all d_i ‚â§ 0: Problem is UNBOUNDED

6. Pivot: Update basis, tableau, and repeat

```

### Pivot Operation

```
If variable x_s enters and x_r leaves:

New tableau element:
ƒÅ_ij = a_ij - (a_is √ó a_rj) / a_rs

Pivot element a_rs becomes 1
Pivot column becomes unit vector e_r

```

---

## üìê Proof: Simplex Optimality

**Theorem:** If all reduced costs are non-negative (cÃÑ ‚â• 0), then the current basic feasible solution is optimal.

**Proof:**

```
Let x^B be current BFS with objective z* = c_B^T x_B

For any feasible x:
z = c^T x = c_B^T x_B + c_N^T x_N

Using Ax = b:
B x_B + N x_N = b
x_B = B^{-1}b - B^{-1}N x_N

Substituting:
z = c_B^T (B^{-1}b - B^{-1}N x_N) + c_N^T x_N
  = c_B^T B^{-1}b + (c_N - N^T B^{-T} c_B)^T x_N
  = z* + cÃÑ_N^T x_N

Since cÃÑ_N ‚â• 0 and x_N ‚â• 0:
z = z* + cÃÑ_N^T x_N ‚â• z*

Therefore x^B is optimal. ‚àé

```

---

## üíª Simplex Code Example

```python
import numpy as np

def simplex(c, A, b):
    """
    Solve: min c·µÄx s.t. Ax = b, x ‚â• 0
    Assumes initial BFS with identity basis in last m columns
    """
    m, n = A.shape
    
    # Initial basis (last m columns)
    basis = list(range(n - m, n))
    
    while True:
        # Extract basis matrix
        B = A[:, basis]
        B_inv = np.linalg.inv(B)
        
        # Basic solution
        x_B = B_inv @ b
        
        # Compute reduced costs
        c_B = c[basis]
        pi = B_inv.T @ c_B  # Dual variables
        
        # Find entering variable (most negative reduced cost)
        reduced_costs = c - A.T @ pi
        reduced_costs[basis] = 0  # Set basic to 0
        
        j = np.argmin(reduced_costs)
        if reduced_costs[j] >= -1e-10:
            # Optimal!
            x = np.zeros(n)
            x[basis] = x_B
            return x, c @ x
        
        # Direction
        d = B_inv @ A[:, j]
        
        if np.all(d <= 0):
            raise ValueError("Problem is unbounded")
        
        # Ratio test
        ratios = np.full(m, np.inf)
        positive_d = d > 1e-10
        ratios[positive_d] = x_B[positive_d] / d[positive_d]
        
        r = np.argmin(ratios)
        
        # Pivot: swap variables
        basis[r] = j

# Two-phase simplex for finding initial BFS
def two_phase_simplex(c, A, b):
    """Handle problems without obvious initial BFS"""
    m, n = A.shape
    
    # Phase 1: Minimize sum of artificial variables
    A_aug = np.hstack([A, np.eye(m)])
    c_phase1 = np.concatenate([np.zeros(n), np.ones(m)])
    
    x_phase1, obj = simplex(c_phase1, A_aug, b)
    
    if obj > 1e-10:
        raise ValueError("Problem is infeasible")
    
    # Phase 2: Solve original problem
    # (Implementation depends on basis from phase 1)
    pass

```

---

## üìä Complexity Analysis

```
Worst case: Exponential O(2‚Åø) pivots (rare in practice)
Average case: O(m) pivots (polynomial in practice)

Klee-Minty cube: Exponential example
‚Ä¢ n-dimensional hypercube
‚Ä¢ 2‚Åø - 1 pivots with Dantzig's rule
‚Ä¢ Solved by randomized pivot rules

Per-pivot cost: O(m¬≤) for tableau update

```

---

## üîß Pivot Rules

| Rule | Description | Behavior |
|------|-------------|----------|
| **Dantzig** | Most negative cÃÑ_j | Classic, can cycle |
| **Bland** | Smallest index j with cÃÑ_j < 0 | Anti-cycling |
| **Steepest edge** | Maximize improvement per unit | Practical |
| **Random** | Random j with cÃÑ_j < 0 | Polynomial expected |

---

# Part 2: LP Duality

## üìê Mathematical Foundations

### Primal-Dual Pair

```
PRIMAL (P):                      DUAL (D):
-------------                    -------------
min  c·µÄx                         max  b·µÄy
s.t. Ax ‚â• b                      s.t. A·µÄy ‚â§ c
     x ‚â• 0                            y ‚â• 0

Relationship:
‚Ä¢ n primal variables ‚Üî n dual constraints
‚Ä¢ m primal constraints ‚Üî m dual variables
‚Ä¢ Primal min ‚Üî Dual max

```

### Converting Between Forms

```
Standard conversions:
+------------------+------------------+

|     PRIMAL       |      DUAL        |
+------------------+------------------+

| a·µ¢·µÄx ‚â• b·µ¢       | y·µ¢ ‚â• 0           |
| a·µ¢·µÄx ‚â§ b·µ¢       | y·µ¢ ‚â§ 0           |
| a·µ¢·µÄx = b·µ¢       | y·µ¢ free          |
+------------------+------------------+

| x‚±º ‚â• 0          | a‚±º·µÄy ‚â§ c‚±º       |
| x‚±º ‚â§ 0          | a‚±º·µÄy ‚â• c‚±º       |
| x‚±º free         | a‚±º·µÄy = c‚±º       |
+------------------+------------------+

```

---

## üìê Weak Duality Theorem

**Theorem:** For any feasible x (primal) and y (dual): b·µÄy ‚â§ c·µÄx

**Proof:**

```
Given:
‚Ä¢ y ‚â• 0, Ax ‚â• b (primal feasibility)
‚Ä¢ x ‚â• 0, A·µÄy ‚â§ c (dual feasibility)

Step 1:
y ‚â• 0 and Ax ‚â• b implies:
y·µÄ(Ax) ‚â• y·µÄb = b·µÄy

Step 2:
x ‚â• 0 and A·µÄy ‚â§ c implies:
x·µÄ(A·µÄy) ‚â§ x·µÄc = c·µÄx

Step 3:
Note that y·µÄAx = x·µÄA·µÄy (scalar transpose)

Combining:
b·µÄy ‚â§ y·µÄAx = x·µÄA·µÄy ‚â§ c·µÄx

Therefore: b·µÄy ‚â§ c·µÄx  ‚àé

Implication: Any dual feasible solution gives a lower bound on primal optimal!

```

---

## üìê Strong Duality Theorem

**Theorem:** If primal has optimal solution x*, then dual has optimal solution y* with c·µÄx* = b·µÄy*

```
The duality gap is zero at optimum!

Conditions for strong duality:
‚Ä¢ Both primal and dual are feasible
‚Ä¢ (Slater's condition for convex programs)

Proof outline (via complementary slackness):
At optimum, KKT conditions hold:
1. ‚àáL = 0 (stationarity)
2. Primal and dual feasibility
3. Œº·µ¢g·µ¢(x) = 0 (complementarity)

These imply strong duality through Lagrangian saddle point. ‚àé

```

### Complementary Slackness

```
At optimum (x*, y*):

Primal slack √ó Dual variable = 0:
    y*‚±º ¬∑ (a‚±º·µÄx* - b‚±º) = 0  for all j

Dual slack √ó Primal variable = 0:
    x*·µ¢ ¬∑ (c·µ¢ - a·µ¢·µÄy*) = 0  for all i

Interpretation:
‚Ä¢ If constraint is slack, dual variable = 0
‚Ä¢ If dual variable > 0, constraint is tight

```

---

## üíª Duality Code Example

```python
import numpy as np
from scipy.optimize import linprog

def solve_primal_dual(c, A_ub, b_ub):
    """
    Solve primal and verify dual relationships
    
    Primal: min c·µÄx s.t. Ax ‚â§ b, x ‚â• 0
    Dual:   max b·µÄy s.t. A·µÄy ‚â§ c, y ‚â• 0
    """
    # Solve primal
    result_primal = linprog(c, A_ub=A_ub, b_ub=b_ub, method='highs')
    x_opt = result_primal.x
    primal_obj = result_primal.fun
    
    # Solve dual: max b·µÄy ‚Üí min -b·µÄy
    m, n = A_ub.shape
    result_dual = linprog(
        -b_ub,  # Negate for maximization
        A_ub=A_ub.T,  # Transpose
        b_ub=c,
        method='highs'
    )
    y_opt = result_dual.x
    dual_obj = -result_dual.fun  # Negate back
    
    print(f"Primal optimal: {primal_obj:.6f}")
    print(f"Dual optimal:   {dual_obj:.6f}")
    print(f"Duality gap:    {abs(primal_obj - dual_obj):.6f}")
    
    # Verify complementary slackness
    primal_slack = b_ub - A_ub @ x_opt
    print(f"\nComplementary slackness check:")
    for j in range(m):
        cs = y_opt[j] * primal_slack[j]
        print(f"  y[{j}] * slack[{j}] = {y_opt[j]:.4f} * {primal_slack[j]:.4f} = {cs:.6f}")
    
    return x_opt, y_opt

# Example: Simple production problem
c = np.array([4, 3])  # Profits (minimize negative)
A = np.array([
    [2, 1],   # Machine 1 hours
    [1, 2],   # Machine 2 hours
])
b = np.array([8, 7])  # Available hours

x_opt, y_opt = solve_primal_dual(-c, A, b)  # Note: negate c for max

```

---

## üåç Economic Interpretation: Shadow Prices

```
Shadow Prices (Dual Variables):

y*‚±º = ‚àÇ(optimal objective) / ‚àÇb‚±º

‚Ä¢ Marginal value of relaxing constraint j
‚Ä¢ How much would you pay for one more unit of resource j?

Example:
If y*‚ÇÅ = 1.5 for machine-1 hours constraint
‚Üí One additional hour of machine 1 improves profit by $1.50

```

---

# Part 3: Interior Point Methods

## üéØ Visual Overview

*Interior point methods follow the central path through the interior, converging to the optimal solution in polynomial time O(‚àön log(1/Œµ)).*

---

## üìê Mathematical Foundations

### Barrier Method

```
Original LP:
    min  c·µÄx
    s.t. Ax = b, x ‚â• 0

Barrier formulation:
    min  c·µÄx - Œº Œ£·µ¢ log(x·µ¢)
    s.t. Ax = b

‚Ä¢ Barrier: -log(x) ‚Üí ‚àû as x ‚Üí 0 (keeps iterates interior)
‚Ä¢ Œº > 0: barrier parameter
‚Ä¢ As Œº ‚Üí 0, solution approaches LP optimum

```

### Central Path

```
For each Œº > 0, barrier problem has unique solution x*(Œº)

Central path: { x*(Œº) : Œº > 0 }

Properties:
‚Ä¢ x*(Œº) strictly in interior (all x·µ¢ > 0)
‚Ä¢ As Œº ‚Üí 0‚Å∫, x*(Œº) ‚Üí x* (LP solution)
‚Ä¢ Path is smooth curve through interior

```

### KKT Conditions for Barrier Problem

```
Optimality conditions at x*(Œº):

‚àá‚ÇìL = c - ŒºX‚Åª¬πe + A·µÄŒª = 0
Ax = b
x > 0

Where X = diag(x), e = (1,1,...,1)·µÄ

Rearranging: XŒª = Œºe  (complementarity modified)

```

### Primal-Dual Interior Point

```
Solve primal AND dual simultaneously!

Primal: min c·µÄx,  Ax = b, x ‚â• 0
Dual:   max b·µÄy,  A·µÄy + s = c, s ‚â• 0

Modified KKT system:
+           + +    +   +        +

|  A   0  0 | | Œîx |   | b - Ax |
|  0  A·µÄ  I | | Œîy | = | c-A·µÄy-s|
|  S   0  X | | Œîs |   | Œºe - XSe|
+           + +    +   +        +

Where X = diag(x), S = diag(s)

```

---

## üìê Convergence Analysis

**Theorem:** Interior point methods converge in O(‚àön log(1/Œµ)) iterations.

**Proof sketch:**

```
Step 1: Define duality measure
  Œº = x·µÄs / n (average complementarity)

Step 2: Show Œº decreases geometrically
  After each Newton step: Œº_new ‚â§ (1 - Œ∏/‚àön) Œº
  where Œ∏ ‚àà (0,1) is step parameter

Step 3: Count iterations
  Starting Œº‚ÇÄ, target Œµ:
  Need k iterations where (1 - Œ∏/‚àön)^k Œº‚ÇÄ ‚â§ Œµ
  
  Taking logs:
  k ‚â• log(Œº‚ÇÄ/Œµ) / log(1/(1-Œ∏/‚àön))
    ‚âà (‚àön/Œ∏) log(Œº‚ÇÄ/Œµ)
    = O(‚àön log(1/Œµ))  ‚àé

```

### Complexity Analysis

```
Number of iterations: O(‚àön log(1/Œµ))

Per iteration: O(n¬≥) for solving linear system

Total: O(n^3.5 log(1/Œµ))

Compare to Simplex:
‚Ä¢ Simplex: O(2‚Åø) worst case, O(m) average
‚Ä¢ Interior: O(‚àön) iterations (polynomial guaranteed)

```

---

## üíª Interior Point Code Example

```python
import numpy as np

def interior_point_lp(c, A, b, tol=1e-8, mu_factor=0.1):
    """
    Primal-Dual Interior Point for LP
    min c·µÄx s.t. Ax = b, x ‚â• 0
    """
    m, n = A.shape
    
    # Initialize (strictly feasible)
    x = np.ones(n)
    s = np.ones(n)  # Slack for dual
    y = np.zeros(m)  # Dual variables
    
    for iteration in range(100):
        # Duality gap and barrier parameter
        mu = (x @ s) / n
        if mu < tol:
            break
        
        # Target (reduce Œº)
        sigma = mu_factor
        mu_target = sigma * mu
        
        # Residuals
        r_primal = b - A @ x
        r_dual = c - A.T @ y - s
        r_cent = mu_target * np.ones(n) - x * s
        
        # Form and solve Newton system
        # [  A    0    0  ] [dx]   [r_primal]
        # [  0   A·µÄ   I  ] [dy] = [r_dual  ]
        # [  S    0    X  ] [ds]   [r_cent  ]
        
        X_inv = np.diag(1/x)
        S = np.diag(s)
        
        # Schur complement: (A X‚Åª¬π S A·µÄ) dy = ...
        M = A @ X_inv @ S @ A.T
        rhs = r_primal + A @ X_inv @ (r_cent - x * r_dual)
        
        dy = np.linalg.solve(M, rhs)
        ds = r_dual - A.T @ dy
        dx = X_inv @ (r_cent - x * ds)
        
        # Step size (stay interior: x, s > 0)
        alpha_primal = min(1.0, 0.99 * min(-x[dx<0] / dx[dx<0], default=1.0))
        alpha_dual = min(1.0, 0.99 * min(-s[ds<0] / ds[ds<0], default=1.0))
        
        # Update
        x = x + alpha_primal * dx
        y = y + alpha_dual * dy
        s = s + alpha_dual * ds
        
        print(f"Iter {iteration}: Œº = {mu:.2e}, obj = {c@x:.6f}")
    
    return x, c @ x

# Example
c = np.array([-1, -2])  # min -x‚ÇÅ - 2x‚ÇÇ (maximize x‚ÇÅ + 2x‚ÇÇ)
A = np.array([[1, 1, 1, 0],
              [2, 1, 0, 1]])  # With slacks
b = np.array([4, 5])

# Note: Need to handle equality constraints properly

```

---

## üìä Comparison: Simplex vs Interior Point

| Aspect | Simplex | Interior Point |
|--------|---------|----------------|
| **Path** | Along edges (vertices) | Through interior |
| **Complexity** | Exponential worst case | Polynomial O(‚àön) |
| **Practice** | Fast for sparse/small | Fast for dense/large |
| **Warm start** | Easy | Difficult |
| **Sensitivity** | Direct from basis | From KKT |

---

## üìö Resources

| Type | Title | Link |
|------|-------|------|
| üìñ | Bertsimas & Tsitsiklis | [Book](https://www.amazon.com/Introduction-Linear-Optimization-Scientific-Computation/dp/1886529191) |
| üìñ | Vanderbei - LP | [Free PDF](https://vanderbei.princeton.edu/LPbook/) |
| üìñ | Boyd & Vandenberghe Ch. 11 | [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) |
| üõ†Ô∏è | Gurobi (Solver) | [Link](https://www.gurobi.com/) |
| üõ†Ô∏è | OR-Tools (Google) | [Link](https://developers.google.com/optimization) |
| üìÑ | Karmarkar (1984) | Original polynomial-time algorithm |
| üìÑ | Mehrotra (1992) | Predictor-corrector method |
| üá®üá≥ | Áü•‰πé Á∫øÊÄßËßÑÂàí | [Áü•‰πé](https://zhuanlan.zhihu.com/p/26377904) |
| üá®üá≥ | ÂçïÁ∫ØÂΩ¢Ê≥ïËØ¶Ëß£ | [Áü•‰πé](https://zhuanlan.zhihu.com/p/31644892) |
| üá®üá≥ | ÂÜÖÁÇπÊ≥ïËØ¶Ëß£ | [Áü•‰πé](https://zhuanlan.zhihu.com/p/48476987) |

---

## üîó Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | SVM, sparse optimization |
| **Operations Research** | Supply chain, scheduling |
| **Finance** | Portfolio optimization |
| **Network Design** | Flow optimization |

---

‚¨ÖÔ∏è [Back: Constrained Optimization](../05_constrained_optimization/) | ‚û°Ô∏è [Next: Integer Programming](../07_integer_programming/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
