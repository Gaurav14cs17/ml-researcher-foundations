<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Convex%20Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## üéØ Why Convexity Matters

```
+---------------------------------------------------------+
|                                                         |
|   Non-convex:                 Convex:                   |
|                                                         |
|       ‚Ä¢                           ‚Ä¢                     |
|      / \                         / \                    |
|     /   \   ‚Ä¢                   /   \                   |
|    ‚Ä¢     \ / \                 /     \                  |
|           ‚Ä¢   ‚Ä¢               ‚Ä¢-------‚Ä¢                 |
|                                                         |
|   Many local minima          ONE global minimum         |
|   Hard to solve              Efficiently solvable       |
|   No guarantees              Polynomial algorithms      |
|                                                         |
+---------------------------------------------------------+

```

---

## üìê Mathematical Definitions

### Convex Set

```
A set C ‚äÜ ‚Ñù‚Åø is convex if:

‚àÄx, y ‚àà C and ‚àÄŒ∏ ‚àà [0,1]:
Œ∏x + (1-Œ∏)y ‚àà C

Interpretation:
"Line segment between any two points lies entirely in C"

Examples:
‚úì Hyperplanes: {x : a·µÄx = b}
‚úì Halfspaces: {x : a·µÄx ‚â§ b}
‚úì Balls: {x : ||x - c|| ‚â§ r}
‚úì Polyhedra: {x : Ax ‚â§ b}
‚úó Non-convex: donut shape, star shape

```

### Convex Function

```
A function f: ‚Ñù‚Åø ‚Üí ‚Ñù is convex if:

f(Œ∏x + (1-Œ∏)y) ‚â§ Œ∏f(x) + (1-Œ∏)f(y)

for all x, y ‚àà dom(f) and Œ∏ ‚àà [0,1]

Interpretation:
"Chord lies above the graph"

       f(y)‚óè
           ‚ï≤ chord
            ‚ï≤
       f(Œ∏x+(1-Œ∏)y)  ‚â§  Œ∏f(x)+(1-Œ∏)f(y)
              ‚óè
             ‚ï± graph
            ‚ï±
       f(x)‚óè

```

### Strict Convexity

```
f is strictly convex if:

f(Œ∏x + (1-Œ∏)y) < Œ∏f(x) + (1-Œ∏)f(y)

for all x ‚â† y and Œ∏ ‚àà (0,1)

Implication: Unique global minimum (if exists)

```

### Strong Convexity

```
f is Œº-strongly convex if:

f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y-x) + (Œº/2)||y-x||¬≤

Equivalent condition:
f(x) - (Œº/2)||x||¬≤ is convex

Properties:
‚Ä¢ Unique minimum exists
‚Ä¢ Gradient descent converges linearly
‚Ä¢ Condition number Œ∫ = L/Œº bounds convergence

```

---

## üìê First and Second Order Conditions

### First-Order Condition

```
If f is differentiable:

f convex  ‚ü∫  f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y-x)  for all x, y

Interpretation:
"Tangent plane lies below the graph"

Consequence:
‚àáf(x*) = 0  ‚üπ  x* is GLOBAL minimum

```

### Second-Order Condition

```
If f is twice differentiable:

f convex  ‚ü∫  ‚àá¬≤f(x) ‚™∞ 0 (positive semi-definite) for all x

f strictly convex  ‚ü∫  ‚àá¬≤f(x) ‚âª 0 (positive definite)

How to check:
‚Ä¢ All eigenvalues of Hessian ‚â• 0
‚Ä¢ Principal minors ‚â• 0
‚Ä¢ Cholesky decomposition exists

```

---

## üìê Examples of Convex Functions

### Common Convex Functions

```
1. Linear: f(x) = a·µÄx + b
   Hessian: ‚àá¬≤f = 0 ‚™∞ 0  ‚úì

2. Affine: f(x) = Ax + b
   (both convex and concave)

3. Quadratic: f(x) = (1/2)x·µÄPx + q·µÄx + r
   Convex iff P ‚™∞ 0

4. Norms: f(x) = ||x||_p for p ‚â• 1
   Always convex (triangle inequality)

5. Exponential: f(x) = eÀ£
   f''(x) = eÀ£ > 0  ‚úì

6. Log-sum-exp: f(x) = log(Œ£·µ¢ eÀ£‚Å±)
   Smooth approximation to max function

7. Negative entropy: f(x) = Œ£·µ¢ x·µ¢ log x·µ¢
   Convex on x > 0

```

### Common Concave Functions

```
1. Logarithm: f(x) = log(x)
   f''(x) = -1/x¬≤ < 0

2. Square root: f(x) = ‚àöx
   f''(x) = -1/(4x^(3/2)) < 0

3. Geometric mean: f(x) = (Œ†·µ¢ x·µ¢)^(1/n)
   Concave on x > 0

```

---

## üìê Preserving Convexity

### Operations That Preserve Convexity

```
1. Non-negative weighted sum:
   f = Œ£·µ¢ Œ±·µ¢f·µ¢ with Œ±·µ¢ ‚â• 0
   convex if all f·µ¢ convex

2. Composition with affine:
   g(x) = f(Ax + b)
   convex if f convex

3. Pointwise maximum:
   f(x) = max{f‚ÇÅ(x), f‚ÇÇ(x), ..., f‚Çñ(x)}
   convex if all f·µ¢ convex

4. Perspective:
   g(x, t) = tf(x/t)
   convex if f convex, t > 0

5. Partial minimization:
   g(x) = inf_{y‚ààC} f(x, y)
   convex if f convex in (x,y) and C convex

```

---

## üìê Convex Optimization Problem

### Standard Form

```
minimize    f‚ÇÄ(x)           (objective)
subject to  f·µ¢(x) ‚â§ 0       i = 1,...,m  (inequalities)
            h‚±º(x) = 0       j = 1,...,p  (equalities)

Convex program if:
‚Ä¢ f‚ÇÄ, f‚ÇÅ, ..., f‚Çò are convex
‚Ä¢ h‚ÇÅ, ..., h‚Çö are affine

```

### Key Property

```
LOCAL minimum = GLOBAL minimum

Proof:
Suppose x* is local min but not global.
Then ‚àÉy with f(y) < f(x*).
By convexity: f(Œ∏y + (1-Œ∏)x*) ‚â§ Œ∏f(y) + (1-Œ∏)f(x*) < f(x*)
for all Œ∏ ‚àà (0,1].
But Œ∏y + (1-Œ∏)x* can be arbitrarily close to x*.
Contradiction with x* being local min! ‚àé

```

---

## üìê Duality Theory

### Lagrangian

```
L(x, Œª, ŒΩ) = f‚ÇÄ(x) + Œ£·µ¢ Œª·µ¢f·µ¢(x) + Œ£‚±º ŒΩ‚±ºh‚±º(x)

where Œª·µ¢ ‚â• 0 (for inequalities)
      ŒΩ‚±º ‚àà ‚Ñù (for equalities)

```

### Dual Function

```
g(Œª, ŒΩ) = inf_x L(x, Œª, ŒΩ)

Properties:
‚Ä¢ g is always concave (even if primal not convex)
‚Ä¢ g(Œª, ŒΩ) ‚â§ p* for any Œª ‚â• 0, ŒΩ  (weak duality)

```

### Dual Problem

```
maximize    g(Œª, ŒΩ)
subject to  Œª ‚â• 0

‚Ä¢ Always a convex problem!
‚Ä¢ Optimal value d* ‚â§ p*

```

### Strong Duality

```
d* = p* when:
1. Slater's condition: ‚àÉx strictly feasible
   (f·µ¢(x) < 0 for all i, h‚±º(x) = 0)
2. Problem is convex

Applications:
‚Ä¢ Dual provides lower bound
‚Ä¢ Complementary slackness for KKT
‚Ä¢ Economic interpretation (shadow prices)

```

---

## üìê Optimality Conditions (KKT)

### KKT Conditions for Convex Problems

```
For convex problem with strong duality:
x*, Œª*, ŒΩ* optimal iff:

1. Stationarity:
   ‚àáf‚ÇÄ(x*) + Œ£·µ¢ Œª·µ¢*‚àáf·µ¢(x*) + Œ£‚±º ŒΩ‚±º*‚àáh‚±º(x*) = 0

2. Primal feasibility:
   f·µ¢(x*) ‚â§ 0, h‚±º(x*) = 0

3. Dual feasibility:
   Œª·µ¢* ‚â• 0

4. Complementary slackness:
   Œª·µ¢*f·µ¢(x*) = 0  for all i

```

### Using KKT to Solve Problems

```
Strategy:
1. Write down KKT conditions
2. Consider cases (which constraints active?)
3. Solve resulting system of equations
4. Verify solution satisfies all conditions

```

---

## üìê Convex Optimization Algorithms

### Gradient Descent

```
For unconstrained smooth convex f:

x_{k+1} = x_k - Œ±‚àáf(x_k)

Convergence (L-smooth):
  f(x_k) - f* ‚â§ O(1/k)

Convergence (Œº-strongly convex):
  f(x_k) - f* ‚â§ (1 - Œº/L)^k (f(x_0) - f*)

```

### Projected Gradient Descent

```
For constrained problem min f(x) s.t. x ‚àà C:

x_{k+1} = Œ†_C(x_k - Œ±‚àáf(x_k))

where Œ†_C is projection onto C:
Œ†_C(y) = argmin_{x‚ààC} ||x - y||¬≤

```

### Proximal Gradient

```
For f(x) = g(x) + h(x) where g smooth, h non-smooth:

x_{k+1} = prox_{Œ±h}(x_k - Œ±‚àág(x_k))

where prox_{h}(y) = argmin_x {h(x) + (1/2)||x-y||¬≤}

```

### Interior Point Methods

```
For constrained problems:

Replace f·µ¢(x) ‚â§ 0 with barrier:
minimize f‚ÇÄ(x) - (1/t)Œ£·µ¢ log(-f·µ¢(x))

As t ‚Üí ‚àû, solution ‚Üí original optimal
Complexity: O(‚àöm log(1/Œµ)) Newton steps

```

---

## üíª Code Examples

### Using CVXPY

```python
import cvxpy as cp
import numpy as np

# Variables
x = cp.Variable(10)

# Objective (convex)
objective = cp.Minimize(cp.sum_squares(x))

# Constraints
constraints = [x >= 0, cp.sum(x) == 1]

# Solve
problem = cp.Problem(objective, constraints)
problem.solve()

print(f"Optimal value: {problem.value}")
print(f"Optimal x: {x.value}")

```

### Check Convexity

```python
import numpy as np
from scipy.linalg import eigvalsh

def is_convex_quadratic(P):
    """Check if f(x) = x'Px is convex"""
    eigenvalues = eigvalsh(P)
    return np.all(eigenvalues >= -1e-10)

def check_hessian_psd(hessian_fn, x, eps=1e-6):
    """Check if Hessian is PSD at point x"""
    H = hessian_fn(x)
    eigenvalues = eigvalsh(H)
    return np.all(eigenvalues >= -eps)

# Example
P = np.array([[2, 1], [1, 3]])
print(f"Quadratic is convex: {is_convex_quadratic(P)}")

```

### Gradient Descent for Convex Function

```python
import numpy as np

def gradient_descent_convex(f, grad_f, x0, alpha=0.01, 
                            max_iter=1000, tol=1e-6):
    """
    Gradient descent for convex function.
    Guaranteed to find global minimum!
    """
    x = x0.copy()
    
    for k in range(max_iter):
        g = grad_f(x)
        
        if np.linalg.norm(g) < tol:
            print(f"Converged in {k} iterations")
            break
            
        x = x - alpha * g
    
    return x

# Example: Minimize ||Ax - b||¬≤ (convex!)
A = np.random.randn(10, 5)
b = np.random.randn(10)

def f(x):
    return 0.5 * np.linalg.norm(A @ x - b)**2

def grad_f(x):
    return A.T @ (A @ x - b)

x0 = np.zeros(5)
x_opt = gradient_descent_convex(f, grad_f, x0)

# Compare with closed-form solution
x_closed = np.linalg.lstsq(A, b, rcond=None)[0]
print(f"GD solution: {x_opt}")
print(f"Closed form: {x_closed}")

```

---

## üìä Convex vs Non-Convex

| Aspect | Convex | Non-Convex |
|--------|--------|------------|
| **Local minima** | = Global | Many |
| **Algorithms** | Polynomial | NP-hard |
| **Guarantees** | Strong | Few |
| **Neural nets** | No | Yes |
| **Linear regression** | Yes | N/A |
| **SVMs** | Yes | No |
| **Logistic reg.** | Yes | No |

---

## üåç Applications

| Application | Problem Type | Method |
|-------------|--------------|--------|
| **Portfolio opt.** | Quadratic | Interior point |
| **Compressed sensing** | L1 minimization | ADMM |
| **SVM** | Quadratic | SMO |
| **Logistic regression** | Log-likelihood | Newton |
| **Optimal transport** | Linear program | Sinkhorn |
| **Control** | SDP, SOCP | Interior point |

---

## üìö Resources

| Type | Title | Link |
|------|-------|------|
| üìñ | Boyd Convex Optimization | [Free PDF](https://web.stanford.edu/~boyd/cvxbook/) |
| üìñ | Nesterov Intro Lectures | [Springer](https://link.springer.com/book/10.1007/978-1-4419-8853-9) |
| üõ†Ô∏è | CVXPY | [cvxpy.org](https://www.cvxpy.org/) |
| üé• | Stanford EE364a | [YouTube](https://www.youtube.com/playlist?list=PL3940DD956CDF0622) |
| üá®üá≥ | Âá∏‰ºòÂåñÂü∫Á°Ä | [Áü•‰πé](https://zhuanlan.zhihu.com/p/25385801) |

---

‚¨ÖÔ∏è [Back: Advanced Methods](../03_advanced_methods/) | ‚û°Ô∏è [Next: Convex Optimization (Main)](../04_convex_optimization/)

> **Note:** This folder covers additional convex optimization topics. See [04_convex_optimization](../04_convex_optimization/) for the main convex optimization content.

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
