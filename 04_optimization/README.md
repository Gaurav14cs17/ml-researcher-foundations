<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Optimization&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## ğŸ“Š Learning Path

```
ğŸš€ Start
    |
    â–¼
+---------------------------------------------------------------------------+
|                                                                           |
|  1. Foundations --â–¶ 2. Basic Methods --â–¶ 3. Advanced --â–¶ 4. Convex       |
|     ğŸ“                 ğŸ“‰                   âš¡              ğŸ“Š            |
|                                                                           |
+---------------------------------------------------------------------------+
                                    |
                                    â–¼
+---------------------------------------------------------------------------+
|                                                                           |
|  5. Constrained --â–¶ 6. Linear Prog --â–¶ 7. Integer Prog --â–¶ 8. ML Optim  |
|     ğŸ”’                 ğŸ“ˆ                 ğŸ”¢                  ğŸ¤– â­       |
|                                                                           |
+---------------------------------------------------------------------------+
                                    |
                                    â–¼
                          9. Metaheuristics ğŸ§¬
                                    |
                                    â–¼
                               ğŸ† Master

```

## ğŸ¯ What You'll Learn

> ğŸ’¡ **Training = Optimization.** Every neural network learns by minimizing a loss function.

<table>
<tr>
<td align="center">

### ğŸ“‰ Gradient Descent
Foundation of learning

</td>
<td align="center">

### ğŸš€ Adam
Default optimizer (90%)

</td>
<td align="center">

### ğŸ”’ Constrained
KKT, SVM derivation

</td>
</tr>
</table>

---

## ğŸ“‚ Complete Folder Structure

### Main Learning Path

| # | Folder | Topic | Description |
|:-:|--------|-------|-------------|
| 1 | [01_foundations/](./01_foundations/) | **Foundations** | Calculus, Linear Algebra, Gradients, Hessian |
| 2 | [02_basic_methods/](./02_basic_methods/) | **Basic Methods** | Gradient Descent, Newton's Method |
| 3 | [03_advanced_methods/](./03_advanced_methods/) | **Advanced Methods** | Quasi-Newton, Conjugate Gradient, BFGS |
| 4 | [04_convex_optimization/](./04_convex_optimization/) | **Convex Optimization** | Convex Functions, ELBO, Duality |
| 5 | [05_constrained_optimization/](./05_constrained_optimization/) | **Constrained Optimization** | Lagrange Multipliers, KKT Conditions |
| 6 | [06_linear_programming/](./06_linear_programming/) | **Linear Programming** | Simplex, Interior Point, Duality |
| 7 | [07_integer_programming/](./07_integer_programming/) | **Integer Programming** | Branch & Bound, MILP |
| 8 | [08_machine_learning/](./08_machine_learning/) | **ML Optimizers** â­ | SGD, Momentum, Adam, AdamW |
| 9 | [09_metaheuristics/](./09_metaheuristics/) | **Metaheuristics** | Genetic Algorithms, Simulated Annealing |

### Additional Topics

| Folder | Topic | Description |
|--------|-------|-------------|
| [03_convex_optimization/](./03_convex_optimization/) | Convex Optimization (Extra) | Additional convex optimization content |
| [04_constrained_optimization/](./04_constrained_optimization/) | Constrained (Extra) | Additional Lagrange/KKT content |
| [05_learning_rate_schedules/](./05_learning_rate_schedules/) | **LR Schedules** | Step, Cosine, Warmup, Cyclical |

---

## ğŸ“š Main Topics

### 1ï¸âƒ£ Foundations

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/>

```
Gradients --â–¶ Hessian --â–¶ Taylor --â–¶ Convexity --â–¶ Lipschitz

```

**Core:** Gradients, Hessian, Taylor Series, Convexity, Linear Algebra

| Subtopic | Description |
|----------|-------------|
| [01_calculus/](./01_foundations/01_calculus/) | Gradients, Hessian, Partial Derivatives |
| [02_linear_algebra/](./01_foundations/02_linear_algebra/) | Eigenvalues, Positive Definiteness, SVD |

<a href="./01_foundations/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 2ï¸âƒ£ Basic Methods

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/>

```
GD --â–¶ Line Search --â–¶ Newton --â–¶ Convergence

```

**Core:** GD: Î¸ â† Î¸ - Î·âˆ‡L(Î¸), Newton's Method, Convergence Analysis

| Subtopic | Description |
|----------|-------------|
| [01_gradient_descent/](./02_basic_methods/01_gradient_descent/) | Vanilla GD, Learning Rates, Convergence |
| [02_newton/](./02_basic_methods/02_newton/) | Newton's Method, Quadratic Convergence |

<a href="./02_basic_methods/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 3ï¸âƒ£ Advanced Methods

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/>

```
Quasi-Newton --â–¶ BFGS --â–¶ L-BFGS --â–¶ Conjugate Gradient

```

**Core:** Quasi-Newton Methods, BFGS, L-BFGS, Conjugate Gradient

<a href="./03_advanced_methods/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 4ï¸âƒ£ Convex Optimization

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/>

```
Convex Sets --â–¶ Convex Funcs --â–¶ Optimality --â–¶ Duality --â–¶ ML Apps

```

**Core:** Convex Functions, First-Order Optimality, Duality, ELBO

| Subtopic | Description |
|----------|-------------|
| [01_elbo/](./04_convex_optimization/01_elbo/) | Evidence Lower Bound, VAE, Diffusion |

<a href="./04_convex_optimization/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 5ï¸âƒ£ Constrained Optimization

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/>

```
Lagrange --â–¶ KKT --â–¶ Inequality --â–¶ SVM --â–¶ Dual

```

**Core:** Lagrange Multipliers, KKT Conditions, SVM Derivation

| Subtopic | Description |
|----------|-------------|
| [01_kkt/](./05_constrained_optimization/01_kkt/) | KKT Conditions, Optimality |
| [02_lagrange/](./05_constrained_optimization/02_lagrange/) | Lagrange Multipliers |

<a href="./05_constrained_optimization/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 6ï¸âƒ£ Linear Programming

<img src="https://img.shields.io/badge/Time-8_hours-blue?style=flat-square"/>

```
LP Formulation --â–¶ Simplex --â–¶ Duality --â–¶ Interior Point

```

**Core:** Simplex Algorithm, LP Duality, Interior Point Methods

| Subtopic | Description |
|----------|-------------|
| [01_simplex/](./06_linear_programming/01_simplex/) | Simplex Algorithm |

<a href="./06_linear_programming/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 7ï¸âƒ£ Integer Programming

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/>

```
IP Formulation --â–¶ Branch & Bound --â–¶ Cutting Planes --â–¶ MILP

```

**Core:** Branch & Bound, Mixed Integer Linear Programming (MILP)

<a href="./07_integer_programming/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 8ï¸âƒ£ ML Optimizers â­â­â­

<img src="https://img.shields.io/badge/Time-10_hours-blue?style=flat-square"/> <img src="https://img.shields.io/badge/ğŸ”¥_MOST_IMPORTANT-critical?style=flat-square"/>

```
SGD --â–¶ Momentum --â–¶ RMSprop --â–¶ Adam --â–¶ AdamW

```

> â­ **Adam is the default optimizer for 90% of models**

| Optimizer | Speed | Best For |
|:---------:|:-----:|----------|
| SGD | Slow | Simple, regularization |
| Momentum | Medium | Convex problems |
| **Adam** | **Fast** | **Default choice** â­ |
| AdamW | Fast | Transformers, LLMs |

| Subtopic | Description |
|----------|-------------|
| [01_adam/](./08_machine_learning/01_adam/) | Adam, AdamW, Bias Correction |
| [02_sgd/](./08_machine_learning/02_sgd/) | SGD, Momentum, Nesterov |

<a href="./08_machine_learning/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

### 9ï¸âƒ£ Metaheuristics

<img src="https://img.shields.io/badge/Time-6_hours-blue?style=flat-square"/>

```
Genetic --â–¶ Annealing --â–¶ Swarm --â–¶ Evolution

```

**Core:** Genetic Algorithms, Simulated Annealing, Particle Swarm

| Subtopic | Description |
|----------|-------------|
| [01_genetic_algorithm/](./09_metaheuristics/01_genetic_algorithm/) | Genetic Algorithm, Crossover, Mutation |

<a href="./09_metaheuristics/README.md"><img src="https://img.shields.io/badge/ğŸ“–_Dive_In-E91E63?style=for-the-badge" alt="Learn"/></a>

---

## ğŸ’¡ Key Algorithms

<table>
<tr>
<td>

### ğŸ“‰ Gradient Descent

```python
Î¸ â† Î¸ - Î·âˆ‡L(Î¸)

```

</td>
<td>

### ğŸƒ Momentum

```python
v â† Î²v + âˆ‡L(Î¸)
Î¸ â† Î¸ - Î·v

```

</td>
<td>

### ğŸš€ Adam

```python
m â† Î²â‚m + (1-Î²â‚)âˆ‡L
v â† Î²â‚‚v + (1-Î²â‚‚)(âˆ‡L)Â²
Î¸ â† Î¸ - Î·Â·mÌ‚/(âˆšvÌ‚+Îµ)

```

</td>
</tr>
</table>

---

## ğŸ”— Prerequisites & Next Steps

```
ğŸ“Š Math --â–¶ ğŸ¯ Optimization --â–¶ ğŸ§¬ ML Theory --â–¶ ğŸš€ Deep Learning --â–¶ âš¡ Training

```

<p align="center">
  <a href="../02_mathematics/README.md"><img src="https://img.shields.io/badge/â†_Prerequisites:_Mathematics-gray?style=for-the-badge" alt="Prev"/></a>
  <a href="../05_ml_theory/README.md"><img src="https://img.shields.io/badge/Next:_ML_Theory_â†’-00C853?style=for-the-badge" alt="Next"/></a>
</p>

---

## ğŸ“š Recommended Resources

| Type | Resource | Focus |
|:----:|----------|-------|
| ğŸ“˜ | [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) | Boyd & Vandenberghe |
| ğŸ“„ | [Adam Paper](https://arxiv.org/abs/1412.6980) | Original Adam |
| ğŸ“„ | [AdamW Paper](https://arxiv.org/abs/1711.05101) | Weight decay fix |
| ğŸ“˜ | [Numerical Optimization](https://www.springer.com/gp/book/9780387303031) | Nocedal & Wright |
| ğŸ¥ | [Stanford EE364a](https://www.youtube.com/playlist?list=PL3940DD956CDF0622) | Convex Optimization Course |

---

## ğŸ—ºï¸ Quick Navigation

| Previous | Current | Next |
|:--------:|:-------:|:----:|
| [ğŸ“ˆ Probability](../03_probability_statistics/README.md) | **ğŸ¯ Optimization** | [ğŸ§¬ ML Theory â†’](../05_ml_theory/README.md) |

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
