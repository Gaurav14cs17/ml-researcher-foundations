<!-- Animated Header -->
<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=120&section=header&text=Advanced%20Optimization%20Methods&fontSize=32&fontColor=fff&animation=twinkling&fontAlignY=35" width="100%"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Section-04-FF6B6B?style=for-the-badge&logo=bookstack&logoColor=white" alt="Section"/>
  <img src="https://img.shields.io/badge/Author-Gaurav_Goswami-blue?style=for-the-badge" alt="Author"/>
  <img src="https://img.shields.io/badge/Updated-December_2025-green?style=for-the-badge" alt="Updated"/>
</p>

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

---

## üéØ Visual Overview

<img src="./images/advanced-methods.svg" width="100%">

*Caption: Advanced optimization methods include Natural Gradient (invariant updates), Proximal Methods (non-smooth regularizers), and research frontiers like SAM, LAMB, Lion, and Sophia.*

---

## üìÇ Overview

Advanced methods go beyond basic gradient descent by:

- Using curvature information (Quasi-Newton)

- Exploiting conjugate directions (CG)

- Handling non-smooth objectives (Proximal)

- Achieving scale invariance (Natural Gradient)

---

## üìä Method Comparison

| Method | Convergence | Memory | Per-iteration Cost |
|--------|-------------|--------|-------------------|
| **GD** | O(1/t) | O(n) | O(n) |
| **Momentum** | O(1/t¬≤) | O(n) | O(n) |
| **BFGS** | Superlinear | O(n¬≤) | O(n¬≤) |
| **L-BFGS** | Superlinear | O(mn) | O(mn) |
| **CG** | O(‚àöŒ∫) | O(n) | O(n) |
| **Newton** | Quadratic | O(n¬≤) | O(n¬≥) |

```
Œ∫ = condition number = Œª_max / Œª_min
Larger Œ∫ = harder optimization

```

---

# Part 1: Quasi-Newton Methods (BFGS, L-BFGS)

## üìê Mathematical Foundations

### Core Idea

```
Newton: x_{k+1} = x_k - H_k‚Åª¬π ‚àáf_k   (requires O(n¬≤) Hessian)

Quasi-Newton: x_{k+1} = x_k - B_k‚Åª¬π ‚àáf_k   (B_k approximates H_k)

Key insight: Build B from gradient differences only!

```

### Secant Condition

```
The approximation B_{k+1} must satisfy:

B_{k+1} ¬∑ s_k = y_k

where:
  s_k = x_{k+1} - x_k        (step taken)
  y_k = ‚àáf_{k+1} - ‚àáf_k      (gradient change)

This matches curvature along the direction traveled

```

### BFGS Update Formula

```
B_{k+1} = B_k + (y_k y_k·µÄ)/(y_k·µÄ s_k) - (B_k s_k s_k·µÄ B_k)/(s_k·µÄ B_k s_k)

Properties:
‚Ä¢ Maintains positive definiteness (if B_0 > 0 and y_k·µÄs_k > 0)
‚Ä¢ Rank-2 update (efficient)
‚Ä¢ Self-correcting behavior

```

### Inverse BFGS (Direct H‚Åª¬π Update)

```
H_{k+1} = (I - œÅ_k s_k y_k·µÄ) H_k (I - œÅ_k y_k s_k·µÄ) + œÅ_k s_k s_k·µÄ

where œÅ_k = 1/(y_k·µÄ s_k)

Sherman-Morrison formula avoids matrix inversion!

```

### L-BFGS (Limited Memory)

```
Store only last m pairs: {s_i, y_i} for i = k-m+1, ..., k

Memory: O(mn) instead of O(n¬≤)
Compute H_k ¬∑ g via two-loop recursion:

Algorithm (Two-Loop Recursion):
1. q = ‚àáf_k
2. for i = k-1, ..., k-m:
      Œ±_i = œÅ_i s_i·µÄ q
      q = q - Œ±_i y_i
3. r = H_0 ¬∑ q   (H_0 = Œ≥I, Œ≥ = s_{k-1}·µÄy_{k-1}/y_{k-1}·µÄy_{k-1})
4. for i = k-m, ..., k-1:
      Œ≤ = œÅ_i y_i·µÄ r
      r = r + (Œ±_i - Œ≤) s_i
5. return r = H_k ‚àáf_k

```

---

## üíª Code Example

```python
import numpy as np

def bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000):
    n = len(x0)
    x = x0.copy()
    H = np.eye(n)  # Initial inverse Hessian approximation
    
    g = grad_f(x)
    
    for k in range(max_iter):
        if np.linalg.norm(g) < tol:
            break
        
        # Search direction
        d = -H @ g
        
        # Line search
        alpha = backtracking(f, grad_f, x, d)
        
        # Update
        s = alpha * d
        x_new = x + s
        g_new = grad_f(x_new)
        y = g_new - g
        
        # BFGS update (inverse form)
        rho = 1.0 / (y @ s)
        I = np.eye(n)
        H = (I - rho * np.outer(s, y)) @ H @ (I - rho * np.outer(y, s))
        H += rho * np.outer(s, s)
        
        x, g = x_new, g_new
    
    return x

def lbfgs(f, grad_f, x0, m=10, tol=1e-6, max_iter=1000):
    """L-BFGS with limited memory m"""
    x = x0.copy()
    g = grad_f(x)
    
    s_list, y_list = [], []
    
    for k in range(max_iter):
        if np.linalg.norm(g) < tol:
            break
        
        # Two-loop recursion to compute H¬∑g
        q = g.copy()
        alphas = []
        
        for s, y in reversed(list(zip(s_list, y_list))):
            rho = 1.0 / (y @ s)
            alpha = rho * (s @ q)
            alphas.append(alpha)
            q = q - alpha * y
        
        # Initial scaling
        if s_list:
            gamma = (s_list[-1] @ y_list[-1]) / (y_list[-1] @ y_list[-1])
        else:
            gamma = 1.0
        r = gamma * q
        
        for (s, y), alpha in zip(zip(s_list, y_list), reversed(alphas)):
            rho = 1.0 / (y @ s)
            beta = rho * (y @ r)
            r = r + (alpha - beta) * s
        
        d = -r  # Search direction
        
        # Line search and update
        alpha = backtracking(f, grad_f, x, d)
        s = alpha * d
        x = x + s
        g_new = grad_f(x)
        y = g_new - g
        
        # Update memory
        if len(s_list) >= m:
            s_list.pop(0)
            y_list.pop(0)
        s_list.append(s)
        y_list.append(y)
        
        g = g_new
    
    return x

```

---

## üìä Convergence Comparison

| Method | Convergence Rate | Per-Iteration Cost | Memory |
|--------|------------------|--------------------| -------|
| Gradient Descent | Linear Œ∫ | O(n) | O(n) |
| Newton | Quadratic | O(n¬≥) | O(n¬≤) |
| BFGS | Superlinear | O(n¬≤) | O(n¬≤) |
| L-BFGS | Superlinear | O(mn) | O(mn) |

```
Superlinear means:
‚Äñx_{k+1} - x*‚Äñ / ‚Äñx_k - x*‚Äñ ‚Üí 0 as k ‚Üí ‚àû

Faster than linear, slower than quadratic

```

---

# Part 2: Conjugate Gradient Method

## üìê Mathematical Foundations

### Problem Setting

```
Solve: Ax = b  where A is symmetric positive definite (SPD)

Equivalent to minimizing:
f(x) = ¬Ωx·µÄAx - b·µÄx

Gradient: ‚àáf(x) = Ax - b = -r  (negative residual)

```

### Conjugate (A-orthogonal) Directions

```
Vectors p‚ÇÄ, p‚ÇÅ, ..., p‚Çô‚Çã‚ÇÅ are A-conjugate if:

p·µ¢·µÄ A p‚±º = 0  for i ‚â† j

Key insight: With n A-conjugate directions, solve in n steps!

Expansion: x* = Œ£·µ¢ Œ±·µ¢ p·µ¢  where Œ±·µ¢ = (p·µ¢·µÄb)/(p·µ¢·µÄAp·µ¢)

```

### CG Algorithm Derivation

```
Start with residual r‚ÇÄ = b - Ax‚ÇÄ = -‚àáf(x‚ÇÄ)

Build conjugate directions from residuals:
p‚Çñ = r‚Çñ + Œ≤‚Çñ p‚Çñ‚Çã‚ÇÅ

Choose Œ≤‚Çñ to ensure p‚Çñ·µÄ A p‚Çñ‚Çã‚ÇÅ = 0:
Œ≤‚Çñ = (r‚Çñ·µÄr‚Çñ)/(r‚Çñ‚Çã‚ÇÅ·µÄr‚Çñ‚Çã‚ÇÅ)

Optimal step along p‚Çñ:
Œ±‚Çñ = (r‚Çñ·µÄr‚Çñ)/(p‚Çñ·µÄAp‚Çñ)

```

### The Complete CG Iteration

```
Given: A (SPD), b, x‚ÇÄ

Initialize:
r‚ÇÄ = b - Ax‚ÇÄ
p‚ÇÄ = r‚ÇÄ

For k = 0, 1, 2, ...:
    Œ±‚Çñ = (r‚Çñ·µÄr‚Çñ) / (p‚Çñ·µÄAp‚Çñ)       # Step size
    x‚Çñ‚Çä‚ÇÅ = x‚Çñ + Œ±‚Çñp‚Çñ               # Update solution
    r‚Çñ‚Çä‚ÇÅ = r‚Çñ - Œ±‚ÇñAp‚Çñ              # Update residual
    
    if ‚Äñr‚Çñ‚Çä‚ÇÅ‚Äñ < tol: STOP
    
    Œ≤‚Çñ = (r‚Çñ‚Çä‚ÇÅ·µÄr‚Çñ‚Çä‚ÇÅ) / (r‚Çñ·µÄr‚Çñ)    # CG coefficient
    p‚Çñ‚Çä‚ÇÅ = r‚Çñ‚Çä‚ÇÅ + Œ≤‚Çñp‚Çñ             # New search direction

Key properties:
‚Ä¢ r·µ¢·µÄr‚±º = 0 for i ‚â† j (residuals orthogonal)
‚Ä¢ p·µ¢·µÄAp‚±º = 0 for i ‚â† j (directions A-conjugate)
‚Ä¢ Converges in ‚â§ n iterations (exact arithmetic)

```

### Convergence Rate

```
Error bound:
‚Äñx‚Çñ - x*‚Äñ_A ‚â§ 2 ¬∑ ((‚àöŒ∫ - 1)/(‚àöŒ∫ + 1))^k ¬∑ ‚Äñx‚ÇÄ - x*‚Äñ_A

where Œ∫ = Œª‚Çò‚Çê‚Çì/Œª‚Çò·µ¢‚Çô (condition number)

Compare to Gradient Descent:
GD: ((Œ∫-1)/(Œ∫+1))^k
CG: ((‚àöŒ∫-1)/(‚àöŒ∫+1))^k ‚âà ((‚àöŒ∫-1)/(‚àöŒ∫+1))^k

CG is MUCH faster for ill-conditioned problems!
Example: Œ∫ = 100
  GD ratio: 0.98^k
  CG ratio: 0.82^k (sqrt effect!)

```

---

## üíª Code Implementation

```python
import numpy as np

def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=None):
    """
    Solve Ax = b using Conjugate Gradient
    A must be symmetric positive definite
    """
    n = len(b)
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    
    if max_iter is None:
        max_iter = n
    
    r = b - A @ x      # Residual
    p = r.copy()       # Search direction
    rs_old = r @ r     # r'r
    
    for k in range(max_iter):
        Ap = A @ p
        alpha = rs_old / (p @ Ap)
        
        x = x + alpha * p
        r = r - alpha * Ap
        
        rs_new = r @ r
        if np.sqrt(rs_new) < tol:
            print(f"Converged in {k+1} iterations")
            break
        
        beta = rs_new / rs_old
        p = r + beta * p
        
        rs_old = rs_new
    
    return x

def preconditioned_cg(A, b, M_inv, x0=None, tol=1e-10, max_iter=None):
    """
    Preconditioned CG: Solve Ax = b with preconditioner M
    M_inv is a function that computes M‚Åª¬πv
    
    Effective condition number: Œ∫(M‚Åª¬πA) << Œ∫(A)
    """
    n = len(b)
    x = np.zeros(n) if x0 is None else x0.copy()
    
    r = b - A @ x
    z = M_inv(r)       # Preconditioned residual
    p = z.copy()
    rz_old = r @ z
    
    for k in range(max_iter or n):
        Ap = A @ p
        alpha = rz_old / (p @ Ap)
        
        x = x + alpha * p
        r = r - alpha * Ap
        
        if np.linalg.norm(r) < tol:
            break
        
        z = M_inv(r)
        rz_new = r @ z
        
        beta = rz_new / rz_old
        p = z + beta * p
        
        rz_old = rz_new
    
    return x

# Example usage
n = 100
A = np.random.randn(n, n)
A = A.T @ A + np.eye(n)  # Make SPD
b = np.random.randn(n)

x_cg = conjugate_gradient(A, b)
x_exact = np.linalg.solve(A, b)
print(f"Error: {np.linalg.norm(x_cg - x_exact)}")

```

---

## üîß Preconditioning

```
Goal: Transform Ax = b to M‚Åª¬πAx = M‚Åª¬πb

Good preconditioner M satisfies:
‚Ä¢ M‚Åª¬πA has small condition number
‚Ä¢ M‚Åª¬πv is cheap to compute

Common preconditioners:
+--------------------+----------------------------+

| Preconditioner     | Description                |
+--------------------+----------------------------+

| Jacobi (M = D)     | Diagonal of A              |
| SSOR               | Symmetric SOR              |
| Incomplete Cholesky| Approximate L L·µÄ          |
| Multigrid          | Hierarchical approach      |
+--------------------+----------------------------+

```

---

## üìä Nonlinear CG (for Optimization)

```
Extend CG to general f(x):

Fletcher-Reeves:
Œ≤‚Çñ·∂†·¥ø = (‚àáf‚Çñ‚Çä‚ÇÅ·µÄ‚àáf‚Çñ‚Çä‚ÇÅ) / (‚àáf‚Çñ·µÄ‚àáf‚Çñ)

Polak-Ribi√®re:
Œ≤‚Çñ·¥æ·¥ø = (‚àáf‚Çñ‚Çä‚ÇÅ·µÄ(‚àáf‚Çñ‚Çä‚ÇÅ - ‚àáf‚Çñ)) / (‚àáf‚Çñ·µÄ‚àáf‚Çñ)

Hestenes-Stiefel:
Œ≤‚Çñ·¥¥À¢ = (‚àáf‚Çñ‚Çä‚ÇÅ·µÄ(‚àáf‚Çñ‚Çä‚ÇÅ - ‚àáf‚Çñ)) / (p‚Çñ·µÄ(‚àáf‚Çñ‚Çä‚ÇÅ - ‚àáf‚Çñ))

Restart: Set Œ≤ = 0 when directions lose conjugacy

```

---

## üöÄ Modern Research Frontiers

### SAM (Sharpness-Aware Minimization)

```
Find flat minima for better generalization:

    min_Œ∏ max_{||Œµ||‚â§œÅ} L(Œ∏ + Œµ)

Approximation:
    1. Compute Œµ = œÅ ‚àáL(Œ∏) / ||‚àáL(Œ∏)||
    2. Update Œ∏ ‚Üê Œ∏ - Œ± ‚àáL(Œ∏ + Œµ)

```

### Lion Optimizer

```
Memory-efficient alternative to Adam:
    Uses sign() instead of full gradient moments
    Much lower memory than Adam
    Strong performance on transformers

```

### Sophia (2nd-Order for LLMs)

```
Scalable 2nd-order optimizer for LLMs:
    Uses Hessian diagonal approximation
    Clip updates based on Hessian
    Faster convergence than Adam

```

---

## üìö References

| Type | Title | Link |
|------|-------|------|
| üìñ | Nocedal & Wright Ch. 6-7 | Numerical Optimization |
| üìÑ | Original BFGS (1970) | Broyden, Fletcher, Goldfarb, Shanno |
| üìÑ | L-BFGS Paper | Liu & Nocedal, 1989 |
| üìñ | Golub & Van Loan Ch. 10 | Matrix Computations |
| üìÑ | Shewchuk (1994) | [An Introduction to CG](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) |
| üìÑ | SAM Paper | [arXiv](https://arxiv.org/abs/2010.01412) |
| üìÑ | Lion Paper | [arXiv](https://arxiv.org/abs/2302.06675) |
| üá®üá≥ | ÊãüÁâõÈ°øÊ≥ïËØ¶Ëß£ | [Áü•‰πé](https://zhuanlan.zhihu.com/p/29672873) |
| üá®üá≥ | ÂÖ±ËΩ≠Ê¢ØÂ∫¶Ê≥ï | [CSDN](https://blog.csdn.net/qq_37466121/article/details/88819089) |

---

## üîó Where This Topic Is Used

| Application | Usage |
|-------------|-------|
| **Machine Learning** | Core concept for ML systems |
| **Deep Learning** | Foundation for neural networks |
| **Research** | Important for understanding papers |

---

‚¨ÖÔ∏è [Back: Basic Methods](../02_basic_methods/) | ‚û°Ô∏è [Next: Convex Optimization](../04_convex_optimization/)

---

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=FF6B6B&height=80&section=footer" width="100%"/>
</p>
